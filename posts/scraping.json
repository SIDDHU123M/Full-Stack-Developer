[{"id":945,"date":"2020-05-21T05:26:21","date_gmt":"2020-05-21T05:26:21","guid":{"rendered":"http://python3.foobrdigital.com/?p=945"},"modified":"2020-12-16T16:48:52","modified_gmt":"2020-12-16T16:48:52","slug":"introduction-4","status":"publish","type":"post","link":"https://python3.foobrdigital.com/introduction-4/","title":{"rendered":"Introduction"},"content":{"rendered":"\n<p>Web scraping is an automatic process of extracting information from web. This chapter will give you an in-depth idea of web scraping, its comparison with web crawling, and why you should opt for web scraping. You will also learn about the components and working of a web scraper.</p>\n\n\n\n<h2>What is Web Scraping?</h2>\n\n\n\n<p>The dictionary meaning of word ‘Scrapping’ implies getting something from the web. Here two questions arise: What we can get from the web and How to get that.</p>\n\n\n\n<p>The answer to the first question is&nbsp;<strong>‘data’</strong>. Data is indispensable for any programmer and the basic requirement of every programming project is the large amount of useful data.</p>\n\n\n\n<p>The answer to the second question is a bit tricky, because there are lots of ways to get data. In general, we may get data from a database or data file and other sources. But what if we need large amount of data that is available online? One way to get such kind of data is to manually search (clicking away in a web browser) and save (copy-pasting into a spreadsheet or file) the required data. This method is quite tedious and time consuming. Another way to get such data is using&nbsp;<strong>web scraping</strong>.</p>\n\n\n\n<p><strong>Web scraping</strong>, also called&nbsp;<strong>web data mining</strong>&nbsp;or&nbsp;<strong>web harvesting</strong>, is the process of constructing an agent which can extract, parse, download and organize useful information from the web automatically. In other words, we can say that instead of manually saving the data from websites, the web scraping software will automatically load and extract data from multiple websites as per our requirement.</p>\n\n\n\n<h2>Origin of Web Scraping</h2>\n\n\n\n<p>The origin of web scraping is screen scrapping, which was used to integrate non-web based applications or native windows applications. Originally screen scraping was used prior to the wide use of World Wide Web (WWW), but it could not scale up WWW expanded. This made it necessary to automate the approach of screen scraping and the technique called&nbsp;<strong>‘Web Scraping’</strong>&nbsp;came into existence.</p>\n\n\n\n<h2>Web Crawling v/s Web Scraping</h2>\n\n\n\n<p>The terms Web Crawling and Scraping are often used interchangeably as the basic concept of them is to extract data. However, they are different from each other. We can understand the basic difference from their definitions.</p>\n\n\n\n<p>Web crawling is basically used to index the information on the page using bots aka crawlers. It is also called&nbsp;<strong>indexing</strong>. On the hand, web scraping is an automated way of extracting the information using bots aka scrapers. It is also called&nbsp;<strong>data extraction</strong>.</p>\n\n\n\n<p>To understand the difference between these two terms, let us look into the comparison table given hereunder −</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><th>Web Crawling</th><th>Web Scraping</th></tr><tr><td>Refers to downloading and storing the contents of a large number of websites.</td><td>Refers to extracting individual data elements from the website by using a site-specific structure.</td></tr><tr><td>Mostly done on large scale.</td><td>Can be implemented at any scale.</td></tr><tr><td>Yields generic information.</td><td>Yields specific information.</td></tr><tr><td>Used by major search engines like Google, Bing, Yahoo.&nbsp;<strong>Googlebot</strong>&nbsp;is an example of a web crawler.</td><td>The information extracted using web scraping can be used to replicate in some other website or can be used to perform data analysis. For example the data elements can be names, address, price etc.</td></tr></tbody></table></figure>\n\n\n\n<h2>Uses of Web Scraping</h2>\n\n\n\n<p>The uses and reasons for using web scraping are as endless as the uses of the World Wide Web. Web scrapers can do anything like ordering online food, scanning online shopping website for you and buying ticket of a match the moment they are available etc. just like a human can do. Some of the important uses of web scraping are discussed here −</p>\n\n\n\n<ul><li><strong>E-commerce Websites</strong>&nbsp;− Web scrapers can collect the data specially related to the price of a specific product from various e-commerce websites for their comparison.</li><li><strong>Content Aggregators</strong>&nbsp;− Web scraping is used widely by content aggregators like news aggregators and job aggregators for providing updated data to their users.</li><li><strong>Marketing and Sales Campaigns</strong>&nbsp;− Web scrapers can be used to get the data like emails, phone number etc. for sales and marketing campaigns.</li><li><strong>Search Engine Optimization (SEO)</strong>&nbsp;− Web scraping is widely used by SEO tools like SEMRush, Majestic etc. to tell business how they rank for search keywords that matter to them.</li><li><strong>Data for Machine Learning Projects</strong>&nbsp;− Retrieval of data for machine learning projects depends upon web scraping.</li></ul>\n\n\n\n<p><strong>Data for Research</strong>&nbsp;− Researchers can collect useful data for the purpose of their research work by saving their time by this automated process.</p>\n\n\n\n<h2>Components of a Web Scraper</h2>\n\n\n\n<p>A web scraper consists of the following components −</p>\n\n\n\n<h3>Web Crawler Module</h3>\n\n\n\n<p>A very necessary component of web scraper, web crawler module, is used to navigate the target website by making HTTP or HTTPS request to the URLs. The crawler downloads the unstructured data (HTML contents) and passes it to extractor, the next module.</p>\n\n\n\n<h3>Extractor</h3>\n\n\n\n<p>The extractor processes the fetched HTML content and extracts the data into semistructured format. This is also called as a parser module and uses different parsing techniques like Regular expression, HTML Parsing, DOM parsing or Artificial Intelligence for its functioning.</p>\n\n\n\n<h3>Data Transformation and Cleaning Module</h3>\n\n\n\n<p>The data extracted above is not suitable for ready use. It must pass through some cleaning module so that we can use it. The methods like String manipulation or regular expression can be used for this purpose. Note that extraction and transformation can be performed in a single step also.</p>\n\n\n\n<h3>Storage Module</h3>\n\n\n\n<p>After extracting the data, we need to store it as per our requirement. The storage module will output the data in a standard format that can be stored in a database or JSON or CSV format.</p>\n\n\n\n<h2>Working of a Web Scraper</h2>\n\n\n\n<p>Web scraper may be defined as a software or script used to download the contents of multiple web pages and extracting data from it.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter\"><img src=\"https://www.tutorialspoint.com/python_web_scraping/images/web_scraper.jpg\" alt=\"Web Scraper\"/></figure></div>\n\n\n\n<p>We can understand the working of a web scraper in simple steps as shown in the diagram given above.</p>\n\n\n\n<h3>Step 1: Downloading Contents from Web Pages</h3>\n\n\n\n<p>In this step, a web scraper will download the requested contents from multiple web pages.</p>\n\n\n\n<h3>Step 2: Extracting Data</h3>\n\n\n\n<p>The data on websites is HTML and mostly unstructured. Hence, in this step, web scraper will parse and extract structured data from the downloaded contents.</p>\n\n\n\n<h3>Step 3: Storing the Data</h3>\n\n\n\n<p>Here, a web scraper will store and save the extracted data in any of the format like CSV, JSON or in database.</p>\n\n\n\n<h3>Step 4: Analyzing the Data</h3>\n\n\n\n<p>After all these steps are successfully done, the web scraper will analyze the data thus obtained.</p>\n","protected":false},"excerpt":{"rendered":"<p>Web scraping is an automatic process of extracting information from web. This chapter will give you an in-depth idea of web scraping, its comparison with web crawling, and why you should opt for web scraping. You will also learn about the components and working of a web scraper. What is Web Scraping? The dictionary meaning [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1819,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/945"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=945"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/945/revisions"}],"predecessor-version":[{"id":946,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/945/revisions/946"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1819"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=945"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=945"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=945"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":947,"date":"2020-05-21T05:30:41","date_gmt":"2020-05-21T05:30:41","guid":{"rendered":"http://python3.foobrdigital.com/?p=947"},"modified":"2020-12-16T16:48:52","modified_gmt":"2020-12-16T16:48:52","slug":"getting-started-with-python","status":"publish","type":"post","link":"https://python3.foobrdigital.com/getting-started-with-python/","title":{"rendered":"Getting Started with Python"},"content":{"rendered":"\n<p>In the first chapter, we have learnt what web scraping is all about. In this chapter, let us see how to implement web scraping using Python.</p>\n\n\n\n<h2>Why Python for Web Scraping?</h2>\n\n\n\n<p>Python is a popular tool for implementing web scraping. Python programming language is also used for other useful projects related to cyber security, penetration testing as well as digital forensic applications. Using the base programming of Python, web scraping can be performed without using any other third party tool.</p>\n\n\n\n<p>Python programming language is gaining huge popularity and the reasons that make Python a good fit for web scraping projects are as below −</p>\n\n\n\n<h3>Syntax Simplicity</h3>\n\n\n\n<p>Python has the simplest structure when compared to other programming languages. This feature of Python makes the testing easier and a developer can focus more on programming.</p>\n\n\n\n<h3>Inbuilt Modules</h3>\n\n\n\n<p>Another reason for using Python for web scraping is the inbuilt as well as external useful libraries it possesses. We can perform many implementations related to web scraping by using Python as the base for programming.</p>\n\n\n\n<h3>Open Source Programming Language</h3>\n\n\n\n<p>Python has huge support from the community because it is an open source programming language.</p>\n\n\n\n<h3>Wide range of Applications</h3>\n\n\n\n<p>Python can be used for various programming tasks ranging from small shell scripts to enterprise web applications.</p>\n\n\n\n<h2>Installation of Python</h2>\n\n\n\n<p>Python distribution is available for platforms like Windows, MAC and Unix/Linux. We need to download only the binary code applicable for our platform to install Python. But in case if the binary code for our platform is not available, we must have a C compiler so that source code can be compiled manually.</p>\n\n\n\n<p>We can install Python on various platforms as follows −</p>\n\n\n\n<h3>Installing Python on Unix and Linux</h3>\n\n\n\n<p>You need to followings steps given below to install Python on Unix/Linux machines −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− Go to the link&nbsp;https://www.python.org/downloads/</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Download the zipped source code available for Unix/Linux on above link.</p>\n\n\n\n<p><strong>Step 3</strong>&nbsp;− Extract the files onto your computer.</p>\n\n\n\n<p><strong>Step 4</strong>&nbsp;− Use the following commands to complete the installation −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>run ./configure script\nmake\nmake install</code></pre>\n\n\n\n<p>You can find installed Python at the standard location&nbsp;<strong>/usr/local/bin</strong>&nbsp;and its libraries at&nbsp;<strong>/usr/local/lib/pythonXX</strong>, where XX is the version of Python.</p>\n\n\n\n<h3>Installing Python on Windows</h3>\n\n\n\n<p>You need to followings steps given below to install Python on Windows machines −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− Go to the link&nbsp;https://www.python.org/downloads/</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Download the Windows installer&nbsp;<strong>python-XYZ.msi</strong>&nbsp;file, where XYZ is the version we need to install.</p>\n\n\n\n<p><strong>Step 3</strong>&nbsp;− Now, save the installer file to your local machine and run the MSI file.</p>\n\n\n\n<p><strong>Step 4</strong>&nbsp;− At last, run the downloaded file to bring up the Python install wizard.</p>\n\n\n\n<h3>Installing Python on Macintosh</h3>\n\n\n\n<p>We must use&nbsp;<strong>Homebrew</strong>&nbsp;for installing Python 3 on Mac OS X. Homebrew is easy to install and a great package installer.</p>\n\n\n\n<p>Homebrew can also be installed by using the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ ruby -e \"$(curl -fsSL\nhttps://raw.githubusercontent.com/Homebrew/install/master/install)\"</code></pre>\n\n\n\n<p>For updating the package manager, we can use the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ brew update</code></pre>\n\n\n\n<p>With the help of the following command, we can install Python3 on our MAC machine −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ brew install python3</code></pre>\n\n\n\n<h2>Setting Up the PATH</h2>\n\n\n\n<p>You can use the following instructions to set up the path on various environments −</p>\n\n\n\n<h3>Setting Up the Path on Unix/Linux</h3>\n\n\n\n<p>Use the following commands for setting up paths using various command shells −</p>\n\n\n\n<h3>For csh shell</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>setenv PATH \"$PATH:/usr/local/bin/python\".</code></pre>\n\n\n\n<h3>For bash shell (Linux)</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>ATH=\"$PATH:/usr/local/bin/python\".</code></pre>\n\n\n\n<h3>For sh or ksh shell</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>PATH=\"$PATH:/usr/local/bin/python\".</code></pre>\n\n\n\n<h3>Setting Up the Path on Windows</h3>\n\n\n\n<p>For setting the path on Windows, we can use the path&nbsp;<strong>%path%;C:\\Python</strong>&nbsp;at the command prompt and then press Enter.</p>\n\n\n\n<h2>Running Python</h2>\n\n\n\n<p>We can start Python using any of the following three ways −</p>\n\n\n\n<h3>Interactive Interpreter</h3>\n\n\n\n<p>An operating system such as UNIX and DOS that is providing a command-line interpreter or shell can be used for starting Python.</p>\n\n\n\n<p>We can start coding in interactive interpreter as follows −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− Enter&nbsp;<strong>python</strong>&nbsp;at the command line.</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Then, we can start coding right away in the interactive interpreter.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$python # Unix/Linux\nor\npython% # Unix/Linux\nor\nC:> python # Windows/DOS</code></pre>\n\n\n\n<h3>Script from the Command-line</h3>\n\n\n\n<p>We can execute a Python script at command line by invoking the interpreter. It can be understood as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$python script.py # Unix/Linux\nor\npython% script.py # Unix/Linux\nor\nC: >python script.py # Windows/DOS</code></pre>\n\n\n\n<h3>Integrated Development Environment</h3>\n\n\n\n<p>We can also run Python from GUI environment if the system is having GUI application that is supporting Python. Some IDEs that support Python on various platforms are given below −</p>\n\n\n\n<p><strong>IDE for UNIX</strong>&nbsp;− UNIX, for Python, has IDLE IDE.</p>\n\n\n\n<p><strong>IDE for Windows</strong>&nbsp;− Windows has PythonWin IDE which has GUI too.</p>\n\n\n\n<p><strong>IDE for Macintosh</strong>&nbsp;− Macintosh has IDLE IDE which is downloadable as either MacBinary or BinHex&#8217;d files from the main website.</p>\n","protected":false},"excerpt":{"rendered":"<p>In the first chapter, we have learnt what web scraping is all about. In this chapter, let us see how to implement web scraping using Python. Why Python for Web Scraping? Python is a popular tool for implementing web scraping. Python programming language is also used for other useful projects related to cyber security, penetration [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1830,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/947"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=947"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/947/revisions"}],"predecessor-version":[{"id":1831,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/947/revisions/1831"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1830"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=947"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=947"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=947"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":949,"date":"2020-05-21T05:40:48","date_gmt":"2020-05-21T05:40:48","guid":{"rendered":"http://python3.foobrdigital.com/?p=949"},"modified":"2020-12-16T16:48:52","modified_gmt":"2020-12-16T16:48:52","slug":"python-modules-for-web-scraping","status":"publish","type":"post","link":"https://python3.foobrdigital.com/python-modules-for-web-scraping/","title":{"rendered":"Python Modules for Web Scraping"},"content":{"rendered":"\n<p>In this chapter, let us learn various Python modules that we can use for web scraping.</p>\n\n\n\n<h2>Python Development Environments using virtualenv</h2>\n\n\n\n<p>Virtualenv is a tool to create isolated Python environments. With the help of virtualenv, we can create a folder that contains all necessary executables to use the packages that our Python project requires. It also allows us to add and modify Python modules without access to the global installation.</p>\n\n\n\n<p>You can use the following command to install&nbsp;<strong>virtualenv</strong>&nbsp;−</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>pip install virtualenv\nCollecting virtualenv\n   Downloading\nhttps:&#47;&#47;files.pythonhosted.org/packages/b6/30/96a02b2287098b23b875bc8c2f58071c3\n5d2efe84f747b64d523721dc2b5/virtualenv-16.0.0-py2.py3-none-any.whl\n(1.9MB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 1.9MB 86kB/s\nInstalling collected packages: virtualenv\nSuccessfully installed virtualenv-16.0.0</code></pre>\n\n\n\n<p>Now, we need to create a directory which will represent the project with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>mkdir webscrap</code></pre>\n\n\n\n<p>Now, enter into that directory with the help of this following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>cd webscrap</code></pre>\n\n\n\n<p>Now, we need to initialize virtual environment folder of our choice as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData\\webscrap>virtualenv websc\nUsing base prefix 'd:\\\\programdata'\nNew python executable in D:\\ProgramData\\webscrap\\websc\\Scripts\\python.exe\nInstalling setuptools, pip, wheel...done.</code></pre>\n\n\n\n<p>Now, activate the virtual environment with the command given below. Once successfully activated, you will see the name of it on the left hand side in brackets.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData\\webscrap>websc\\scripts\\activate</code></pre>\n\n\n\n<p>We can install any module in this environment as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(websc) (base) D:\\ProgramData\\webscrap>pip install requests\nCollecting requests\n   Downloading\nhttps:&#47;&#47;files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69\nc4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (9\n1kB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 92kB 148kB/s\nCollecting chardet&lt;3.1.0,>=3.0.2 (from requests)\n   Downloading\nhttps://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca\n55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133\nkB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 143kB 369kB/s\nCollecting certifi>=2017.4.17 (from requests)\n   Downloading\nhttps://files.pythonhosted.org/packages/df/f7/04fee6ac349e915b82171f8e23cee6364\n4d83663b34c539f7a09aed18f9e/certifi-2018.8.24-py2.py3-none-any.whl\n(147kB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 153kB 527kB/s\nCollecting urllib3&lt;1.24,>=1.21.1 (from requests)\n   Downloading\nhttps://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c5\n3851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133k\nB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 143kB 517kB/s\nCollecting idna&lt;2.8,>=2.5 (from requests)\n   Downloading\nhttps://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746\na97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 61kB 339kB/s\nInstalling collected packages: chardet, certifi, urllib3, idna, requests\nSuccessfully installed certifi-2018.8.24 chardet-3.0.4 idna-2.7 requests-2.19.1\nurllib3-1.23</code></pre>\n\n\n\n<p>For deactivating the virtual environment, we can use the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(websc) (base) D:\\ProgramData\\webscrap>deactivate\n(base) D:\\ProgramData\\webscrap></code></pre>\n\n\n\n<p>You can see that (websc) has been deactivated.</p>\n\n\n\n<h2>Python Modules for Web Scraping</h2>\n\n\n\n<p>Web scraping is the process of constructing an agent which can extract, parse, download and organize useful information from the web automatically. In other words, instead of manually saving the data from websites, the web scraping software will automatically load and extract data from multiple websites as per our requirement.</p>\n\n\n\n<p>In this section, we are going to discuss about useful Python libraries for web scraping.</p>\n\n\n\n<h2>Requests</h2>\n\n\n\n<p>It is a simple python web scraping library. It is an efficient HTTP library used for accessing web pages. With the help of&nbsp;<strong>Requests</strong>, we can get the raw HTML of web pages which can then be parsed for retrieving the data. Before using&nbsp;<strong>requests</strong>, let us understand its installation.</p>\n\n\n\n<h3>Installing Requests</h3>\n\n\n\n<p>We can install it in either on our virtual environment or on the global installation. With the help of&nbsp;<strong>pip</strong>&nbsp;command, we can easily install it as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData> pip install requests\nCollecting requests\nUsing cached\nhttps:&#47;&#47;files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69\nc4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl\nRequirement already satisfied: idna&lt;2.8,>=2.5 in d:\\programdata\\lib\\sitepackages\n(from requests) (2.6)\nRequirement already satisfied: urllib3&lt;1.24,>=1.21.1 in\nd:\\programdata\\lib\\site-packages (from requests) (1.22)\nRequirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\lib\\sitepackages\n(from requests) (2018.1.18)\nRequirement already satisfied: chardet&lt;3.1.0,>=3.0.2 in\nd:\\programdata\\lib\\site-packages (from requests) (3.0.4)\nInstalling collected packages: requests\nSuccessfully installed requests-2.19.1</code></pre>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we are making a GET HTTP request for a web page. For this we need to first import requests library as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>In &#91;1]: import requests</code></pre>\n\n\n\n<p>In this following line of code, we use requests to make a GET HTTP requests for the url:&nbsp;https://authoraditiagarwal.com/&nbsp;by making a GET request.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>In &#91;2]: r = requests.get('https://authoraditiagarwal.com/')</code></pre>\n\n\n\n<p>Now we can retrieve the content by using&nbsp;<strong>.text</strong>&nbsp;property as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>In &#91;5]: r.text&#91;:200]</code></pre>\n\n\n\n<p>Observe that in the following output, we got the first 200 characters.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Out&#91;5]: '&lt;!DOCTYPE html>\\n&lt;html lang=\"en-US\"\\n\\titemscope\n\\n\\titemtype=\"http://schema.org/WebSite\" \\n\\tprefix=\"og: http://ogp.me/ns#\"\n>\\n&lt;head>\\n\\t&lt;meta charset\n=\"UTF-8\" />\\n\\t&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE'</code></pre>\n\n\n\n<h2>Urllib3</h2>\n\n\n\n<p>It is another Python library that can be used for retrieving data from URLs similar to the&nbsp;<strong>requests</strong>&nbsp;library. You can read more on this at its technical documentation at&nbsp;https://urllib3.readthedocs.io/en/latest/.</p>\n\n\n\n<h3>Installing Urllib3</h3>\n\n\n\n<p>Using the&nbsp;<strong>pip</strong>&nbsp;command, we can install&nbsp;<strong>urllib3</strong>&nbsp;either in our virtual environment or in global installation.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>pip install urllib3\nCollecting urllib3\nUsing cached\nhttps:&#47;&#47;files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c5\n3851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl\nInstalling collected packages: urllib3\nSuccessfully installed urllib3-1.23</code></pre>\n\n\n\n<h3>Example: Scraping using Urllib3 and BeautifulSoup</h3>\n\n\n\n<p>In the following example, we are scraping the web page by using&nbsp;<strong>Urllib3</strong>&nbsp;and&nbsp;<strong>BeautifulSoup</strong>. We are using&nbsp;<strong>Urllib3</strong>&nbsp;at the place of requests library for getting the raw data (HTML) from web page. Then we are using&nbsp;<strong>BeautifulSoup</strong>&nbsp;for parsing that HTML data.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import urllib3\nfrom bs4 import BeautifulSoup\nhttp = urllib3.PoolManager()\nr = http.request('GET', 'https://authoraditiagarwal.com')\nsoup = BeautifulSoup(r.data, 'lxml')\nprint (soup.title)\nprint (soup.title.text)</code></pre>\n\n\n\n<p>This is the output you will observe when you run this code −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&lt;title>Learn and Grow with Aditi Agarwal&lt;/title>\nLearn and Grow with Aditi Agarwal</code></pre>\n\n\n\n<h2>Selenium</h2>\n\n\n\n<p>It is an open source automated testing suite for web applications across different browsers and platforms. It is not a single tool but a suite of software. We have selenium bindings for Python, Java, C#, Ruby and JavaScript. Here we are going to perform web scraping by using selenium and its Python bindings. You can learn more about Selenium with Java on the link&nbsp;Selenium.</p>\n\n\n\n<p>Selenium Python bindings provide a convenient API to access Selenium WebDrivers like Firefox, IE, Chrome, Remote etc. The current supported Python versions are 2.7, 3.5 and above.</p>\n\n\n\n<h3>Installing Selenium</h3>\n\n\n\n<p>Using the&nbsp;<strong>pip</strong>&nbsp;command, we can install&nbsp;<strong>urllib3</strong>&nbsp;either in our virtual environment or in global installation.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install selenium</code></pre>\n\n\n\n<p>As selenium requires a driver to interface with the chosen browser, we need to download it. The following table shows different browsers and their links for downloading the same.</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><td><strong>Chrome</strong></td><td>https://sites.google.com/a/chromium.org/</td></tr><tr><td><strong>Edge</strong></td><td>https://developer.microsoft.com/</td></tr><tr><td><strong>Firefox</strong></td><td>https://github.com/</td></tr><tr><td><strong>Safari</strong></td><td>https://webkit.org/</td></tr></tbody></table></figure>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>This example shows web scraping using selenium. It can also be used for testing which is called selenium testing.</p>\n\n\n\n<p>After downloading the particular driver for the specified version of browser, we need to do programming in Python.</p>\n\n\n\n<p>First, need to import&nbsp;<strong>webdriver</strong>&nbsp;from selenium as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from selenium import webdriver</code></pre>\n\n\n\n<p>Now, provide the path of web driver which we have downloaded as per our requirement −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r'C:\\\\Users\\\\gaurav\\\\Desktop\\\\Chromedriver'\nbrowser = webdriver.Chrome(executable_path = path)</code></pre>\n\n\n\n<p>Now, provide the url which we want to open in that web browser now controlled by our Python script.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>browser.get('https://authoraditiagarwal.com/leadershipmanagement')</code></pre>\n\n\n\n<p>We can also scrape a particular element by providing the xpath as provided in lxml.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>browser.find_element_by_xpath('/html/body').click()</code></pre>\n\n\n\n<p>You can check the browser, controlled by Python script, for output.</p>\n\n\n\n<h2>Scrapy</h2>\n\n\n\n<p>Scrapy is a fast, open-source web crawling framework written in Python, used to extract the data from the web page with the help of selectors based on XPath. Scrapy was first released on June 26, 2008 licensed under BSD, with a milestone 1.0 releasing in June 2015. It provides us all the tools we need to extract, process and structure the data from websites.</p>\n\n\n\n<h3>Installing Scrapy</h3>\n\n\n\n<p>Using the&nbsp;<strong>pip</strong>&nbsp;command, we can install&nbsp;<strong>urllib3</strong>&nbsp;either in our virtual environment or in global installation.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install scrapy</code></pre>\n\n\n\n<p>For more detail study of Scrapy you can go to the link&nbsp;Scrapy</p>\n","protected":false},"excerpt":{"rendered":"<p>In this chapter, let us learn various Python modules that we can use for web scraping. Python Development Environments using virtualenv Virtualenv is a tool to create isolated Python environments. With the help of virtualenv, we can create a folder that contains all necessary executables to use the packages that our Python project requires. It [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1809,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/949"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=949"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/949/revisions"}],"predecessor-version":[{"id":1820,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/949/revisions/1820"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1809"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=949"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=949"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=949"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":959,"date":"2020-05-21T05:47:08","date_gmt":"2020-05-21T05:47:08","guid":{"rendered":"http://python3.foobrdigital.com/?p=959"},"modified":"2020-12-16T16:48:52","modified_gmt":"2020-12-16T16:48:52","slug":"legality-of-web-scraping","status":"publish","type":"post","link":"https://python3.foobrdigital.com/legality-of-web-scraping/","title":{"rendered":"Legality of Web Scraping"},"content":{"rendered":"\n<p>With Python, we can scrape any website or particular elements of a web page but do you have any idea whether it is legal or not? Before scraping any website we must have to know about the legality of web scraping. This chapter will explain the concepts related to legality of web scraping.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>Generally, if you are going to use the scraped data for personal use, then there may not be any problem. But if you are going to republish that data, then before doing the same you should make download request to the owner or do some background research about policies as well about the data you are going to scrape.</p>\n\n\n\n<h2>Research Required Prior to Scraping</h2>\n\n\n\n<p>If you are targeting a website for scraping data from it, we need to understand its scale and structure. Following are some of the files which we need to analyze before starting web scraping.</p>\n\n\n\n<h3>Analyzing robots.txt</h3>\n\n\n\n<p>Actually most of the publishers allow programmers to crawl their websites at some extent. In other sense, publishers want specific portions of the websites to be crawled. To define this, websites must put some rules for stating which portions can be crawled and which cannot be. Such rules are defined in a file called&nbsp;<strong>robots.txt</strong>.</p>\n\n\n\n<p><strong>robots.txt</strong>&nbsp;is human readable file used to identify the portions of the website that crawlers are allowed as well as not allowed to scrape. There is no standard format of robots.txt file and the publishers of website can do modifications as per their needs. We can check the robots.txt file for a particular website by providing a slash and robots.txt after url of that website. For example, if we want to check it for Google.com, then we need to type&nbsp;https://www.google.com/robots.txt&nbsp;and we will get something as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>User-agent: *\nDisallow: /search\nAllow: /search/about\nAllow: /search/static\nAllow: /search/howsearchworks\nDisallow: /sdch\nDisallow: /groups\nDisallow: /index.html?\nDisallow: /?\nAllow: /?hl=\nDisallow: /?hl=*&amp;\nAllow: /?hl=*&amp;gws_rd=ssl$\nand so on……..</code></pre>\n\n\n\n<p>Some of the most common rules that are defined in a website’s robots.txt file are as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>   User-agent: BadCrawler\nDisallow: /\n﻿</code></pre>\n\n\n\n<p>The above rule means the robots.txt file asks a crawler with&nbsp;<strong>BadCrawler</strong>&nbsp;user agent not to crawl their website.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>User-agent: *\nCrawl-delay: 5\nDisallow: /trap</code></pre>\n\n\n\n<p>The above rule means the robots.txt file delays a crawler for 5 seconds between download requests for all user-agents for avoiding overloading server. The&nbsp;<strong>/trap</strong>&nbsp;link will try to block malicious crawlers who follow disallowed links. There are many more rules that can be defined by the publisher of the website as per their requirements. Some of them are discussed here −</p>\n\n\n\n<h3>Analyzing Sitemap files</h3>\n\n\n\n<p>What you supposed to do if you want to crawl a website for updated information? You will crawl every web page for getting that updated information, but this will increase the server traffic of that particular website. That is why websites provide sitemap files for helping the crawlers to locate updating content without needing to crawl every web page. Sitemap standard is defined at&nbsp;http://www.sitemaps.org/protocol.html.</p>\n\n\n\n<h3>Content of Sitemap file</h3>\n\n\n\n<p>The following is the content of sitemap file of&nbsp;https://www.microsoft.com/robots.txt&nbsp;that is discovered in robot.txt file −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Sitemap: https://www.microsoft.com/en-us/explore/msft_sitemap_index.xml\nSitemap: https://www.microsoft.com/learning/sitemap.xml\nSitemap: https://www.microsoft.com/en-us/licensing/sitemap.xml\nSitemap: https://www.microsoft.com/en-us/legal/sitemap.xml\nSitemap: https://www.microsoft.com/filedata/sitemaps/RW5xN8\nSitemap: https://www.microsoft.com/store/collections.xml\nSitemap: https://www.microsoft.com/store/productdetailpages.index.xml\nSitemap: https://www.microsoft.com/en-us/store/locations/store-locationssitemap.xml</code></pre>\n\n\n\n<p>The above content shows that the sitemap lists the URLs on website and further allows a webmaster to specify some additional information like last updated date, change of contents, importance of URL with relation to others etc. about each URL.</p>\n\n\n\n<h3>What is the Size of Website?</h3>\n\n\n\n<p>Is the size of a website, i.e. the number of web pages of a website affects the way we crawl? Certainly yes. Because if we have less number of web pages to crawl, then the efficiency would not be a serious issue, but suppose if our website has millions of web pages, for example Microsoft.com, then downloading each web page sequentially would take several months and then efficiency would be a serious concern.</p>\n\n\n\n<h3>Checking Website’s Size</h3>\n\n\n\n<p>By checking the size of result of Google’s crawler, we can have an estimate of the size of a website. Our result can be filtered by using the keyword&nbsp;<strong>site</strong>&nbsp;while doing the Google search. For example, estimating the size of&nbsp;https://authoraditiagarwal.com/&nbsp;is given below −</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://www.tutorialspoint.com/python_web_scraping/images/checking_the_size.jpg\" alt=\"Checking The Size\"/></figure>\n\n\n\n<p>You can see there are around 60 results which mean it is not a big website and crawling would not lead the efficiency issue.</p>\n\n\n\n<h3>Which technology is used by website?</h3>\n\n\n\n<p>Another important question is whether the technology used by website affects the way we crawl? Yes, it affects. But how we can check about the technology used by a website? There is a Python library named&nbsp;<strong>builtwith</strong>&nbsp;with the help of which we can find out about the technology used by a website.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example we are going to check the technology used by the website&nbsp;https://authoraditiagarwal.com&nbsp;with the help of Python library&nbsp;<strong>builtwith</strong>. But before using this library, we need to install it as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>pip install builtwith\nCollecting builtwith\n   Downloading\nhttps:&#47;&#47;files.pythonhosted.org/packages/9b/b8/4a320be83bb3c9c1b3ac3f9469a5d66e0\n2918e20d226aa97a3e86bddd130/builtwith-1.3.3.tar.gz\nRequirement already satisfied: six in d:\\programdata\\lib\\site-packages (from\nbuiltwith) (1.10.0)\nBuilding wheels for collected packages: builtwith\n   Running setup.py bdist_wheel for builtwith ... done\n   Stored in directory:\nC:\\Users\\gaurav\\AppData\\Local\\pip\\Cache\\wheels\\2b\\00\\c2\\a96241e7fe520e75093898b\nf926764a924873e0304f10b2524\nSuccessfully built builtwith\nInstalling collected packages: builtwith\nSuccessfully installed builtwith-1.3.3</code></pre>\n\n\n\n<p>Now, with the help of following simple line of codes we can check the technology used by a particular website −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>In &#91;1]: import builtwith\nIn &#91;2]: builtwith.parse('http://authoraditiagarwal.com')\nOut&#91;2]:\n{'blogs': &#91;'PHP', 'WordPress'],\n   'cms': &#91;'WordPress'],\n   'ecommerce': &#91;'WooCommerce'],\n   'font-scripts': &#91;'Font Awesome'],\n   'javascript-frameworks': &#91;'jQuery'],\n   'programming-languages': &#91;'PHP'],\n   'web-servers': &#91;'Apache']}</code></pre>\n\n\n\n<h3>Who is the owner of website?</h3>\n\n\n\n<p>The owner of the website also matters because if the owner is known for blocking the crawlers, then the crawlers must be careful while scraping the data from website. There is a protocol named&nbsp;<strong>Whois</strong>&nbsp;with the help of which we can find out about the owner of the website.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example we are going to check the owner of the website say&nbsp;microsoft.com&nbsp;with the help of Whois. But before using this library, we need to install it as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>pip install python-whois\nCollecting python-whois\n   Downloading\nhttps:&#47;&#47;files.pythonhosted.org/packages/63/8a/8ed58b8b28b6200ce1cdfe4e4f3bbc8b8\n5a79eef2aa615ec2fef511b3d68/python-whois-0.7.0.tar.gz (82kB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 92kB 164kB/s\nRequirement already satisfied: future in d:\\programdata\\lib\\site-packages (from\npython-whois) (0.16.0)\nBuilding wheels for collected packages: python-whois\n   Running setup.py bdist_wheel for python-whois ... done\n   Stored in directory:\nC:\\Users\\gaurav\\AppData\\Local\\pip\\Cache\\wheels\\06\\cb\\7d\\33704632b0e1bb64460dc2b\n4dcc81ab212a3d5e52ab32dc531\nSuccessfully built python-whois\nInstalling collected packages: python-whois\nSuccessfully installed python-whois-0.7.0</code></pre>\n\n\n\n<p>Now, with the help of following simple line of codes we can check the technology used by a particular website −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>In &#91;1]: import whois\nIn &#91;2]: print (whois.whois('microsoft.com'))\n{\n   \"domain_name\": &#91;\n      \"MICROSOFT.COM\",\n      \"microsoft.com\"\n   ],\n   -------\n   \"name_servers\": &#91;\n      \"NS1.MSFT.NET\",\n      \"NS2.MSFT.NET\",\n      \"NS3.MSFT.NET\",\n      \"NS4.MSFT.NET\",\n      \"ns3.msft.net\",\n      \"ns1.msft.net\",\n      \"ns4.msft.net\",\n      \"ns2.msft.net\"\n   ],\n   \"emails\": &#91;\n      \"abusecomplaints@markmonitor.com\",\n      \"domains@microsoft.com\",\n      \"msnhst@microsoft.com\",\n      \"whoisrelay@markmonitor.com\"\n   ],\n}</code></pre>\n","protected":false},"excerpt":{"rendered":"<p>With Python, we can scrape any website or particular elements of a web page but do you have any idea whether it is legal or not? Before scraping any website we must have to know about the legality of web scraping. This chapter will explain the concepts related to legality of web scraping. Introduction Generally, [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1810,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/959"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=959"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/959/revisions"}],"predecessor-version":[{"id":1821,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/959/revisions/1821"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1810"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=959"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=959"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=959"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":961,"date":"2020-05-21T05:54:09","date_gmt":"2020-05-21T05:54:09","guid":{"rendered":"http://python3.foobrdigital.com/?p=961"},"modified":"2020-12-16T16:48:52","modified_gmt":"2020-12-16T16:48:52","slug":"data-extraction","status":"publish","type":"post","link":"https://python3.foobrdigital.com/data-extraction/","title":{"rendered":"Data Extraction"},"content":{"rendered":"\n<p>Analyzing a web page means understanding its structure . Now, the question arises why it is important for web scraping? In this chapter, let us understand this in detail.</p>\n\n\n\n<h2>Web page Analysis</h2>\n\n\n\n<p>Web page analysis is important because without analyzing we are not able to know in which form we are going to receive the data from (structured or unstructured) that web page after extraction. We can do web page analysis in the following ways −</p>\n\n\n\n<h3>Viewing Page Source</h3>\n\n\n\n<p>This is a way to understand how a web page is structured by examining its source code. To implement this, we need to right click the page and then must select the&nbsp;<strong>View page source</strong>&nbsp;option. Then, we will get the data of our interest from that web page in the form of HTML. But the main concern is about whitespaces and formatting which is difficult for us to format.</p>\n\n\n\n<h3>Inspecting Page Source by Clicking Inspect Element Option</h3>\n\n\n\n<p>This is another way of analyzing web page. But the difference is that it will resolve the issue of formatting and whitespaces in the source code of web page. You can implement this by right clicking and then selecting the&nbsp;<strong>Inspect</strong>&nbsp;or&nbsp;<strong>Inspect element</strong>&nbsp;option from menu. It will provide the information about particular area or element of that web page.</p>\n\n\n\n<h2>Different Ways to Extract Data from Web Page</h2>\n\n\n\n<p>The following methods are mostly used for extracting data from a web page −</p>\n\n\n\n<h3>Regular Expression</h3>\n\n\n\n<p>They are highly specialized programming language embedded in Python. We can use it through&nbsp;<strong>re</strong>&nbsp;module of Python. It is also called RE or regexes or regex patterns. With the help of regular expressions, we can specify some rules for the possible set of strings we want to match from the data.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In the following example, we are going to scrape data about India from&nbsp;http://example.webscraping.com&nbsp;after matching the contents of &lt;td&gt; with the help of regular expression.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import re\nimport urllib.request\nresponse =\n   urllib.request.urlopen('http://example.webscraping.com/places/default/view/India-102')\nhtml = response.read()\ntext = html.decode()\nre.findall('&lt;td class=\"w2p_fw\">(.*?)&lt;/td>',text)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<p>The corresponding output will be as shown here −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;\n   '&lt;img src=\"/places/static/images/flags/in.png\" />',\n   '3,287,590 square kilometres',\n   '1,173,108,018',\n   'IN',\n   'India',\n   'New Delhi',\n   '&lt;a href=\"/places/default/continent/AS\">AS&lt;/a>',\n   '.in',\n   'INR',\n   'Rupee',\n   '91',\n   '######',\n   '^(\\\\d{6})$',\n   'enIN,hi,bn,te,mr,ta,ur,gu,kn,ml,or,pa,as,bh,sat,ks,ne,sd,kok,doi,mni,sit,sa,fr,lus,inc',\n   '&lt;div>\n      &lt;a href=\"/places/default/iso/CN\">CN &lt;/a>\n      &lt;a href=\"/places/default/iso/NP\">NP &lt;/a>\n      &lt;a href=\"/places/default/iso/MM\">MM &lt;/a>\n      &lt;a href=\"/places/default/iso/BT\">BT &lt;/a>\n      &lt;a href=\"/places/default/iso/PK\">PK &lt;/a>\n      &lt;a href=\"/places/default/iso/BD\">BD &lt;/a>\n   &lt;/div>'\n]\n﻿</code></pre>\n\n\n\n<p>Observe that in the above output you can see the details about country India by using regular expression.</p>\n\n\n\n<h2>Beautiful Soup</h2>\n\n\n\n<p>Suppose we want to collect all the hyperlinks from a web page, then we can use a parser called BeautifulSoup which can be known in more detail at&nbsp;https://www.crummy.com/software/BeautifulSoup/bs4/doc/.&nbsp;In simple words, BeautifulSoup is a Python library for pulling data out of HTML and XML files. It can be used with requests, because it needs an input (document or url) to create a soup object asit cannot fetch a web page by itself. You can use the following Python script to gather the title of web page and hyperlinks.</p>\n\n\n\n<h3>Installing Beautiful Soup</h3>\n\n\n\n<p>Using the&nbsp;<strong>pip</strong>&nbsp;command, we can install&nbsp;<strong>beautifulsoup</strong>&nbsp;either in our virtual environment or in global installation.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>pip install bs4\nCollecting bs4\n   Downloading\nhttps:&#47;&#47;files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89\na94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\nRequirement already satisfied: beautifulsoup4 in d:\\programdata\\lib\\sitepackages\n(from bs4) (4.6.0)\nBuilding wheels for collected packages: bs4\n   Running setup.py bdist_wheel for bs4 ... done\n   Stored in directory:\nC:\\Users\\gaurav\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d\n52235414c3467d8889be38dd472\nSuccessfully built bs4\nInstalling collected packages: bs4\nSuccessfully installed bs4-0.0.1</code></pre>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>Note that in this example, we are extending the above example implemented with requests python module. we are using&nbsp;<strong>r.text</strong>&nbsp;for creating a soup object which will further be used to fetch details like title of the webpage.</p>\n\n\n\n<p>First, we need to import necessary Python modules −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nfrom bs4 import BeautifulSoup</code></pre>\n\n\n\n<p>In this following line of code we use requests to make a GET HTTP requests for the url:&nbsp;https://authoraditiagarwal.com/&nbsp;by making a GET request.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>r = requests.get('https://authoraditiagarwal.com/')</code></pre>\n\n\n\n<p>Now we need to create a Soup object as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>soup = BeautifulSoup(r.text, 'lxml')\nprint (soup.title)\nprint (soup.title.text)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<p>The corresponding output will be as shown here −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&lt;title>Learn and Grow with Aditi Agarwal&lt;/title>\nLearn and Grow with Aditi Agarwal</code></pre>\n\n\n\n<h2>Lxml</h2>\n\n\n\n<p>Another Python library we are going to discuss for web scraping is lxml. It is a highperformance HTML and XML parsing library. It is comparatively fast and straightforward. You can read about it more on&nbsp;https://lxml.de/.</p>\n\n\n\n<h3>Installing lxml</h3>\n\n\n\n<p>Using the pip command, we can install&nbsp;<strong>lxml</strong>&nbsp;either in our virtual environment or in global installation.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>(base) D:\\ProgramData>pip install lxml\nCollecting lxml\n   Downloading\nhttps:&#47;&#47;files.pythonhosted.org/packages/b9/55/bcc78c70e8ba30f51b5495eb0e\n3e949aa06e4a2de55b3de53dc9fa9653fa/lxml-4.2.5-cp36-cp36m-win_amd64.whl\n(3.\n6MB)\n   100% |¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 3.6MB 64kB/s\nInstalling collected packages: lxml\nSuccessfully installed lxml-4.2.5</code></pre>\n\n\n\n<h3>Example: Data extraction using lxml and requests</h3>\n\n\n\n<p>In the following example, we are scraping a particular element of the web page from&nbsp;<strong>authoraditiagarwal.com</strong>&nbsp;by using lxml and requests −</p>\n\n\n\n<p>First, we need to import the requests and html from lxml library as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nfrom lxml import html </code></pre>\n\n\n\n<p>Now we need to provide the url of web page to scrap</p>\n\n\n\n<pre class=\"wp-block-code\"><code>url = 'https://authoraditiagarwal.com/leadershipmanagement/'</code></pre>\n\n\n\n<p>Now we need to provide the path&nbsp;<strong>(Xpath)</strong>&nbsp;to particular element of that web page −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = '//*&#91;@id=\"panel-836-0-0-1\"]/div/div/p&#91;1]'\nresponse = requests.get(url)\nbyte_string = response.content\nsource_code = html.fromstring(byte_string)\ntree = source_code.xpath(path)\nprint(tree&#91;0].text_content()) </code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<p>The corresponding output will be as shown here −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>The Sprint Burndown or the Iteration Burndown chart is a powerful tool to communicate\ndaily progress to the stakeholders. It tracks the completion of work for a given sprint\nor an iteration. The horizontal axis represents the days within a Sprint. The vertical \naxis represents the hours remaining to complete the committed work.</code></pre>\n","protected":false},"excerpt":{"rendered":"<p>Analyzing a web page means understanding its structure . Now, the question arises why it is important for web scraping? In this chapter, let us understand this in detail. Web page Analysis Web page analysis is important because without analyzing we are not able to know in which form we are going to receive the [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1811,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/961"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=961"}],"version-history":[{"count":5,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/961/revisions"}],"predecessor-version":[{"id":2498,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/961/revisions/2498"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1811"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=961"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=961"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=961"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":970,"date":"2020-05-21T06:01:20","date_gmt":"2020-05-21T06:01:20","guid":{"rendered":"http://python3.foobrdigital.com/?p=970"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"data-processing","status":"publish","type":"post","link":"https://python3.foobrdigital.com/data-processing/","title":{"rendered":"Data Processing"},"content":{"rendered":"\n<p>In earlier chapters, we learned about extracting the data from web pages or web scraping by various Python modules. In this chapter, let us look into various techniques to process the data that has been scraped.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>To process the data that has been scraped, we must store the data on our local machine in a particular format like spreadsheet (CSV), JSON or sometimes in databases like MySQL.</p>\n\n\n\n<h2>CSV and JSON Data Processing</h2>\n\n\n\n<p>First, we are going to write the information, after grabbing from web page, into a CSV file or a spreadsheet. Let us first understand through a simple example in which we will first grab the information using&nbsp;<strong>BeautifulSoup</strong>&nbsp;module, as did earlier, and then by using Python CSV module we will write that textual information into CSV file.</p>\n\n\n\n<p>First, we need to import the necessary Python libraries as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nfrom bs4 import BeautifulSoup\nimport csv</code></pre>\n\n\n\n<p>In this following line of code, we use requests to make a GET HTTP requests for the url:&nbsp;https://authoraditiagarwal.com/&nbsp;by making a GET request.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>r = requests.get('https://authoraditiagarwal.com/')</code></pre>\n\n\n\n<p>Now, we need to create a Soup object as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>soup = BeautifulSoup(r.text, 'lxml')</code></pre>\n\n\n\n<p>Now, with the help of next lines of code, we will write the grabbed data into a CSV file named dataprocessing.csv.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>f = csv.writer(open(' dataprocessing.csv ','w'))\nf.writerow(&#91;'Title'])\nf.writerow(&#91;soup.title.text])\n﻿</code></pre>\n\n\n\n<p>After running this script, the textual information or the title of the webpage will be saved in the above mentioned CSV file on your local machine.</p>\n\n\n\n<p>Similarly, we can save the collected information in a JSON file. The following is an easy to understand Python script for doing the same in which we are grabbing the same information as we did in last Python script, but this time the grabbed information is saved in JSONfile.txt by using JSON Python module.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport json\nr = requests.get('https://authoraditiagarwal.com/')\nsoup = BeautifulSoup(r.text, 'lxml')\ny = json.dumps(soup.title.text)\nwith open('JSONFile.txt', 'wt') as outfile:\n   json.dump(y, outfile)</code></pre>\n\n\n\n<p>After running this script, the grabbed information i.e. title of the webpage will be saved in the above mentioned text file on your local machine.</p>\n\n\n\n<h2>Data Processing using AWS S3</h2>\n\n\n\n<p>Sometimes we may want to save scraped data in our local storage for archive purpose. But what if the we need to store and analyze this data at a massive scale? The answer is cloud storage service named Amazon S3 or AWS S3 (Simple Storage Service). Basically AWS S3 is an object storage which is built to store and retrieve any amount of data from anywhere.</p>\n\n\n\n<p>We can follow the following steps for storing data in AWS S3 −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− First we need an AWS account which will provide us the secret keys for using in our Python script while storing the data. It will create a S3 bucket in which we can store our data.</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Next, we need to install&nbsp;<strong>boto3</strong>&nbsp;Python library for accessing S3 bucket. It can be installed with the help of the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install boto3</code></pre>\n\n\n\n<p><strong>Step 3</strong>&nbsp;− Next, we can use the following Python script for scraping data from web page and saving it to AWS S3 bucket.</p>\n\n\n\n<p>First, we need to import Python libraries for scraping, here we are working with&nbsp;<strong>requests</strong>, and&nbsp;<strong>boto3</strong>&nbsp;saving data to S3 bucket.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nimport boto3</code></pre>\n\n\n\n<p>Now we can scrape the data from our URL.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>data = requests.get(\"Enter the URL\").text</code></pre>\n\n\n\n<p>Now for storing data to S3 bucket, we need to create S3 client as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>s3 = boto3.client('s3')\nbucket_name = \"our-content\"</code></pre>\n\n\n\n<p>Next line of code will create S3 bucket as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>s3.create_bucket(Bucket = bucket_name, ACL = 'public-read')\ns3.put_object(Bucket = bucket_name, Key = '', Body = data, ACL = \"public-read\")</code></pre>\n\n\n\n<p>Now you can check the bucket with name our-content from your AWS account.</p>\n\n\n\n<h2>Data processing using MySQL</h2>\n\n\n\n<p>Let us learn how to process data using MySQL. If you want to learn about MySQL, then you can follow the link <a rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://www.tutorialspoint.com/mysql/index.htm\"><a href=\"https://codemeals.com/\">https://codemeals.com/</a></a></p>\n\n\n\n<p>With the help of following steps, we can scrape and process data into MySQL table −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− First, by using MySQL we need to create a database and table in which we want to save our scraped data. For example, we are creating the table with following query −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>CREATE TABLE Scrap_pages (id BIGINT(7) NOT NULL AUTO_INCREMENT,\ntitle VARCHAR(200), content VARCHAR(10000),PRIMARY KEY(id));</code></pre>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Next, we need to deal with Unicode. Note that MySQL does not handle Unicode by default. We need to turn on this feature with the help of following commands which will change the default character set for the database, for the table and for both of the columns −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>ALTER DATABASE scrap CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;\nALTER TABLE Scrap_pages CONVERT TO CHARACTER SET utf8mb4 COLLATE\nutf8mb4_unicode_ci;\nALTER TABLE Scrap_pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4\nCOLLATE utf8mb4_unicode_ci;\nALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4\nCOLLATE utf8mb4_unicode_ci;</code></pre>\n\n\n\n<p><strong>Step 3</strong>&nbsp;− Now, integrate MySQL with Python. For this, we will need PyMySQL which can be installed with the help of the following command</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install PyMySQL \n﻿</code></pre>\n\n\n\n<p><strong>Step 4</strong>&nbsp;− Now, our database named Scrap, created earlier, is ready to save the data, after scraped from web, into table named Scrap_pages. Here in our example we are going to scrape data from Wikipedia and it will be saved into our database.</p>\n\n\n\n<p>First, we need to import the required Python modules.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport datetime\nimport random\nimport pymysql\nimport re</code></pre>\n\n\n\n<p>Now, make a connection, that is integrate this with Python.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>conn = pymysql.connect(host='127.0.0.1',user='root', passwd = None, db = 'mysql',\ncharset = 'utf8')\ncur = conn.cursor()\ncur.execute(\"USE scrap\")\nrandom.seed(datetime.datetime.now())\ndef store(title, content):\n   cur.execute('INSERT INTO scrap_pages (title, content) VALUES ''(\"%s\",\"%s\")', (title, content))\n   cur.connection.commit()</code></pre>\n\n\n\n<p>Now, connect with Wikipedia and get data from it.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>def getLinks(articleUrl):\n   html = urlopen('http://en.wikipedia.org'+articleUrl)\n   bs = BeautifulSoup(html, 'html.parser')\n   title = bs.find('h1').get_text()\n   content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n   store(title, content)\n   return bs.find('div', {'id':'bodyContent'}).findAll('a',href=re.compile('^(/wiki/)((?!:).)*$'))\nlinks = getLinks('/wiki/Kevin_Bacon')\ntry:\n   while len(links) > 0:\n      newArticle = links&#91;random.randint(0, len(links)-1)].attrs&#91;'href']\n      print(newArticle)\n      links = getLinks(newArticle)</code></pre>\n\n\n\n<p>Lastly, we need to close both cursor and connection.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>finally:\n   cur.close()\n   conn.close()</code></pre>\n\n\n\n<p>This will save the data gather from Wikipedia into table named scrap_pages. If you are familiar with MySQL and web scraping, then the above code would not be tough to understand.</p>\n\n\n\n<h2>Data processing using PostgreSQL</h2>\n\n\n\n<p>PostgreSQL, developed by a worldwide team of volunteers, is an open source relational database Management system (RDMS). The process of processing the scraped data using PostgreSQL is similar to that of MySQL. There would be two changes: First, the commands would be different to MySQL and second, here we will use&nbsp;<strong>psycopg2</strong>&nbsp;Python library to perform its integration with Python.</p>\n\n\n\n<p>If you are not familiar with PostgreSQL then you can learn it at <a href=\"https://codemeals.com/\">https://codemeals.com/</a>.&nbsp;And with the help of following command we can install psycopg2 Python library −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install psycopg2 </code></pre>\n","protected":false},"excerpt":{"rendered":"<p>In earlier chapters, we learned about extracting the data from web pages or web scraping by various Python modules. In this chapter, let us look into various techniques to process the data that has been scraped. Introduction To process the data that has been scraped, we must store the data on our local machine in [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1812,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/970"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=970"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/970/revisions"}],"predecessor-version":[{"id":2463,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/970/revisions/2463"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1812"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=970"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=970"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=970"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":982,"date":"2020-05-21T06:08:47","date_gmt":"2020-05-21T06:08:47","guid":{"rendered":"http://python3.foobrdigital.com/?p=982"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"processing-images-and-videos","status":"publish","type":"post","link":"https://python3.foobrdigital.com/processing-images-and-videos/","title":{"rendered":"Processing Images and Videos"},"content":{"rendered":"\n<p>Web scraping usually involves downloading, storing and processing the web media content. In this chapter, let us understand how to process the content downloaded from the web.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>The web media content that we obtain during scraping can be images, audio and video files, in the form of non-web pages as well as data files. But, can we trust the downloaded data especially on the extension of data we are going to download and store in our computer memory? This makes it essential to know about the type of data we are going to store locally.</p>\n\n\n\n<h2>Getting Media Content from Web Page</h2>\n\n\n\n<p>In this section, we are going to learn how we can download media content which correctly represents the media type based on the information from web server. We can do it with the help of Python&nbsp;<strong>requests</strong>&nbsp;module as we did in previous chapter.</p>\n\n\n\n<p>First, we need to import necessary Python modules as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests</code></pre>\n\n\n\n<p>Now, provide the URL of the media content we want to download and store locally.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>url = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"</code></pre>\n\n\n\n<p>Use the following code to create HTTP response object.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>r = requests.get(url) </code></pre>\n\n\n\n<p>With the help of following line of code, we can save the received content as .png file.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>with open(\"ThinkBig.png\",'wb') as f:\n   f.write(r.content) </code></pre>\n\n\n\n<p>After running the above Python script, we will get a file named ThinkBig.png, which would have the downloaded image.</p>\n\n\n\n<h2>Extracting Filename from URL</h2>\n\n\n\n<p>After downloading the content from web site, we also want to save it in a file with a file name found in the URL. But we can also check, if numbers of additional fragments exist in URL too. For this, we need to find the actual filename from the URL.</p>\n\n\n\n<p>With the help of following Python script, using&nbsp;<strong>urlparse</strong>, we can extract the filename from URL −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import urllib3\nimport os\nurl = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"\na = urlparse(url)\na.path</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>'/wp-content/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg'\nos.path.basename(a.path)</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>'MetaSlider_ThinkBig-1080x180.jpg'</code></pre>\n\n\n\n<p>Once you run the above script, we will get the filename from URL.</p>\n\n\n\n<h2>Information about Type of Content from URL</h2>\n\n\n\n<p>While extracting the contents from web server, by GET request, we can also check its information provided by the web server. With the help of following Python script we can determine what web server means with the type of the content −</p>\n\n\n\n<p>First, we need to import necessary Python modules as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests</code></pre>\n\n\n\n<p>Now, we need to provide the URL of the media content we want to download and store locally.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>url = \"https://authoraditiagarwal.com/wpcontent/uploads/2018/05/MetaSlider_ThinkBig-1080x180.jpg\"</code></pre>\n\n\n\n<p>Following line of code will create HTTP response object.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>r = requests.get(url, allow_redirects=True)</code></pre>\n\n\n\n<p>Now, we can get what type of information about content can be provided by web server.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>for headers in r.headers: print(headers)</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Date\nServer\nUpgrade\nConnection\nLast-Modified\nAccept-Ranges\nContent-Length\nKeep-Alive\nContent-Type</code></pre>\n\n\n\n<p>With the help of following line of code we can get the particular information about content type, say content-type −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print (r.headers.get('content-type'))</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>image/jpeg</code></pre>\n\n\n\n<p>With the help of following line of code, we can get the particular information about content type, say EType −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print (r.headers.get('ETag'))</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>None</code></pre>\n\n\n\n<p>Observe the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print (r.headers.get('content-length'))</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>12636</code></pre>\n\n\n\n<p>With the help of following line of code we can get the particular information about content type, say Server −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print (r.headers.get('Server'))</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Apache</code></pre>\n\n\n\n<h2>Generating Thumbnail for Images</h2>\n\n\n\n<p>Thumbnail is a very small description or representation. A user may want to save only thumbnail of a large image or save both the image as well as thumbnail. In this section we are going to create a thumbnail of the image named&nbsp;<strong>ThinkBig.png</strong>&nbsp;downloaded in the previous section “Getting media content from web page”.</p>\n\n\n\n<p>For this Python script, we need to install Python library named Pillow, a fork of the Python Image library having useful functions for manipulating images. It can be installed with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install pillow</code></pre>\n\n\n\n<p>The following Python script will create a thumbnail of the image and will save it to the current directory by prefixing thumbnail file with&nbsp;<strong>Th_</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>import glob\nfrom PIL import Image\nfor infile in glob.glob(\"ThinkBig.png\"):\n   img = Image.open(infile)\n   img.thumbnail((128, 128), Image.ANTIALIAS)\n   if infile&#91;0:2] != \"Th_\":\n      img.save(\"Th_\" + infile, \"png\")</code></pre>\n\n\n\n<p>The above code is very easy to understand and you can check for the thumbnail file in the current directory.</p>\n\n\n\n<h2>Screenshot from Website</h2>\n\n\n\n<p>In web scraping, a very common task is to take screenshot of a website. For implementing this, we are going to use selenium and webdriver. The following Python script will take the screenshot from website and will save it to current directory.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>From selenium import webdriver\npath = r'C:\\\\Users\\\\gaurav\\\\Desktop\\\\Chromedriver'\nbrowser = webdriver.Chrome(executable_path = path)\nbrowser.get('https://tutorialspoint.com/')\nscreenshot = browser.save_screenshot('screenshot.png')\nbrowser.quit</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>DevTools listening on ws://127.0.0.1:1456/devtools/browser/488ed704-9f1b-44f0-\na571-892dc4c90eb7\n&lt;bound method WebDriver.quit of &lt;selenium.webdriver.chrome.webdriver.WebDriver\n(session=\"37e8e440e2f7807ef41ca7aa20ce7c97\")>></code></pre>\n\n\n\n<p>After running the script, you can check your current directory for&nbsp;<strong>screenshot.png</strong>&nbsp;file.</p>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://www.tutorialspoint.com/python_web_scraping/images/screenshot.jpg\" alt=\"Screenshot\"/></figure>\n\n\n\n<h2>Thumbnail Generation for Video</h2>\n\n\n\n<p>Suppose we have downloaded videos from website and wanted to generate thumbnails for them so that a specific video, based on its thumbnail, can be clicked. For generating thumbnail for videos we need a simple tool called&nbsp;<strong>ffmpeg</strong>&nbsp;which can be downloaded from&nbsp;<strong>www.ffmpeg.org</strong>. After downloading, we need to install it as per the specifications of our OS.</p>\n\n\n\n<p>The following Python script will generate thumbnail of the video and will save it to our local directory −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import subprocess\nvideo_MP4_file = “C:\\Users\\gaurav\\desktop\\solar.mp4\nthumbnail_image_file = 'thumbnail_solar_video.jpg'\nsubprocess.call(&#91;'ffmpeg', '-i', video_MP4_file, '-ss', '00:00:20.000', '-\n   vframes', '1', thumbnail_image_file, \"-y\"]) </code></pre>\n\n\n\n<p>After running the above script, we will get the thumbnail named&nbsp;<strong>thumbnail_solar_video.jpg</strong>&nbsp;saved in our local directory.</p>\n\n\n\n<h2>Ripping an MP4 video to an MP3</h2>\n\n\n\n<p>Suppose you have downloaded some video file from a website, but you only need audio from that file to serve your purpose, then it can be done in Python with the help of Python library called&nbsp;<strong>moviepy</strong>&nbsp;which can be installed with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install moviepy</code></pre>\n\n\n\n<p>Now, after successfully installing moviepy with the help of following script we can convert and MP4 to MP3.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import moviepy.editor as mp\nclip = mp.VideoFileClip(r\"C:\\Users\\gaurav\\Desktop\\1234.mp4\")\nclip.audio.write_audiofile(\"movie_audio.mp3\")</code></pre>\n\n\n\n<p>You can observe the output as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;MoviePy] Writing audio in movie_audio.mp3\n100%|¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦\n¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦¦| 674/674 &#91;00:01&lt;00:00,\n476.30it/s]\n&#91;MoviePy] Done.</code></pre>\n\n\n\n<p>The above script will save the audio MP3 file in the local directory.</p>\n","protected":false},"excerpt":{"rendered":"<p>Web scraping usually involves downloading, storing and processing the web media content. In this chapter, let us understand how to process the content downloaded from the web. Introduction The web media content that we obtain during scraping can be images, audio and video files, in the form of non-web pages as well as data files. [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1813,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/982"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=982"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/982/revisions"}],"predecessor-version":[{"id":2461,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/982/revisions/2461"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1813"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=982"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=982"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=982"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1005,"date":"2020-05-21T06:17:41","date_gmt":"2020-05-21T06:17:41","guid":{"rendered":"http://python3.foobrdigital.com/?p=1005"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"dealing-with-text","status":"publish","type":"post","link":"https://python3.foobrdigital.com/dealing-with-text/","title":{"rendered":"Dealing with Text"},"content":{"rendered":"\n<p>In the previous chapter, we have seen how to deal with videos and images that we obtain as a part of web scraping content. In this chapter we are going to deal with text analysis by using Python library and will learn about this in detail.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>You can perform text analysis in by using Python library called Natural Language Tool Kit (NLTK). Before proceeding into the concepts of NLTK, let us understand the relation between text analysis and web scraping.</p>\n\n\n\n<p>Analyzing the words in the text can lead us to know about which words are important, which words are unusual, how words are grouped. This analysis eases the task of web scraping.</p>\n\n\n\n<h2>Getting started with NLTK</h2>\n\n\n\n<p>The Natural language toolkit (NLTK) is collection of Python libraries which is designed especially for identifying and tagging parts of speech found in the text of natural language like English.</p>\n\n\n\n<h3>Installing NLTK</h3>\n\n\n\n<p>You can use the following command to install NLTK in Python −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install nltk</code></pre>\n\n\n\n<p>If you are using Anaconda, then a conda package for NLTK can be built by using the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>conda install -c anaconda nltk</code></pre>\n\n\n\n<h3>Downloading NLTK’s Data</h3>\n\n\n\n<p>After installing NLTK, we have to download preset text repositories. But before downloading text preset repositories, we need to import NLTK with the help of&nbsp;<strong>import</strong>&nbsp;command as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>mport nltk</code></pre>\n\n\n\n<p>Now, with the help of following command NLTK data can be downloaded −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>nltk.download()</code></pre>\n\n\n\n<p>Installation of all available packages of NLTK will take some time, but it is always recommended to install all the packages.</p>\n\n\n\n<h2>Installing Other Necessary packages</h2>\n\n\n\n<p>We also need some other Python packages like&nbsp;<strong>gensim</strong>&nbsp;and&nbsp;<strong>pattern</strong>&nbsp;for doing text analysis as well as building building natural language processing applications by using NLTK.</p>\n\n\n\n<p><strong>gensim</strong>&nbsp;− A robust semantic modeling library which is useful for many applications. It can be installed by the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install gensim</code></pre>\n\n\n\n<p><strong>pattern</strong>&nbsp;− Used to make&nbsp;<strong>gensim</strong>&nbsp;package work properly. It can be installed by the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install pattern</code></pre>\n\n\n\n<h2>Tokenization</h2>\n\n\n\n<p>The Process of breaking the given text, into the smaller units called tokens, is called tokenization. These tokens can be the words, numbers or punctuation marks. It is also called&nbsp;<strong>word segmentation</strong>.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<figure class=\"wp-block-image\"><img src=\"https://www.tutorialspoint.com/python_web_scraping/images/tokenization.jpg\" alt=\"Tokenization\"/></figure>\n\n\n\n<p>NLTK module provides different packages for tokenization. We can use these packages as per our requirement. Some of the packages are described here −</p>\n\n\n\n<p><strong>sent_tokenize package</strong>&nbsp;− This package will divide the input text into sentences. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.tokenize import sent_tokenize</code></pre>\n\n\n\n<p><strong>word_tokenize package</strong>&nbsp;− This package will divide the input text into words. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.tokenize import word_tokenize</code></pre>\n\n\n\n<p><strong>WordPunctTokenizer package</strong>&nbsp;− This package will divide the input text as well as the punctuation marks into words. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.tokenize import WordPuncttokenizer</code></pre>\n\n\n\n<h2>Stemming</h2>\n\n\n\n<p>In any language, there are different forms of a words. A language includes lots of variations due to the grammatical reasons. For example, consider the words&nbsp;<strong>democracy</strong>,&nbsp;<strong>democratic</strong>, and&nbsp;<strong>democratization</strong>. For machine learning as well as for web scraping projects, it is important for machines to understand that these different words have the same base form. Hence we can say that it can be useful to extract the base forms of the words while analyzing the text.</p>\n\n\n\n<p>This can be achieved by stemming which may be defined as the heuristic process of extracting the base forms of the words by chopping off the ends of words.</p>\n\n\n\n<p>NLTK module provides different packages for stemming. We can use these packages as per our requirement. Some of these packages are described here −</p>\n\n\n\n<p><strong>PorterStemmer package</strong>&nbsp;− Porter’s algorithm is used by this Python stemming package to extract the base form. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.stem.porter import PorterStemmer</code></pre>\n\n\n\n<p>For example, after giving the word&nbsp;<strong>‘writing’</strong>&nbsp;as the input to this stemmer, the output would be the word&nbsp;<strong>‘write’</strong>&nbsp;after stemming.</p>\n\n\n\n<p><strong>LancasterStemmer package</strong>&nbsp;− Lancaster’s algorithm is used by this Python stemming package to extract the base form. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.stem.lancaster import LancasterStemmer</code></pre>\n\n\n\n<p>For example, after giving the word&nbsp;<strong>‘writing’</strong>&nbsp;as the input to this stemmer then the output would be the word&nbsp;<strong>‘writ’</strong>&nbsp;after stemming.</p>\n\n\n\n<p><strong>SnowballStemmer package</strong>&nbsp;− Snowball’s algorithm is used by this Python stemming package to extract the base form. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.stem.snowball import SnowballStemmer</code></pre>\n\n\n\n<p>For example, after giving the word ‘writing’ as the input to this stemmer then the output would be the word ‘write’ after stemming.</p>\n\n\n\n<h2>Lemmatization</h2>\n\n\n\n<p>An other way to extract the base form of words is by lemmatization, normally aiming to remove inflectional endings by using vocabulary and morphological analysis. The base form of any word after lemmatization is called lemma.</p>\n\n\n\n<p>NLTK module provides following packages for lemmatization −</p>\n\n\n\n<p><strong>WordNetLemmatizer package</strong>&nbsp;− It will extract the base form of the word depending upon whether it is used as noun as a verb. You can use the following command to import this package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from nltk.stem import WordNetLemmatizer</code></pre>\n\n\n\n<h2>Chunking</h2>\n\n\n\n<p>Chunking, which means dividing the data into small chunks, is one of the important processes in natural language processing to identify the parts of speech and short phrases like noun phrases. Chunking is to do the labeling of tokens. We can get the structure of the sentence with the help of chunking process.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we are going to implement Noun-Phrase chunking by using NLTK Python module. NP chunking is a category of chunking which will find the noun phrases chunks in the sentence.</p>\n\n\n\n<h3>Steps for implementing noun phrase chunking</h3>\n\n\n\n<p>We need to follow the steps given below for implementing noun-phrase chunking −</p>\n\n\n\n<h3>Step 1 − Chunk grammar definition</h3>\n\n\n\n<p>In the first step we will define the grammar for chunking. It would consist of the rules which we need to follow.</p>\n\n\n\n<h3>Step 2 − Chunk parser creation</h3>\n\n\n\n<p>Now, we will create a chunk parser. It would parse the grammar and give the output.</p>\n\n\n\n<h3>Step 3 − The Output</h3>\n\n\n\n<p>In this last step, the output would be produced in a tree format.</p>\n\n\n\n<p>First, we need to import the NLTK package as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import nltk</code></pre>\n\n\n\n<p>Next, we need to define the sentence. Here DT: the determinant, VBP: the verb, JJ: the adjective, IN: the preposition and NN: the noun.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>sentence = &#91;(\"a\", \"DT\"),(\"clever\",\"JJ\"),(\"fox\",\"NN\"),(\"was\",\"VBP\"),(\"jumping\",\"VBP\"),(\"over\",\"IN\"),(\"the\",\"DT\"),(\"wall\",\"NN\")]</code></pre>\n\n\n\n<p>Next, we are giving the grammar in the form of regular expression.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>grammar = \"NP:{&lt;DT>?&lt;JJ>*&lt;NN>}\"</code></pre>\n\n\n\n<p>Now, next line of code will define a parser for parsing the grammar.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>parser_chunking = nltk.RegexpParser(grammar)</code></pre>\n\n\n\n<p>Now, the parser will parse the sentence.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>parser_chunking.parse(sentence)</code></pre>\n\n\n\n<p>Next, we are giving our output in the variable.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Output = parser_chunking.parse(sentence)</code></pre>\n\n\n\n<p>With the help of following code, we can draw our output in the form of a tree as shown below.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>output.draw()</code></pre>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter\"><img src=\"https://www.tutorialspoint.com/python_web_scraping/images/phrase_chunking.jpg\" alt=\"Phrase Chunking\"/></figure></div>\n\n\n\n<h2>Bag of Word (BoW) Model Extracting and converting the Text into Numeric Form</h2>\n\n\n\n<p>Bag of Word (BoW), a useful model in natural language processing, is basically used to extract the features from text. After extracting the features from the text, it can be used in modeling in machine learning algorithms because raw data cannot be used in ML applications.</p>\n\n\n\n<h3>Working of BoW Model</h3>\n\n\n\n<p>Initially, model extracts a vocabulary from all the words in the document. Later, using a document term matrix, it would build a model. In this way, BoW model represents the document as a bag of words only and the order or structure is discarded.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>Suppose we have the following two sentences −</p>\n\n\n\n<p><strong>Sentence1</strong>&nbsp;− This is an example of Bag of Words model.</p>\n\n\n\n<p><strong>Sentence2</strong>&nbsp;− We can extract features by using Bag of Words model.</p>\n\n\n\n<p>Now, by considering these two sentences, we have the following 14 distinct words −</p>\n\n\n\n<ul><li>This</li><li>is</li><li>an</li><li>example</li><li>bag</li><li>of</li><li>words</li><li>model</li><li>we</li><li>can</li><li>extract</li><li>features</li><li>by</li><li>using</li></ul>\n\n\n\n<h2>Building a Bag of Words Model in NLTK</h2>\n\n\n\n<p>Let us look into the following Python script which will build a BoW model in NLTK.</p>\n\n\n\n<p>First, import the following package −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.feature_extraction.text import CountVectorizer </code></pre>\n\n\n\n<p>Next, define the set of sentences −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Sentences=&#91;'This is an example of Bag of Words model.', ' We can extract\n   features by using Bag of Words model.']\n   vector_count = CountVectorizer()\n   features_text = vector_count.fit_transform(Sentences).todense()\n   print(vector_count.vocabulary_)\n﻿</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<p>It shows that we have 14 distinct words in the above two sentences −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>{\n   'this': 10, 'is': 7, 'an': 0, 'example': 4, 'of': 9, \n   'bag': 1, 'words': 13, 'model': 8, 'we': 12, 'can': 3, \n   'extract': 5, 'features': 6, 'by': 2, 'using':11\n}\n﻿</code></pre>\n\n\n\n<h2>Topic Modeling: Identifying Patterns in Text Data</h2>\n\n\n\n<p>Generally documents are grouped into topics and topic modeling is a technique to identify the patterns in a text that corresponds to a particular topic. In other words, topic modeling is used to uncover abstract themes or hidden structure in a given set of documents.</p>\n\n\n\n<p>You can use topic modeling in following scenarios −</p>\n\n\n\n<h3>Text Classification</h3>\n\n\n\n<p>Classification can be improved by topic modeling because it groups similar words together rather than using each word separately as a feature.</p>\n\n\n\n<h3>Recommender Systems</h3>\n\n\n\n<p>We can build recommender systems by using similarity measures.</p>\n\n\n\n<h2>Topic Modeling Algorithms</h2>\n\n\n\n<p>We can implement topic modeling by using the following algorithms −</p>\n\n\n\n<p><strong>Latent Dirichlet Allocation(LDA)</strong>&nbsp;− It is one of the most popular algorithm that uses the probabilistic graphical models for implementing topic modeling.</p>\n\n\n\n<p><strong>Latent Semantic Analysis(LDA) or Latent Semantic Indexing(LSI)</strong>&nbsp;− It is based upon Linear Algebra and uses the concept of SVD (Singular Value Decomposition) on document term matrix.</p>\n\n\n\n<p><strong>Non-Negative Matrix Factorization (NMF)</strong>&nbsp;− It is also based upon Linear Algebra as like LDA.</p>\n\n\n\n<p>The above mentioned algorithms would have the following elements −</p>\n\n\n\n<ul><li>Number of topics: Parameter</li><li>Document-Word Matrix: Input</li><li>WTM (Word Topic Matrix) &amp; TDM (Topic Document Matrix): Output</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>In the previous chapter, we have seen how to deal with videos and images that we obtain as a part of web scraping content. In this chapter we are going to deal with text analysis by using Python library and will learn about this in detail. Introduction You can perform text analysis in by using [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1814,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1005"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1005"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1005/revisions"}],"predecessor-version":[{"id":2460,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1005/revisions/2460"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1814"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1005"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1005"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1005"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1015,"date":"2020-05-21T06:21:46","date_gmt":"2020-05-21T06:21:46","guid":{"rendered":"http://python3.foobrdigital.com/?p=1015"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"dynamic-websites","status":"publish","type":"post","link":"https://python3.foobrdigital.com/dynamic-websites/","title":{"rendered":"Dynamic Websites"},"content":{"rendered":"\n<p>In this chapter, let us learn how to perform web scraping on dynamic websites and the concepts involved in detail.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>Web scraping is a complex task and the complexity multiplies if the website is dynamic. According to United Nations Global Audit of Web Accessibility more than 70% of the websites are dynamic in nature and they rely on JavaScript for their functionalities.</p>\n\n\n\n<h2>Dynamic Website Example</h2>\n\n\n\n<p>Let us look at an example of a dynamic website and know about why it is difficult to scrape. Here we are going to take example of searching from a website named <strong>http://example.webscraping.com/places/default/search</strong>. But how can we say that this website is of dynamic nature? It can be judged from the output of following Python script which will try to scrape data from above mentioned webpage −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import re\nimport urllib.request\nresponse = urllib.request.urlopen('http://example.webscraping.com/places/default/search')\nhtml = response.read()\ntext = html.decode()\nre.findall('(.*?)',text)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91; ]</code></pre>\n\n\n\n<p>The above output shows that the example scraper failed to extract information because the &lt;div&gt; element we are trying to find is empty.</p>\n\n\n\n<h2>Approaches for Scraping data from Dynamic Websites</h2>\n\n\n\n<p>We have seen that the scraper cannot scrape the information from a dynamic website because the data is loaded dynamically with JavaScript. In such cases, we can use the following two techniques for scraping data from dynamic JavaScript dependent websites −</p>\n\n\n\n<ul><li>Reverse Engineering JavaScript</li><li>Rendering JavaScript</li></ul>\n\n\n\n<h2>Reverse Engineering JavaScript</h2>\n\n\n\n<p>The process called reverse engineering would be useful and lets us understand how data is loaded dynamically by web pages.</p>\n\n\n\n<p>For doing this, we need to click the&nbsp;<strong>inspect element</strong>&nbsp;tab for a specified URL. Next, we will click&nbsp;<strong>NETWORK</strong>&nbsp;tab to find all the requests made for that web page including search.json with a path of&nbsp;<strong>/ajax</strong>. Instead of accessing AJAX data from browser or via NETWORK tab, we can do it with the help of following Python script too −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nurl=requests.get('http://example.webscraping.com/ajax/search.json?page=0&amp;page_size=10&amp;search_term=a')\nurl.json() </code></pre>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>The above script allows us to access JSON response by using Python json method. Similarly we can download the raw string response and by using python’s json.loads method, we can load it too. We are doing this with the help of following Python script. It will basically scrape all of the countries by searching the letter of the alphabet ‘a’ and then iterating the resulting pages of the JSON responses.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nimport string\nPAGE_SIZE = 15\nurl = 'http://example.webscraping.com/ajax/' + 'search.json?page={}&amp;page_size={}&amp;search_term=a'\ncountries = set()\nfor letter in string.ascii_lowercase:\n   print('Searching with %s' % letter)\n   page = 0\n   while True:\n   response = requests.get(url.format(page, PAGE_SIZE, letter))\n   data = response.json()\n   print('adding %d records from the page %d' %(len(data.get('records')),page))\n   for record in data.get('records'):countries.add(record&#91;'country'])\n   page += 1\n   if page >= data&#91;'num_pages']:\n      break\n   with open('countries.txt', 'w') as countries_file:\n   countries_file.write('n'.join(sorted(countries))) </code></pre>\n\n\n\n<p>After running the above script, we will get the following output and the records would be saved in the file named countries.txt.</p>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>Searching with a\nadding 15 records from the page 0\nadding 15 records from the page 1\n...</code></pre>\n\n\n\n<h2>Rendering JavaScript</h2>\n\n\n\n<p>In the previous section, we did reverse engineering on web page that how API worked and how we can use it to retrieve the results in single request. However, we can face following difficulties while doing reverse engineering −</p>\n\n\n\n<ul><li>Sometimes websites can be very difficult. For example, if the website is made with advanced browser tool such as Google Web Toolkit (GWT), then the resulting JS code would be machine-generated and difficult to understand and reverse engineer.</li><li>Some higher level frameworks like&nbsp;<strong>React.js</strong>&nbsp;can make reverse engineering difficult by abstracting already complex JavaScript logic.</li></ul>\n\n\n\n<p>The solution to the above difficulties is to use a browser rendering engine that parses HTML, applies the CSS formatting and executes JavaScript to display a web page.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, for rendering Java Script we are going to use a familiar Python module Selenium. The following Python code will render a web page with the help of Selenium −</p>\n\n\n\n<p>First, we need to import webdriver from selenium as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from selenium import webdriver</code></pre>\n\n\n\n<p>Now, provide the path of web driver which we have downloaded as per our requirement −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r'C:\\\\Users\\\\gaurav\\\\Desktop\\\\Chromedriver'\ndriver = webdriver.Chrome(executable_path = path)</code></pre>\n\n\n\n<p>Now, provide the url which we want to open in that web browser now controlled by our Python script.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>driver.get('http://example.webscraping.com/search')</code></pre>\n\n\n\n<p>Now, we can use ID of the search toolbox for setting the element to select.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>driver.find_element_by_id('search_term').send_keys('.')</code></pre>\n\n\n\n<p>Next, we can use java script to set the select box content as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>js = \"document.getElementById('page_size').options&#91;1].text = '100';\"\ndriver.execute_script(js)</code></pre>\n\n\n\n<p>The following line of code shows that search is ready to be clicked on the web page −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>driver.find_element_by_id('search').click()</code></pre>\n\n\n\n<p>Next line of code shows that it will wait for 45 seconds for completing the AJAX request.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>driver.implicitly_wait(45)</code></pre>\n\n\n\n<p>Now, for selecting country links, we can use the CSS selector as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>links = driver.find_elements_by_css_selector('#results a')</code></pre>\n\n\n\n<p>Now the text of each link can be extracted for creating the list of countries −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>countries = &#91;link.text for link in links]\nprint(countries)\ndriver.close()</code></pre>\n","protected":false},"excerpt":{"rendered":"<p>In this chapter, let us learn how to perform web scraping on dynamic websites and the concepts involved in detail. Introduction Web scraping is a complex task and the complexity multiplies if the website is dynamic. According to United Nations Global Audit of Web Accessibility more than 70% of the websites are dynamic in nature [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1815,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1015"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1015"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1015/revisions"}],"predecessor-version":[{"id":2458,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1015/revisions/2458"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1815"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1015"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1015"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1015"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1019,"date":"2020-05-21T06:25:18","date_gmt":"2020-05-21T06:25:18","guid":{"rendered":"http://python3.foobrdigital.com/?p=1019"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"form-based-websites","status":"publish","type":"post","link":"https://python3.foobrdigital.com/form-based-websites/","title":{"rendered":"Form based Websites"},"content":{"rendered":"\n<p>In the previous chapter, we have seen scraping dynamic websites. In this chapter, let us understand scraping of websites that work on user based inputs, that is form based websites.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>These days WWW (World Wide Web) is moving towards social media as well as usergenerated contents. So the question arises how we can access such kind of information that is beyond login screen? For this we need to deal with forms and logins.</p>\n\n\n\n<p>In previous chapters, we worked with HTTP GET method to request information but in this chapter we will work with HTTP POST method that pushes information to a web server for storage and analysis.</p>\n\n\n\n<h2>Interacting with Login forms</h2>\n\n\n\n<p>While working on Internet, you must have interacted with login forms many times. They may be very simple like including only a very few HTML fields, a submit button and an action page or they may be complicated and have some additional fields like email, leave a message along with captcha for security reasons.</p>\n\n\n\n<p>In this section, we are going to deal with a simple submit form with the help of Python requests library.</p>\n\n\n\n<p>First, we need to import requests library as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests</code></pre>\n\n\n\n<p>Now, we need to provide the information for the fields of login form.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>parameters = {‘Name’:’Enter your name’, ‘Email-id’:’Your Emailid’,’Message’:’Type your message here’}</code></pre>\n\n\n\n<p>In next line of code, we need to provide the URL on which action of the form would happen.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>r = requests.post(“enter the URL”, data = parameters)\nprint(r.text)\n﻿</code></pre>\n\n\n\n<p>After running the script, it will return the content of the page where action has happened.</p>\n\n\n\n<p>Suppose if you want to submit any image with the form, then it is very easy with requests.post(). You can understand it with the help of following Python script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nfile = {‘Uploadfile’: open(’C:\\Usres\\desktop\\123.png’,‘rb’)}\nr = requests.post(“enter the URL”, files = file)\nprint(r.text)</code></pre>\n\n\n\n<h2>Loading Cookies from the Web Server</h2>\n\n\n\n<p>A cookie, sometimes called web cookie or internet cookie, is a small piece of data sent from a website and our computer stores it in a file located inside our web browser.</p>\n\n\n\n<p>In the context of dealings with login forms, cookies can be of two types. One, we dealt in the previous section, that allows us to submit information to a website and second which lets us to remain in a permanent “logged-in” state throughout our visit to the website. For the second kind of forms, websites use cookies to keep track of who is logged in and who is not.</p>\n\n\n\n<h3>What do cookies do?</h3>\n\n\n\n<p>These days most of the websites are using cookies for tracking. We can understand the working of cookies with the help of following steps −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− First, the site will authenticate our login credentials and stores it in our browser’s cookie. This cookie generally contains a server-generated toke, time-out and tracking information.</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Next, the website will use the cookie as a proof of authentication. This authentication is always shown whenever we visit the website.</p>\n\n\n\n<p>Cookies are very problematic for web scrapers because if web scrapers do not keep track of the cookies, the submitted form is sent back and at the next page it seems that they never logged in. It is very easy to track the cookies with the help of Python&nbsp;<strong>requests</strong>&nbsp;library, as shown below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nparameters = {‘Name’:’Enter your name’, ‘Email-id’:’Your Emailid’,’Message’:’Type your message here’}\nr = requests.post(“enter the URL”, data = parameters)</code></pre>\n\n\n\n<p>In the above line of code, the URL would be the page which will act as the processor for the login form.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(‘The cookie is:’)\nprint(r.cookies.get_dict())\nprint(r.text)</code></pre>\n\n\n\n<p>After running the above script, we will retrieve the cookies from the result of last request.</p>\n\n\n\n<p>There is another issue with cookies that sometimes websites frequently modify cookies without warning. Such kind of situation can be dealt with&nbsp;<strong>requests.Session()</strong>&nbsp;as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import requests\nsession = requests.Session()\nparameters = {‘Name’:’Enter your name’, ‘Email-id’:’Your Emailid’,’Message’:’Type your message here’}\nr = session.post(“enter the URL”, data = parameters)</code></pre>\n\n\n\n<p>In the above line of code, the URL would be the page which will act as the processor for the login form.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(‘The cookie is:’)\nprint(r.cookies.get_dict())\nprint(r.text)</code></pre>\n\n\n\n<p>Observe that you can easily understand the difference between script with session and without session.</p>\n\n\n\n<h2>Automating forms with Python</h2>\n\n\n\n<p>In this section we are going to deal with a Python module named Mechanize that will reduce our work and automate the process of filling up forms.</p>\n\n\n\n<h3>Mechanize module</h3>\n\n\n\n<p>Mechanize module provides us a high-level interface to interact with forms. Before starting using it we need to install it with the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install mechanize</code></pre>\n\n\n\n<p>Note that it would work only in Python 2.x.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we are going to automate the process of filling a login form having two fields namely email and password −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import mechanize\nbrwsr = mechanize.Browser()\nbrwsr.open(Enter the URL of login)\nbrwsr.select_form(nr = 0)\nbrwsr&#91;'email'] = ‘Enter email’\nbrwsr&#91;'password'] = ‘Enter password’\nresponse = brwsr.submit()\nbrwsr.submit()</code></pre>\n\n\n\n<p>The above code is very easy to understand. First, we imported mechanize module. Then a Mechanize browser object has been created. Then, we navigated to the login URL and selected the form. After that, names and values are passed directly to the browser object.</p>\n","protected":false},"excerpt":{"rendered":"<p>In the previous chapter, we have seen scraping dynamic websites. In this chapter, let us understand scraping of websites that work on user based inputs, that is form based websites. Introduction These days WWW (World Wide Web) is moving towards social media as well as usergenerated contents. So the question arises how we can access [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1816,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1019"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1019"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1019/revisions"}],"predecessor-version":[{"id":1827,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1019/revisions/1827"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1816"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1019"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1019"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1019"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1025,"date":"2020-05-21T06:27:43","date_gmt":"2020-05-21T06:27:43","guid":{"rendered":"http://python3.foobrdigital.com/?p=1025"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"processing-captcha","status":"publish","type":"post","link":"https://python3.foobrdigital.com/processing-captcha/","title":{"rendered":"Processing CAPTCHA"},"content":{"rendered":"\n<p>In this chapter, let us understand how to perform web scraping and processing CAPTCHA that is used for testing a user for human or robot.</p>\n\n\n\n<h2>What is CAPTCHA?</h2>\n\n\n\n<p>The full form of CAPTCHA is&nbsp;<strong>Completely Automated Public Turing test to tell Computers and Humans Apart</strong>, which clearly suggests that it is a test to determine whether the user is human or not.</p>\n\n\n\n<p>A CAPTCHA is a distorted image which is usually not easy to detect by computer program but a human can somehow manage to understand it. Most of the websites use CAPTCHA to prevent bots from interacting.</p>\n\n\n\n<h2>Loading CAPTCHA with Python</h2>\n\n\n\n<p>Suppose we want to do registration on a website and there is form with CAPTCHA, then before loading the CAPTCHA image we need to know about the specific information required by the form. With the help of next Python script we can understand the form requirements of registration form on website named&nbsp;http://example.webscrapping.com.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import lxml.html\nimport urllib.request as urllib2\nimport pprint\nimport http.cookiejar as cookielib\ndef form_parsing(html):\n   tree = lxml.html.fromstring(html)\n   data = {}\n   for e in tree.cssselect('form input'):\n      if e.get('name'):\n         data&#91;e.get('name')] = e.get('value')\n   return data\nREGISTER_URL = '&lt;a target=\"_blank\" rel=\"nofollow\" \n   href=\"http://example.webscraping.com/user/register\">http://example.webscraping.com/user/register'&lt;/a>\nckj = cookielib.CookieJar()\nbrowser = urllib2.build_opener(urllib2.HTTPCookieProcessor(ckj))\nhtml = browser.open(\n   '&lt;a target=\"_blank\" rel=\"nofollow\" \n      href=\"http://example.webscraping.com/places/default/user/register?_next\">\n      http://example.webscraping.com/places/default/user/register?_next&lt;/a> = /places/default/index'\n).read()\nform = form_parsing(html)\npprint.pprint(form)</code></pre>\n\n\n\n<p>In the above Python script, first we defined a function that will parse the form by using lxml python module and then it will print the form requirements as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>{\n   '_formkey': '5e306d73-5774-4146-a94e-3541f22c95ab',\n   '_formname': 'register',\n   '_next': '/places/default/index',\n   'email': '',\n   'first_name': '',\n   'last_name': '',\n   'password': '',\n   'password_two': '',\n   'recaptcha_response_field': None\n}</code></pre>\n\n\n\n<p>You can check from the above output that all the information except&nbsp;<strong>recpatcha_response_field</strong>&nbsp;are understandable and straightforward. Now the question arises that how we can handle this complex information and download CAPTCHA. It can be done with the help of pillow Python library as follows;</p>\n\n\n\n<h2>Pillow Python Package</h2>\n\n\n\n<p>Pillow is a fork of the Python Image library having useful functions for manipulating images. It can be installed with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install pillow\n﻿</code></pre>\n\n\n\n<p>In the next example we will use it for loading the CAPTCHA −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from io import BytesIO\nimport lxml.html\nfrom PIL import Image\ndef load_captcha(html):\n   tree = lxml.html.fromstring(html)\n   img_data = tree.cssselect('div#recaptcha img')&#91;0].get('src')\n   img_data = img_data.partition(',')&#91;-1]\n   binary_img_data = img_data.decode('base64')\n   file_like = BytesIO(binary_img_data)\n   img = Image.open(file_like)\n   return img</code></pre>\n\n\n\n<p>The above python script is using&nbsp;<strong>pillow</strong>&nbsp;python package and defining a function for loading CAPTCHA image. It must be used with the function named&nbsp;<strong>form_parser()</strong>&nbsp;that is defined in the previous script for getting information about the registration form. This script will save the CAPTCHA image in a useful format which further can be extracted as string.</p>\n\n\n\n<h2>OCR: Extracting Text from Image using Python</h2>\n\n\n\n<p>After loading the CAPTCHA in a useful format, we can extract it with the help of Optical Character Recognition (OCR), a process of extracting text from the images. For this purpose, we are going to use open source Tesseract OCR engine. It can be installed with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pip install pytesseract</code></pre>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>Here we will extend the above Python script, which loaded the CAPTCHA by using Pillow Python Package, as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import pytesseract\nimg = get_captcha(html)\nimg.save('captcha_original.png')\ngray = img.convert('L')\ngray.save('captcha_gray.png')\nbw = gray.point(lambda x: 0 if x &lt; 1 else 255, '1')\nbw.save('captcha_thresholded.png')</code></pre>\n\n\n\n<p>The above Python script will read the CAPTCHA in black and white mode which would be clear and easy to pass to tesseract as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pytesseract.image_to_string(bw)</code></pre>\n\n\n\n<p>After running the above script we will get the CAPTCHA of registration form as the output.</p>\n","protected":false},"excerpt":{"rendered":"<p>In this chapter, let us understand how to perform web scraping and processing CAPTCHA that is used for testing a user for human or robot. What is CAPTCHA? The full form of CAPTCHA is&nbsp;Completely Automated Public Turing test to tell Computers and Humans Apart, which clearly suggests that it is a test to determine whether [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1817,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1025"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1025"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1025/revisions"}],"predecessor-version":[{"id":1828,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1025/revisions/1828"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1817"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1025"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1025"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1025"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1029,"date":"2020-05-21T06:31:22","date_gmt":"2020-05-21T06:31:22","guid":{"rendered":"http://python3.foobrdigital.com/?p=1029"},"modified":"2020-12-16T16:48:51","modified_gmt":"2020-12-16T16:48:51","slug":"testing-with-scrapers","status":"publish","type":"post","link":"https://python3.foobrdigital.com/testing-with-scrapers/","title":{"rendered":"Testing with Scrapers"},"content":{"rendered":"\n<p>This chapter explains how to perform testing using web scrapers in Python.</p>\n\n\n\n<h2>Introduction</h2>\n\n\n\n<p>In large web projects, automated testing of website’s backend is performed regularly but the frontend testing is skipped often. The main reason behind this is that the programming of websites is just like a net of various markup and programming languages. We can write unit test for one language but it becomes challenging if the interaction is being done in another language. That is why we must have suite of tests to make sure that our code is performing as per our expectation.</p>\n\n\n\n<h2>Testing using Python</h2>\n\n\n\n<p>When we are talking about testing, it means unit testing. Before diving deep into testing with Python, we must know about unit testing. Following are some of the characteristics of unit testing −</p>\n\n\n\n<ul><li>At-least one aspect of the functionality of a component would be tested in each unit test.</li><li>Each unit test is independent and can also run independently.</li><li>Unit test does not interfere with success or failure of any other test.</li><li>Unit tests can run in any order and must contain at least one assertion.</li></ul>\n\n\n\n<h2>Unittest − Python Module</h2>\n\n\n\n<p>Python module named Unittest for unit testing is comes with all the standard Python installation. We just need to import it and rest is the task of unittest.TestCase class which will do the followings −</p>\n\n\n\n<ul><li>SetUp and tearDown functions are provided by unittest.TestCase class. These functions can run before and after each unit test.</li><li>It also provides assert statements to allow tests to pass or fail.</li><li>It runs all the functions that begin with test_ as unit test.</li></ul>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example we are going to combine web scraping with&nbsp;<strong>unittest</strong>. We will test Wikipedia page for searching string ‘Python’. It will basically do two tests, first weather the title page is same as the search string i.e.‘Python’ or not and second test makes sure that the page has a content div.</p>\n\n\n\n<p>First, we will import the required Python modules. We are using BeautifulSoup for web scraping and of course unittest for testing.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport unittest</code></pre>\n\n\n\n<p>Now we need to define a class which will extend unittest.TestCase. Global object bs would be shared between all tests. A unittest specified function setUpClass will accomplish it. Here we will define two functions, one for testing the title page and other for testing the page content.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>class Test(unittest.TestCase):\n   bs = None\n   def setUpClass():\n      url = '&lt;a target=\"_blank\" rel=\"nofollow\" href=\"https://en.wikipedia.org/wiki/Python\">https://en.wikipedia.org/wiki/Python'&lt;/a>\n      Test.bs = BeautifulSoup(urlopen(url), 'html.parser')\n   def test_titleText(self):\n      pageTitle = Test.bs.find('h1').get_text()\n      self.assertEqual('Python', pageTitle);\n   def test_contentExists(self):\n      content = Test.bs.find('div',{'id':'mw-content-text'})\n      self.assertIsNotNone(content)\nif __name__ == '__main__':\n   unittest.main()</code></pre>\n\n\n\n<p>After running the above script we will get the following output −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>----------------------------------------------------------------------\nRan 2 tests in 2.773s\n\nOK\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: False\n\nD:\\ProgramData\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870:\nUserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)</code></pre>\n\n\n\n<h2>Testing with Selenium</h2>\n\n\n\n<p>Let us discuss how to use Python Selenium for testing. It is also called Selenium testing. Both Python&nbsp;<strong>unittest</strong>&nbsp;and&nbsp;<strong>Selenium</strong>&nbsp;do not have much in common. We know that Selenium sends the standard Python commands to different browsers, despite variation in their browser&#8217;s design. Recall that we already installed and worked with Selenium in previous chapters. Here we will create test scripts in Selenium and use it for automation.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>With the help of next Python script, we are creating test script for the automation of Facebook Login page. You can modify the example for automating other forms and logins of your choice, however the concept would be same.</p>\n\n\n\n<p>First for connecting to web browser, we will import webdriver from selenium module −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from selenium import webdriver</code></pre>\n\n\n\n<p>Now, we need to import Keys from selenium module.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from selenium.webdriver.common.keys import Keys</code></pre>\n\n\n\n<p>Next we need to provide username and password for login into our facebook account</p>\n\n\n\n<pre class=\"wp-block-code\"><code>user = \"gauravleekha@gmail.com\"\npwd = \"\"</code></pre>\n\n\n\n<p>Next, provide the path to web driver for Chrome.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r'C:\\\\Users\\\\gaurav\\\\Desktop\\\\Chromedriver'\ndriver = webdriver.Chrome(executable_path=path)\ndriver.get(\"http://www.facebook.com\")</code></pre>\n\n\n\n<p>Now we will verify the conditions by using assert keyword.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>assert \"Facebook\" in driver.title</code></pre>\n\n\n\n<p>With the help of following line of code we are sending values to the email section. Here we are searching it by its id but we can do it by searching it by name as&nbsp;<strong>driver.find_element_by_name(&#8220;email&#8221;)</strong>.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>element = driver.find_element_by_id(\"email\")\nelement.send_keys(user)</code></pre>\n\n\n\n<p>With the help of following line of code we are sending values to the password section. Here we are searching it by its id but we can do it by searching it by name as&nbsp;<strong>driver.find_element_by_name(&#8220;pass&#8221;)</strong>.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>element = driver.find_element_by_id(\"pass\")\nelement.send_keys(pwd)</code></pre>\n\n\n\n<p>Next line of code is used to press enter/login after inserting the values in email and password field.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>element.send_keys(Keys.RETURN)</code></pre>\n\n\n\n<p>Now we will close the browser.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>driver.close()</code></pre>\n\n\n\n<p>After running the above script, Chrome web browser will be opened and you can see email and password is being inserted and clicked on login button.</p>\n\n\n\n<h2>Comparison: unittest or Selenium</h2>\n\n\n\n<p>The comparison of unittest and selenium is difficult because if you want to work with large test suites, the syntactical rigidity of unites is required. On the other hand, if you are going to test website flexibility then Selenium test would be our first choice. But what if we can combine both of them. We can import selenium into Python unittest and get the best of both. Selenium can be used to get information about a website and unittest can evaluate whether that information meets the criteria for passing the test or not.</p>\n\n\n\n<p>For example, we are rewriting the above Python script for automation of Facebook login by combining both of them as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import unittest\nfrom selenium import webdriver\n\nclass InputFormsCheck(unittest.TestCase):\n   def setUp(self):\n      self.driver = webdriver.Chrome(r'C:\\Users\\gaurav\\Desktop\\chromedriver')\n      def test_singleInputField(self):\n      user = \"gauravleekha@gmail.com\"\n      pwd = \"\"\n      pageUrl = \"http://www.facebook.com\"\n      driver=self.driver\n      driver.maximize_window()\n      driver.get(pageUrl)\n      assert \"Facebook\" in driver.title\n      elem = driver.find_element_by_id(\"email\")\n      elem.send_keys(user)\n      elem = driver.find_element_by_id(\"pass\")\n      elem.send_keys(pwd)\n      elem.send_keys(Keys.RETURN)\n   def tearDown(self):\n      self.driver.close()\nif __name__ == \"__main__\":\n   unittest.main()</code></pre>\n","protected":false},"excerpt":{"rendered":"<p>This chapter explains how to perform testing using web scrapers in Python. Introduction In large web projects, automated testing of website’s backend is performed regularly but the frontend testing is skipped often. The main reason behind this is that the programming of websites is just like a net of various markup and programming languages. We [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1818,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[7,55,130],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1029"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1029"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1029/revisions"}],"predecessor-version":[{"id":1829,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1029/revisions/1829"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1818"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1029"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1029"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1029"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}}]