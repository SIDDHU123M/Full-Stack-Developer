[{"id":1194,"date":"2020-05-21T07:27:13","date_gmt":"2020-05-21T07:27:13","guid":{"rendered":"http://python3.foobrdigital.com/?p=1194"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"basics","status":"publish","type":"post","link":"https://python3.foobrdigital.com/basics/","title":{"rendered":"Basics"},"content":{"rendered":"\n<p>We are living in the ‘age of data’ that is enriched with better computational power and more storage resources,. This data or information is increasing day by day, but the real challenge is to make sense of all the data. Businesses &amp; organizations are trying to deal with it by building intelligent systems using the concepts and methodologies from Data science, Data Mining and Machine learning. Among them, machine learning is the most exciting field of computer science. It would not be wrong if we call machine learning the application and science of algorithms that provides sense to the data.</p>\n\n\n\n<h2>What is Machine Learning?</h2>\n\n\n\n<p>Machine Learning (ML) is that field of computer science with the help of which computer systems can provide sense to data in much the same way as human beings do.</p>\n\n\n\n<p>In simple words, ML is a type of artificial intelligence that extract patterns out of raw data by using an algorithm or method. The main focus of ML is to allow computer systems learn from experience without being explicitly programmed or human intervention.</p>\n\n\n\n<h2>Need for Machine Learning</h2>\n\n\n\n<p>Human beings, at this moment, are the most intelligent and advanced species on earth because they can think, evaluate and solve complex problems. On the other side, AI is still in its initial stage and haven’t surpassed human intelligence in many aspects. Then the question is that what is the need to make machine learn? The most suitable reason for doing this is, “to make decisions, based on data, with efficiency and scale”.</p>\n\n\n\n<p>Lately, organizations are investing heavily in newer technologies like Artificial Intelligence, Machine Learning and Deep Learning to get the key information from data to perform several real-world tasks and solve problems. We can call it data-driven decisions taken by machines, particularly to automate the process. These data-driven decisions can be used, instead of using programing logic, in the problems that cannot be programmed inherently. The fact is that we can’t do without human intelligence, but other aspect is that we all need to solve real-world problems with efficiency at a huge scale. That is why the need for machine learning arises.</p>\n\n\n\n<h2>Why &amp; When to Make Machines Learn?</h2>\n\n\n\n<p>We have already discussed the need for machine learning, but another question arises that in what scenarios we must make the machine learn? There can be several circumstances where we need machines to take data-driven decisions with efficiency and at a huge scale. The followings are some of such circumstances where making machines learn would be more effective.</p>\n\n\n\n<h3>Lack of human expertise</h3>\n\n\n\n<p>The very first scenario in which we want a machine to learn and take data-driven decisions, can be the domain where there is a lack of human expertise. The examples can be navigations in unknown territories or spatial planets.</p>\n\n\n\n<h3>Dynamic scenarios</h3>\n\n\n\n<p>There are some scenarios which are dynamic in nature i.e. they keep changing over time. In case of these scenarios and behaviors, we want a machine to learn and take data-driven decisions. Some of the examples can be network connectivity and availability of infrastructure in an organization.</p>\n\n\n\n<h3>Difficulty in translating expertise into computational tasks</h3>\n\n\n\n<p>There can be various domains in which humans have their expertise,; however, they are unable to translate this expertise into computational tasks. In such circumstances we want machine learning. The examples can be the domains of speech recognition, cognitive tasks etc.</p>\n\n\n\n<h2>Machine Learning Model</h2>\n\n\n\n<p>Before discussing the machine learning model, we must need to understand the following formal definition of ML given by professor Mitchell −</p>\n\n\n\n<p>“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p>\n\n\n\n<p>The above definition is basically focusing on three parameters, also the main components of any learning algorithm, namely Task(T), Performance(P) and experience (E). In this context, we can simplify this definition as −</p>\n\n\n\n<p>ML is a field of AI consisting of learning algorithms that −</p>\n\n\n\n<ul><li>Improve their performance (P)</li><li>At executing some task (T)</li><li>Over time with experience (E)</li></ul>\n\n\n\n<p>Based on the above, the following diagram represents a Machine Learning Model −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"429\" height=\"400\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/machine_learning_model.png\" alt=\"\" class=\"wp-image-1211\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/machine_learning_model.png 429w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/machine_learning_model-300x280.png 300w\" sizes=\"(max-width: 429px) 100vw, 429px\" /></figure>\n\n\n\n<p>Let us discuss them more in detail now −</p>\n\n\n\n<h3>Task(T)</h3>\n\n\n\n<p>From the perspective of problem, we may define the task T as the real-world problem to be solved. The problem can be anything like finding best house price in a specific location or to find best marketing strategy etc. On the other hand, if we talk about machine learning, the definition of task is different because it is difficult to solve ML based tasks by conventional programming approach.</p>\n\n\n\n<p>A task T is said to be a ML based task when it is based on the process and the system must follow for operating on data points. The examples of ML based tasks are Classification, Regression, Structured annotation, Clustering, Transcription etc.</p>\n\n\n\n<h3>Experience (E)</h3>\n\n\n\n<p>As name suggests, it is the knowledge gained from data points provided to the algorithm or model. Once provided with the dataset, the model will run iteratively and will learn some inherent pattern. The learning thus acquired is called experience(E). Making an analogy with human learning, we can think of this situation as in which a human being is learning or gaining some experience from various attributes like situation, relationships etc. Supervised, unsupervised and reinforcement learning are some ways to learn or gain experience. The experience gained by out ML model or algorithm will be used to solve the task T.</p>\n\n\n\n<h3>Performance (P)</h3>\n\n\n\n<p>An ML algorithm is supposed to perform task and gain experience with the passage of time. The measure which tells whether ML algorithm is performing as per expectation or not is its performance (P). P is basically a quantitative metric that tells how a model is performing the task, T, using its experience, E. There are many metrics that help to understand the ML performance, such as accuracy score, F1 score, confusion matrix, precision, recall, sensitivity etc.</p>\n\n\n\n<h2>Challenges in Machines Learning</h2>\n\n\n\n<p>While Machine Learning is rapidly evolving, making significant strides with cybersecurity and autonomous cars, this segment of AI as whole still has a long way to go. The reason behind is that ML has not been able to overcome number of challenges. The challenges that ML is facing currently are −</p>\n\n\n\n<p><strong>Quality of data</strong>&nbsp;− Having good-quality data for ML algorithms is one of the biggest challenges. Use of low-quality data leads to the problems related to data preprocessing and feature extraction.</p>\n\n\n\n<p><strong>Time-Consuming task</strong>&nbsp;− Another challenge faced by ML models is the consumption of time especially for data acquisition, feature extraction and retrieval.</p>\n\n\n\n<p><strong>Lack of specialist persons</strong>&nbsp;− As ML technology is still in its infancy stage, availability of expert resources is a tough job.</p>\n\n\n\n<p><strong>No clear objective for formulating business problems</strong>&nbsp;− Having no clear objective and well-defined goal for business problems is another key challenge for ML because this technology is not that mature yet.</p>\n\n\n\n<p><strong>Issue of overfitting &amp; underfitting</strong>&nbsp;− If the model is overfitting or underfitting, it cannot be represented well for the problem.</p>\n\n\n\n<p><strong>Curse of dimensionality</strong>&nbsp;− Another challenge ML model faces is too many features of data points. This can be a real hindrance.</p>\n\n\n\n<p><strong>Difficulty in deployment</strong>&nbsp;− Complexity of the ML model makes it quite difficult to be deployed in real life.</p>\n\n\n\n<h2>Applications of Machines Learning</h2>\n\n\n\n<p>Machine Learning is the most rapidly growing technology and according to researchers we are in the golden year of AI and ML. It is used to solve many real-world complex problems which cannot be solved with traditional approach. Following are some real-world applications of ML −</p>\n\n\n\n<ul><li>Emotion analysis</li><li>Sentiment analysis</li><li>Error detection and prevention</li><li>Weather forecasting and prediction</li><li>Stock market analysis and forecasting</li><li>Speech synthesis</li><li>Speech recognition</li><li>Customer segmentation</li><li>Object recognition</li><li>Fraud detection</li><li>Fraud prevention</li><li>Recommendation of products to customer in online shopping</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>We are living in the ‘age of data’ that is enriched with better computational power and more storage resources,. This data or information is increasing day by day, but the real challenge is to make sense of all the data. Businesses &amp; organizations are trying to deal with it by building intelligent systems using the [&hellip;]</p>\n","protected":false},"author":2,"featured_media":1927,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1194"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/2"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1194"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1194/revisions"}],"predecessor-version":[{"id":1213,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1194/revisions/1213"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1927"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1194"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1194"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1194"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1193,"date":"2020-05-21T07:30:13","date_gmt":"2020-05-21T07:30:13","guid":{"rendered":"http://python3.foobrdigital.com/?p=1193"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"ecosystem","status":"publish","type":"post","link":"https://python3.foobrdigital.com/ecosystem/","title":{"rendered":"Ecosystem"},"content":{"rendered":"\n<h2>An Introduction to Python</h2>\n\n\n\n<p>Python is a popular object-oriented programing language having the capabilities of high-level programming language. Its easy to learn syntax and portability capability makes it popular these days. The followings facts gives us the introduction to Python −</p>\n\n\n\n<ul><li>Python was developed by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands.</li><li>It was written as the successor of programming language named ‘ABC’.</li><li>It’s first version was released in 1991.</li><li>The name Python was picked by Guido van Rossum from a TV show named Monty Python’s Flying Circus.</li><li>It is an open source programming language which means that we can freely download it and use it to develop programs. It can be downloaded from&nbsp;www.python.org..</li><li>Python programming language is having the features of Java and C both. It is having the elegant ‘C’ code and on the other hand, it is having classes and objects like Java for object-oriented programming.</li><li>It is an interpreted language, which means the source code of Python program would be first converted into bytecode and then executed by Python virtual machine.</li></ul>\n\n\n\n<h2>Strengths and Weaknesses of Python</h2>\n\n\n\n<p>Every programming language has some strengths as well as weaknesses, so does Python too.</p>\n\n\n\n<h3>Strengths</h3>\n\n\n\n<p>According to studies and surveys, Python is the fifth most important language as well as the most popular language for machine learning and data science. It is because of the following strengths that Python has −</p>\n\n\n\n<p><strong>Easy to learn and understand</strong>&nbsp;− The syntax of Python is simpler; hence it is relatively easy, even for beginners also, to learn and understand the language.</p>\n\n\n\n<p><strong>Multi-purpose language</strong>&nbsp;− Python is a multi-purpose programming language because it supports structured programming, object-oriented programming as well as functional programming.</p>\n\n\n\n<p><strong>Huge number of modules</strong>&nbsp;− Python has huge number of modules for covering every aspect of programming. These modules are easily available for use hence making Python an extensible language.</p>\n\n\n\n<p><strong>Support of open source community</strong>&nbsp;− As being open source programming language, Python is supported by a very large developer community. Due to this, the bugs are easily fixed by the Python community. This characteristic makes Python very robust and adaptive.</p>\n\n\n\n<p><strong>Scalability</strong>&nbsp;− Python is a scalable programming language because it provides an improved structure for supporting large programs than shell-scripts.</p>\n\n\n\n<h3>Weakness</h3>\n\n\n\n<p>Although Python is a popular and powerful programming language, it has its own weakness of slow execution speed.</p>\n\n\n\n<p>The execution speed of Python is slow as compared to compiled languages because Python is an interpreted language. This can be the major area of improvement for Python community.</p>\n\n\n\n<h2>Installing Python</h2>\n\n\n\n<p>For working in Python, we must first have to install it. You can perform the installation of Python in any of the following two ways −</p>\n\n\n\n<ul><li>Installing Python individually</li><li>Using Pre-packaged Python distribution: Anaconda</li></ul>\n\n\n\n<p>Let us discuss these each in detail.</p>\n\n\n\n<h3>Installing Python Individually</h3>\n\n\n\n<p>If you want to install Python on your computer, then then you need to download only the binary code applicable for your platform. Python distribution is available for Windows, Linux and Mac platforms.</p>\n\n\n\n<p>The following is a quick overview of installing Python on the above-mentioned platforms −</p>\n\n\n\n<p><strong>On Unix and Linux platform</strong></p>\n\n\n\n<p>With the help of following steps, we can install Python on Unix and Linux platform −</p>\n\n\n\n<ul><li>First, go to&nbsp;www.python.org/downloads/.</li><li>Next, click on the link to download zipped source code available for Unix/Linux.</li><li>Now, Download and extract files.</li><li>Next, we can edit the&nbsp;<em>Modules/Setup</em>&nbsp;file if we want to customize some options.<ul><li>Next, write the command&nbsp;<strong>run ./configure script</strong></li><li>make</li><li>make install</li></ul></li></ul>\n\n\n\n<p><strong>On Windows platform</strong></p>\n\n\n\n<p>With the help of following steps, we can install Python on Windows platform −</p>\n\n\n\n<ul><li>First, go to&nbsp;https://www.python.org/downloads/.</li><li>Next, click on the link for Windows installer python-XYZ.msi file. Here XYZ is the version we wish to install.</li><li>Now, we must run the file that is downloaded. It will take us to the Python install wizard, which is easy to use. Now, accept the default settings and wait until the install is finished.</li></ul>\n\n\n\n<p><strong>On Macintosh platform</strong></p>\n\n\n\n<p>For Mac OS X, Homebrew, a great and easy to use package installer is recommended to install Python 3. In case if you don&#8217;t have Homebrew, you can install it with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"</code></pre>\n\n\n\n<p>It can be updated with the command below −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ brew update\n</code></pre>\n\n\n\n<p>Now, to install Python3 on your system, we need to run the following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>$ brew install python3\n</code></pre>\n\n\n\n<h3>Using Pre-packaged Python Distribution: Anaconda</h3>\n\n\n\n<p>Anaconda is a packaged compilation of Python which have all the libraries widely used in Data science. We can follow the following steps to setup Python environment using Anaconda −</p>\n\n\n\n<ul><li><strong>Step 1</strong>&nbsp;− First, we need to download the required installation package from Anaconda distribution. The link for the same is&nbsp;www.anaconda.com/distribution/.&nbsp;You can choose from Windows, Mac and Linux OS as per your requirement.</li><li><strong>Step 2</strong>&nbsp;− Next, select the Python version you want to install on your machine. The latest Python version is 3.7. There you will get the options for 64-bit and 32-bit Graphical installer both.</li><li><strong>Step 3</strong>&nbsp;− After selecting the OS and Python version, it will download the Anaconda installer on your computer. Now, double click the file and the installer will install Anaconda package.</li><li><strong>Step 4</strong>&nbsp;− For checking whether it is installed or not, open a command prompt and type Python as follows</li><li></li><li></li></ul>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"600\" height=\"313\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/python_distribution.png\" alt=\"\" class=\"wp-image-1221\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/python_distribution.png 600w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/python_distribution-300x157.png 300w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></figure>\n\n\n\n<p>You can also check this in detailed video lecture at&nbsp;https://www.codemeals.com/python_essentials_online_training/getting_started_with_anaconda.asp.</p>\n\n\n\n<h2>Why Python for Data Science?</h2>\n\n\n\n<p>Python is the fifth most important language as well as most popular language for Machine learning and data science. The following are the features of Python that makes it the preferred choice of language for data science −</p>\n\n\n\n<h3>Extensive set of packages</h3>\n\n\n\n<p>Python has an extensive and powerful set of packages which are ready to be used in various domains. It also has packages like&nbsp;<strong>numpy, scipy, pandas, scikit-learn</strong>&nbsp;etc. which are required for machine learning and data science.</p>\n\n\n\n<h3>Easy prototyping</h3>\n\n\n\n<p>Another important feature of Python that makes it the choice of language for data science is the easy and fast prototyping. This feature is useful for developing new algorithm.</p>\n\n\n\n<h3>Collaboration feature</h3>\n\n\n\n<p>The field of data science basically needs good collaboration and Python provides many useful tools that make this extremely.</p>\n\n\n\n<h3>One language for many domains</h3>\n\n\n\n<p>A typical data science project includes various domains like data extraction, data manipulation, data analysis, feature extraction, modelling, evaluation, deployment and updating the solution. As Python is a multi-purpose language, it allows the data scientist to address all these domains from a common platform.</p>\n\n\n\n<h2>Components of Python ML Ecosystem</h2>\n\n\n\n<p>In this section, let us discuss some core Data Science libraries that form the components of Python Machine learning ecosystem. These useful components make Python an important language for Data Science. Though there are many such components, let us discuss some of the importance components of Python ecosystem here −</p>\n\n\n\n<ul><li>Jupyter Notebook&nbsp;− Jupyter notebooks basically provides an interactive computational environment for developing Python based Data Science applications.</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>An Introduction to Python Python is a popular object-oriented programing language having the capabilities of high-level programming language. Its easy to learn syntax and portability capability makes it popular these days. The followings facts gives us the introduction to Python − Python was developed by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands. [&hellip;]</p>\n","protected":false},"author":2,"featured_media":1928,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1193"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/2"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1193"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1193/revisions"}],"predecessor-version":[{"id":1952,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1193/revisions/1952"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1928"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1193"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1193"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1193"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1192,"date":"2020-05-21T07:31:27","date_gmt":"2020-05-21T07:31:27","guid":{"rendered":"http://python3.foobrdigital.com/?p=1192"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"methods","status":"publish","type":"post","link":"https://python3.foobrdigital.com/methods/","title":{"rendered":"Methods"},"content":{"rendered":"\n<p>There are various ML algorithms, techniques and methods that can be used to build models for solving real-life problems by using data. In this chapter, we are going to discuss such different kinds of methods.</p>\n\n\n\n<h2>Different Types of Methods</h2>\n\n\n\n<p>The following are various ML methods based on some broad categories −</p>\n\n\n\n<ul><li>Based on human supervision</li><li>Unsupervised Learning</li><li>Semi-supervised Learning</li><li>Reinforcement Learning</li></ul>\n\n\n\n<h2>Tasks Suited for Machine Learning</h2>\n\n\n\n<p>The following diagram shows what type of task is appropriate for various ML problems −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"600\" height=\"467\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/task_for_ml_problems.png\" alt=\"\" class=\"wp-image-1227\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/task_for_ml_problems.png 600w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/task_for_ml_problems-300x234.png 300w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></figure>\n\n\n\n<h3>Based on learning ability</h3>\n\n\n\n<p>In the learning process, the following are some methods that are based on learning ability −</p>\n\n\n\n<p><strong>Batch Learning</strong></p>\n\n\n\n<p>In many cases, we have end-to-end Machine Learning systems in which we need to train the model in one go by using whole available training data. Such kind of learning method or algorithm is called&nbsp;<strong>Batch or Offline learning</strong>. It is called Batch or Offline learning because it is a one-time procedure and the model will be trained with data in one single batch. The following are the main steps of Batch learning methods −</p>\n\n\n\n<ul><li><strong>Step 1</strong>&nbsp;− First, we need to collect all the training data for start training the model.</li><li><strong>Step 2</strong>&nbsp;− Now, start the training of model by providing whole training data in one go.</li><li><strong>Step 3</strong>&nbsp;− Next, stop learning/training process once you got satisfactory results/performance.</li><li><strong>Step 4</strong>&nbsp;− Finally, deploy this trained model into production. Here, it will predict the output for new data sample.</li></ul>\n\n\n\n<h3>Online Learning</h3>\n\n\n\n<p>It is completely opposite to the batch or offline learning methods. In these learning methods, the training data is supplied in multiple incremental batches, called mini-batches, to the algorithm. Followings are the main steps of Online learning methods −</p>\n\n\n\n<ul><li><strong>Step 1</strong>&nbsp;− First, we need to collect all the training data for starting training of the model.</li><li><strong>Step 2</strong>&nbsp;− Now, start the training of model by providing a mini-batch of training data to the algorithm.</li><li><strong>Step 3</strong>&nbsp;− Next, we need to provide the mini-batches of training data in multiple increments to the algorithm.</li><li><strong>Step 4</strong>&nbsp;− As it will not stop like batch learning hence after providing whole training data in mini-batches, provide new data samples also to it.</li><li><strong>Step 5</strong>&nbsp;− Finally, it will keep learning over a period of time based on the new data samples.</li></ul>\n\n\n\n<h3>Based on Generalization Approach</h3>\n\n\n\n<p>In the learning process, followings are some methods that are based on generalization approaches −</p>\n\n\n\n<h3>Instance based Learning</h3>\n\n\n\n<p>Instance based learning method is one of the useful methods that build the ML models by doing generalization based on the input data. It is opposite to the previously studied learning methods in the way that this kind of learning involves ML systems as well as methods that uses the raw data points themselves to draw the outcomes for newer data samples without building an explicit model on training data.</p>\n\n\n\n<p>In simple words, instance-based learning basically starts working by looking at the input data points and then using a similarity metric, it will generalize and predict the new data points.</p>\n\n\n\n<h3>Model based Learning</h3>\n\n\n\n<p>In Model based learning methods, an iterative process takes place on the ML models that are built based on various model parameters, called hyper parameters and in which input data is used to extract the features. In this learning, hyper parameters are optimized based on various model validation techniques. That is why we can say that Model based learning methods uses more traditional ML approach towards generalization.</p>\n","protected":false},"excerpt":{"rendered":"<p>There are various ML algorithms, techniques and methods that can be used to build models for solving real-life problems by using data. In this chapter, we are going to discuss such different kinds of methods. Different Types of Methods The following are various ML methods based on some broad categories − Based on human supervision [&hellip;]</p>\n","protected":false},"author":2,"featured_media":1929,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1192"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/2"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1192"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1192/revisions"}],"predecessor-version":[{"id":2352,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1192/revisions/2352"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1929"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1192"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1192"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1192"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1191,"date":"2020-05-21T07:34:20","date_gmt":"2020-05-21T07:34:20","guid":{"rendered":"http://python3.foobrdigital.com/?p=1191"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"data-loading","status":"publish","type":"post","link":"https://python3.foobrdigital.com/data-loading/","title":{"rendered":"Data Loading"},"content":{"rendered":"\n<p>Suppose if you want to start a ML project then what is the first and most important thing you would require? It is the data that we need to load for starting any of the ML project. With respect to data, the most common format of data for ML projects is CSV (comma-separated values).</p>\n\n\n\n<p>Basically, CSV is a simple file format which is used to store tabular data (number and text) such as a spreadsheet in plain text. In Python, we can load CSV data into with different ways but before loading CSV data we must have to take care about some considerations.</p>\n\n\n\n<h2>Consideration While Loading CSV data</h2>\n\n\n\n<p>CSV data format is the most common format for ML data, but we need to take care about following major considerations while loading the same into our ML projects.</p>\n\n\n\n<h3>File Header</h3>\n\n\n\n<p>In CSV data files, the header contains the information for each field. We must use the same delimiter for the header file and for data file because it is the header file that specifies how should data fields be interpreted.</p>\n\n\n\n<p>The following are the two cases related to CSV file header which must be considered −</p>\n\n\n\n<ul><li><strong>Case-I: When Data file is having a file header</strong>&nbsp;− It will automatically assign the names to each column of data if data file is having a file header.</li><li><strong>Case-II: When Data file is not having a file header</strong>&nbsp;− We need to assign the names to each column of data manually if data file is not having a file header.</li></ul>\n\n\n\n<p>In both the cases, we must need to specify explicitly weather our CSV file contains header or not.</p>\n\n\n\n<h3>Comments</h3>\n\n\n\n<p>Comments in any data file are having their significance. In CSV data file, comments are indicated by a hash (#) at the start of the line. We need to consider comments while loading CSV data into ML projects because if we are having comments in the file then we may need to indicate, depends upon the method we choose for loading, whether to expect those comments or not.</p>\n\n\n\n<h3>Delimiter</h3>\n\n\n\n<p>In CSV data files, comma (,) character is the standard delimiter. The role of delimiter is to separate the values in the fields. It is important to consider the role of delimiter while uploading the CSV file into ML projects because we can also use a different delimiter such as a tab or white space. But in the case of using a different delimiter than standard one, we must have to specify it explicitly.</p>\n\n\n\n<h3>Quotes</h3>\n\n\n\n<p>In CSV data files, double quotation (“ ”) mark is the default quote character. It is important to consider the role of quotes while uploading the CSV file into ML projects because we can also use other quote character than double quotation mark. But in case of using a different quote character than standard one, we must have to specify it explicitly.</p>\n\n\n\n<h2>Methods to Load CSV Data File</h2>\n\n\n\n<p>While working with ML projects, the most crucial task is to load the data properly into it. The most common data format for ML projects is CSV and it comes in various flavors and varying difficulties to parse. In this section, we are going to discuss about three common approaches in Python to load CSV data file −</p>\n\n\n\n<h3>Load CSV with Python Standard Library</h3>\n\n\n\n<p>The first and most used approach to load CSV data file is the use of Python standard library which provides us a variety of built-in modules namely&nbsp;<em>csv module</em>&nbsp;and the&nbsp;<em>reader()function</em>. The following is an example of loading CSV data file with the help of it −</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we are using the iris&nbsp;<em>flower data set</em>&nbsp;which can be downloaded into our local directory. After loading the data file, we can convert it into</p>\n\n\n\n<p>NumPyarray and use it for ML projects. Following is the Python script for loading CSV data file −</p>\n\n\n\n<p>First, we need to import the csv module provided by Python standard library as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import csv\n</code></pre>\n\n\n\n<p>Next, we need to import Numpy module for converting the loaded data into NumPy array.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\n</code></pre>\n\n\n\n<p>Now, provide the full path of the file, stored on our local directory, having the CSV data file −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r\"c:\\iris.csv\"\n</code></pre>\n\n\n\n<p>Next, use the csv.reader()function to read data from CSV file −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>with open(path,'r') as f:\nreader = csv.reader(f,delimiter = ',')\nheaders = next(reader)\ndata = list(reader)\ndata = np.array(data).astype(float)</code></pre>\n\n\n\n<p>We can print the names of the headers with the following line of script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(headers)\n</code></pre>\n\n\n\n<p>The following line of script will print the shape of the data i.e. number of rows &amp; columns in the file −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(data.shape)\n</code></pre>\n\n\n\n<p>Next script line will give the first three line of data file −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(data&#91;:3])</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;'sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n(150, 4)\n&#91;&#91;5.1 3.5 1.4 0.2]\n&#91;4.9 3. 1.4 0.2]\n&#91;4.7 3.2 1.3 0.2]]</code></pre>\n\n\n\n<h2>Load CSV with NumPy</h2>\n\n\n\n<p>Another approach to load CSV data file is&nbsp;<em>NumPy</em>&nbsp;and&nbsp;<em>numpy.loadtxt()</em>&nbsp;function. The following is an example of loading CSV data file with the help of it −</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we are using the Pima Indians Dataset having the data of diabetic patients. This dataset is a numeric dataset with no header. It can also be downloaded into our local directory. After loading the data file, we can convert it into&nbsp;<em>NumPy</em>&nbsp;array and use it for ML projects. The following is the Python script for loading CSV data file −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from numpy import loadtxt\npath = r\"C:\\pima-indians-diabetes.csv\"\ndatapath= open(path, 'r')\ndata = loadtxt(datapath, delimiter=\",\")\nprint(data.shape)\nprint(data&#91;:3])</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>(768, 9)\n&#91;&#91; 6. 148. 72. 35. 0. 33.6 0.627 50. 1.]\n&#91; 1. 85. 66. 29. 0. 26.6 0.351 31. 0.]\n&#91; 8. 183. 64. 0. 0. 23.3 0.672 32. 1.]]</code></pre>\n\n\n\n<h2>Load CSV with Pandas</h2>\n\n\n\n<p>Another approach to load CSV data file is by&nbsp;<em>Pandas</em>&nbsp;and&nbsp;<em>pandas.read_csv()function</em>. This is the very flexible function that returns a pandas.DataFrame which can be used immediately for plotting. The following is an example of loading CSV data file with the help of it −</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>Here, we will be implementing two Python scripts, first is with Iris data set having headers and another is by using the&nbsp;<em>Pima Indians Dataset</em>&nbsp;which is a numeric dataset with no header. Both the datasets can be downloaded into local directory.</p>\n\n\n\n<p><strong>Script-1</strong></p>\n\n\n\n<p>The following is the Python script for loading CSV data file using&nbsp;<em>Pandas</em>&nbsp;on Iris&nbsp;<em>Data set</em>&nbsp;−</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\npath = r\"C:\\iris.csv\"\ndata = read_csv(path)\nprint(data.shape)\nprint(data&#91;:3])</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>(150, 4)\nsepal_length sepal_width petal_length petal_width\n0 5.1        3.5         1.4          0.2\n1 4.9        3.0         1.4          0.2\n2 4.7        3.2         1.3          0.2</code></pre>\n\n\n\n<p><strong>Script-2</strong></p>\n\n\n\n<p>The following is the Python script for loading CSV data file, along with providing the headers names too, using Pandas on Pima Indians Diabetes dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\npath = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=headernames)\nprint(data.shape)\nprint(data&#91;:3])</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>(768, 9)\n   preg  plas    pres  skin  test  mass   pedi  age  class\n0     6   148      72    35     0  33.6  0.627   50      1\n1     1    85      66    29     0  26.6  0.351   31      0\n2     8   183      64     0     0  23.3  0.672   32      1</code></pre>\n\n\n\n<p>The difference between above used three approaches for loading CSV data file can easily be understood with the help of given examples.</p>\n","protected":false},"excerpt":{"rendered":"<p>Suppose if you want to start a ML project then what is the first and most important thing you would require? It is the data that we need to load for starting any of the ML project. With respect to data, the most common format of data for ML projects is CSV (comma-separated values). Basically, [&hellip;]</p>\n","protected":false},"author":2,"featured_media":1930,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1191"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/2"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1191"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1191/revisions"}],"predecessor-version":[{"id":1953,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1191/revisions/1953"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1930"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1191"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1191"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1191"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1190,"date":"2020-05-21T07:36:51","date_gmt":"2020-05-21T07:36:51","guid":{"rendered":"http://python3.foobrdigital.com/?p=1190"},"modified":"2020-09-24T09:01:10","modified_gmt":"2020-09-24T09:01:10","slug":"understanding-data-with-statistics","status":"publish","type":"post","link":"https://python3.foobrdigital.com/understanding-data-with-statistics/","title":{"rendered":"Understanding Data with Statistics"},"content":{"rendered":"\n<h2>Introduction</h2>\n\n\n\n<p>While working with machine learning projects, usually we ignore two most important parts called&nbsp;<strong><em>mathematics</em></strong>&nbsp;and&nbsp;<strong><em>data</em></strong>. It is because, we know that ML is a data driven approach and our ML model will produce only as good or as bad results as the data we provided to it.</p>\n\n\n\n<p>In the previous chapter, we discussed how we can upload CSV data into our ML project, but it would be good to understand the data before uploading it. We can understand the data by two ways, with statistics and with visualization.</p>\n\n\n\n<p>In this chapter, with the help of following Python recipes, we are going to understand ML data with statistics.</p>\n\n\n\n<h2>Looking at Raw Data</h2>\n\n\n\n<p>The very first recipe is for looking at your raw data. It is important to look at raw data because the insight we will get after looking at raw data will boost our chances to better pre-processing as well as handling of data for ML projects.</p>\n\n\n\n<p>Following is a Python script implemented by using head() function of Pandas DataFrame on Pima Indians diabetes dataset to look at the first 50 rows to get better understanding of it −</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-preformatted\">from pandas import read_csv\npath = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=headernames)\nprint(data.head(50))</pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>preg plas  pres skin test   mass pedi   age      class\n0     6    148  72   35     0    33.6   0.627    50     1\n1     1     85  66   29     0    26.6   0.351    31     0\n2     8    183  64    0     0    23.3   0.672    32     1\n3     1     89  66   23    94    28.1   0.167    21     0\n4     0    137  40   35   168    43.1   2.288    33     1\n5     5    116  74    0     0    25.6   0.201    30     0\n6     3     78  50   32    88    31.0   0.248    26     1\n7    10    115   0    0     0    35.3   0.134    29     0\n8     2    197  70   45   543    30.5   0.158    53     1\n9     8    125  96    0     0     0.0   0.232    54     1\n10    4    110  92    0     0    37.6   0.191    30     0\n11   10    168  74    0     0    38.0   0.537    34     1\n12   10    139  80    0     0    27.1   1.441    57     0\n13    1    189  60   23   846    30.1   0.398    59     1\n14    5    166  72   19   175    25.8   0.587    51     1\n15    7    100   0    0     0    30.0   0.484    32     1\n16    0    118  84   47   230    45.8   0.551    31     1\n17    7    107  74    0     0    29.6   0.254    31     1\n18    1    103  30   38    83    43.3   0.183    33     0\n19    1    115  70   30    96    34.6   0.529    32     1\n20    3    126  88   41   235    39.3   0.704    27     0\n21    8     99  84    0     0    35.4   0.388    50     0\n22    7    196  90    0     0    39.8   0.451    41     1\n23    9    119  80   35     0    29.0   0.263    29     1\n24   11    143  94   33   146    36.6   0.254    51     1\n25   10    125  70   26   115    31.1   0.205    41     1\n26    7    147  76    0     0    39.4   0.257    43     1 \n27    1     97  66   15   140    23.2   0.487    22     0\n28   13    145  82   19   110    22.2   0.245    57     0\n29    5    117  92    0     0    34.1   0.337    38     0\n30    5    109  75   26     0    36.0   0.546    60     0\n31    3    158  76   36   245    31.6   0.851    28     1\n32    3     88  58   11    54    24.8   0.267    22     0 \n33    6     92  92    0     0    19.9   0.188    28     0\n34   10    122  78   31     0    27.6   0.512    45     0 \n35    4    103  60   33   192    24.0   0.966    33     0\n36   11    138  76    0     0    33.2   0.420    35     0\n37    9    102  76   37     0    32.9   0.665    46     1\n38    2     90  68   42     0    38.2   0.503    27     1\n39    4    111  72   47   207    37.1   1.390    56     1\n40    3    180  64   25    70    34.0   0.271    26     0\n41    7    133  84    0     0    40.2   0.696    37     0\n42    7    106  92   18     0    22.7   0.235    48     0\n43    9    171 110   24   240    45.4   0.721    54     1 \n44    7    159  64    0     0    27.4   0.294    40     0\n45    0    180  66   39     0    42.0   1.893    25     1\n46    1    146  56    0     0    29.7   0.564    29     0\n47    2     71  70   27     0    28.0   0.586    22     0\n48    7    103  66   32     0    39.1   0.344    31     1\n49    7    105   0    0     0    0.0    0.305    24     0\n﻿</code></pre>\n\n\n\n<p>We can observe from the above output that first column gives the row number which can be very useful for referencing a specific observation.</p>\n\n\n\n<h2>Checking Dimensions of Data</h2>\n\n\n\n<p>It is always a good practice to know how much data, in terms of rows and columns, we are having for our ML project. The reasons behind are −</p>\n\n\n\n<ul><li>Suppose if we have too many rows and columns then it would take long time to run the algorithm and train the model.</li><li>Suppose if we have too less rows and columns then it we would not have enough data to well train the model.</li></ul>\n\n\n\n<p>Following is a Python script implemented by printing the shape property on Pandas Data Frame. We are going to implement it on iris data set for getting the total number of rows and columns in it.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\npath = r\"C:\\iris.csv\"\ndata = read_csv(path)\nprint(data.shape)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-preformatted\">(150, 4)\n</pre>\n\n\n\n<p>We can easily observe from the output that iris data set, we are going to use, is having 150 rows and 4 columns.</p>\n\n\n\n<h2>Getting Each Attribute’s Data Type</h2>\n\n\n\n<p>It is another good practice to know data type of each attribute. The reason behind is that, as per to the requirement, sometimes we may need to convert one data type to another. For example, we may need to convert string into floating point or int for representing categorial or ordinal values. We can have an idea about the attribute’s data type by looking at the raw data, but another way is to use&nbsp;<em>dtypes</em>&nbsp;property of Pandas DataFrame. With the help of&nbsp;<em>dtypes</em>&nbsp;property we can categorize each attributes data type. It can be understood with the help of following Python script −</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\npath = r\"C:\\iris.csv\"\ndata = read_csv(path)\nprint(data.dtypes)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>sepal_length   float64\nsepal_width    float64\npetal_length   float64\npetal_width    float64\ndtype: object</code></pre>\n\n\n\n<p>From the above output, we can easily get the datatypes of each attribute.</p>\n\n\n\n<h2>Statistical Summary of Data</h2>\n\n\n\n<p>We have discussed Python recipe to get the shape i.e. number of rows and columns, of data but many times we need to review the summaries out of that shape of data. It can be done with the help of&nbsp;<em>describe()</em>&nbsp;function of Pandas DataFrame that further provide the following 8 statistical properties of each &amp; every data attribute −</p>\n\n\n\n<ul><li>Count</li><li>Mean</li><li>Standard Deviation</li><li>Minimum Value</li><li>Maximum value</li><li>25%</li><li>Median i.e. 50%</li><li>75%</li></ul>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom pandas import set_option\npath = r\"C:\\pima-indians-diabetes.csv\"\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=names)\nset_option('display.width', 100)\nset_option('precision', 2)\nprint(data.shape)\nprint(data.describe())</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>(768, 9)\n       preg     plas     pres    skin     test     mass     pedi     age      class\ncount  768.00   768.00   768.00  768.00   768.00   768.00   768.00   768.00   768.00\nmean     3.85   120.89    69.11   20.54    79.80    31.99     0.47    33.24     0.35\nstd      3.37    31.97    19.36   15.95   115.24     7.88     0.33    11.76     0.48\nmin      0.00     0.00     0.00    0.00     0.00     0.00     0.08    21.00     0.00\n25%      1.00    99.00    62.00    0.00     0.00    27.30     0.24    24.00     0.00\n50%      3.00   117.00    72.00   23.00    30.50    32.00     0.37    29.00     0.00\n75%      6.00   140.25    80.00   32.00   127.25    36.60     0.63    41.00     1.00\nmax     17.00   199.00   122.00   99.00   846.00    67.10     2.42    81.00     1.00</code></pre>\n\n\n\n<p>From the above output, we can observe the statistical summary of the data of Pima Indian Diabetes dataset along with shape of data.</p>\n\n\n\n<h2>Reviewing Class Distribution</h2>\n\n\n\n<p>Class distribution statistics is useful in classification problems where we need to know the balance of class values. It is important to know class value distribution because if we have highly imbalanced class distribution i.e. one class is having lots more observations than other class, then it may need special handling at data preparation stage of our ML project. We can easily get class distribution in Python with the help of Pandas DataFrame.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\npath = r\"C:\\pima-indians-diabetes.csv\"\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=names)\ncount_class = data.groupby('class').size()\nprint(count_class)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>Class\n0 500\n1 268\ndtype: int64</code></pre>\n\n\n\n<p>From the above output, it can be clearly seen that the number of observations with class 0 are almost double than number of observations with class 1.</p>\n\n\n\n<h2>Reviewing Correlation between Attributes</h2>\n\n\n\n<p>The relationship between two variables is called correlation. In statistics, the most common method for calculating correlation is Pearson’s Correlation Coefficient. It can have three values as follows −</p>\n\n\n\n<ul><li><strong>Coefficient value = 1</strong>&nbsp;− It represents full&nbsp;<strong>positive</strong>&nbsp;correlation between variables.</li><li><strong>Coefficient value = -1</strong>&nbsp;− It represents full&nbsp;<strong>negative</strong>&nbsp;correlation between variables.</li><li><strong>Coefficient value = 0</strong>&nbsp;− It represents&nbsp;<strong>no</strong>&nbsp;correlation at all between variables.</li></ul>\n\n\n\n<p>It is always good for us to review the pairwise correlations of the attributes in our dataset before using it into ML project because some machine learning algorithms such as linear regression and logistic regression will perform poorly if we have highly correlated attributes. In Python, we can easily calculate a correlation matrix of dataset attributes with the help of&nbsp;<em>corr()</em>&nbsp;function on Pandas DataFrame.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom pandas import set_option\npath = r\"C:\\pima-indians-diabetes.csv\"\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=names)\nset_option('display.width', 100)\nset_option('precision', 2)\ncorrelations = data.corr(method='pearson')\nprint(correlations)</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>preg   plas   pres   skin   test    mass    pedi    age    class\npreg   1.00   0.13   0.14  -0.08   -0.07    0.02   -0.03   0.54    0.22\nplas   0.13   1.00   0.15   0.06    0.33    0.22    0.14   0.26    0.47\npres   0.14   0.15   1.00   0.21    0.09    0.28    0.04   0.24    0.07\nskin  -0.08   0.06   0.21   1.00    0.44    0.39    0.18  -0.11    0.07\ntest  -0.07   0.33   0.09   0.44    1.00    0.20    0.19  -0.04    0.13\nmass   0.02   0.22   0.28   0.39    0.20    1.00    0.14   0.04    0.29\npedi  -0.03   0.14   0.04   0.18    0.19    0.14    1.00   0.03    0.17\nage    0.54   0.26   0.24  -0.11   -0.04    0.04    0.03   1.00    0.24\nclass  0.22   0.47   0.07   0.07    0.13    0.29    0.17   0.24    1.00</code></pre>\n\n\n\n<p>The matrix in above output gives the correlation between all the pairs of the attribute in dataset.</p>\n\n\n\n<h2>Reviewing Skew of Attribute Distribution</h2>\n\n\n\n<p>Skewness may be defined as the distribution that is assumed to be Gaussian but appears distorted or shifted in one direction or another, or either to the left or right. Reviewing the skewness of attributes is one of the important tasks due to following reasons −</p>\n\n\n\n<ul><li>Presence of skewness in data requires the correction at data preparation stage so that we can get more accuracy from our model.</li><li>Most of the ML algorithms assumes that data has a Gaussian distribution i.e. either normal of bell curved data.</li></ul>\n\n\n\n<p>In Python, we can easily calculate the skew of each attribute by using&nbsp;<strong>skew()</strong>&nbsp;function on Pandas DataFrame.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\npath = r\"C:\\pima-indians-diabetes.csv\"\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=names)\nprint(data.skew())</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>preg   0.90\nplas   0.17\npres  -1.84\nskin   0.11\ntest   2.27\nmass  -0.43\npedi   1.92\nage    1.13\nclass  0.64\ndtype: float64</code></pre>\n\n\n\n<p>From the above output, positive or negative skew can be observed. If the value is closer to zero, then it shows less skew.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction While working with machine learning projects, usually we ignore two most important parts called&nbsp;mathematics&nbsp;and&nbsp;data. It is because, we know that ML is a data driven approach and our ML model will produce only as good or as bad results as the data we provided to it. In the previous chapter, we discussed how we [&hellip;]</p>\n","protected":false},"author":2,"featured_media":1962,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1190"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/2"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1190"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1190/revisions"}],"predecessor-version":[{"id":2493,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1190/revisions/2493"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1962"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1190"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1190"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1190"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1189,"date":"2020-05-21T07:37:53","date_gmt":"2020-05-21T07:37:53","guid":{"rendered":"http://python3.foobrdigital.com/?p=1189"},"modified":"2020-09-24T09:01:10","modified_gmt":"2020-09-24T09:01:10","slug":"understanding-data-with-visualization","status":"publish","type":"post","link":"https://python3.foobrdigital.com/understanding-data-with-visualization/","title":{"rendered":"Understanding Data with Visualization"},"content":{"rendered":"\n<p>In the previous chapter, we have discussed the importance of data for Machine Learning algorithms along with some Python recipes to understand the data with statistics. There is another way called Visualization, to understand the data.</p>\n\n\n\n<p>With the help of data visualization, we can see how the data looks like and what kind of correlation is held by the attributes of data. It is the fastest way to see if the features correspond to the output. With the help of following Python recipes, we can understand ML data with statistics.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"600\" height=\"287\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/data_visualization_techniques.png\" alt=\"\" class=\"wp-image-1245\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/data_visualization_techniques.png 600w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/data_visualization_techniques-300x144.png 300w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></figure>\n\n\n\n<h2>Univariate Plots: Understanding Attributes Independently</h2>\n\n\n\n<p>The simplest type of visualization is single-variable or “univariate” visualization. With the help of univariate visualization, we can understand each attribute of our dataset independently. The following are some techniques in Python to implement univariate visualization −</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><th>Sr.No</th><th>Univariate Plots &amp; Description</th></tr><tr><td>1</td><td> Histograms group the data in bins and is the fastest way to get idea about the distribution of each attribute in dataset.</td></tr><tr><td>2</td><td>Density Plots Another quick and easy technique for getting each attributes distribution is Density plots.</td></tr><tr><td>3</td><td>Box and Whisker Plots Box and Whisker plots, also called box plots in short, is another useful technique to review the distribution of each attribute’s distribution.</td></tr></tbody></table></figure>\n\n\n\n<h2>Multivariate Plots: Interaction Among Multiple Variables</h2>\n\n\n\n<p>Another type of visualization is multi-variable or “multivariate” visualization. With the help of multivariate visualization, we can understand interaction between multiple attributes of our dataset. The following are some techniques in Python to implement multivariate visualization −</p>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><th>Sr.No</th><th>Multivariate Plots &amp; Description</th></tr><tr><td>1</td><td>Correlation Matrix Plot Correlation is an indication about the changes between two variables.</td></tr><tr><td>2</td><td>Scatter Matrix Plot Scatter plots shows how much one variable is affected by another or the relationship between them with the help of dots in two dimensions.</td></tr></tbody></table></figure>\n\n\n\n<p></p>\n","protected":false},"excerpt":{"rendered":"<p>In the previous chapter, we have discussed the importance of data for Machine Learning algorithms along with some Python recipes to understand the data with statistics. There is another way called Visualization, to understand the data. With the help of data visualization, we can see how the data looks like and what kind of correlation [&hellip;]</p>\n","protected":false},"author":2,"featured_media":1932,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1189"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/2"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1189"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1189/revisions"}],"predecessor-version":[{"id":2347,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1189/revisions/2347"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1932"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1189"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1189"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1189"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1373,"date":"2020-05-21T09:06:48","date_gmt":"2020-05-21T09:06:48","guid":{"rendered":"http://python3.foobrdigital.com/?p=1373"},"modified":"2020-09-24T09:01:10","modified_gmt":"2020-09-24T09:01:10","slug":"preparing-data","status":"publish","type":"post","link":"https://python3.foobrdigital.com/preparing-data/","title":{"rendered":"Preparing Data"},"content":{"rendered":"\n<h2>Introduction</h2>\n\n\n\n<p>Machine Learning algorithms are completely dependent on data because it is the most crucial aspect that makes model training possible. On the other hand, if we won’t be able to make sense out of that data, before feeding it to ML algorithms, a machine will be useless. In simple words, we always need to feed right data i.e. the data in correct scale, format and containing meaningful features, for the problem we want machine to solve.</p>\n\n\n\n<p>This makes data preparation the most important step in ML process. Data preparation may be defined as the procedure that makes our dataset more appropriate for ML process.</p>\n\n\n\n<h2>Why Data Pre-processing?</h2>\n\n\n\n<p>After selecting the raw data for ML training, the most important task is data pre-processing. In broad sense, data preprocessing will convert the selected data into a form we can work with or can feed to ML algorithms. We always need to preprocess our data so that it can be as per the expectation of machine learning algorithm.</p>\n\n\n\n<h2>Data Pre-processing Techniques</h2>\n\n\n\n<p>We have the following data pre-processing techniques that can be applied on data set to produce data for ML algorithms −</p>\n\n\n\n<h3>Scaling</h3>\n\n\n\n<p>Most probably our dataset comprises of the attributes with varying scale, but we cannot provide such data to ML algorithm hence it requires rescaling. Data rescaling makes sure that attributes are at same scale. Generally, attributes are rescaled into the range of 0 and 1. ML algorithms like gradient descent and k-Nearest Neighbors requires scaled data. We can rescale the data with the help of&nbsp;<em>MinMaxScaler</em>&nbsp;class of&nbsp;<em>scikit-learn</em>&nbsp;Python library.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In this example we will rescale the data of Pima Indians Diabetes dataset which we used earlier. First, the CSV data will be loaded (as done in the previous chapters) and then with the help of&nbsp;<em>MinMaxScaler</em>&nbsp;class, it will be rescaled in the range of 0 and 1.</p>\n\n\n\n<p>The first few lines of the following script are same as we have written in previous chapters while loading CSV data.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom numpy import set_printoptions\nfrom sklearn import preprocessing\npath = r'C:\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(path, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Now, we can use&nbsp;<em>MinMaxScaler</em>&nbsp;class to rescale the data in the range of 0 and 1.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>data_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\ndata_rescaled = data_scaler.fit_transform(array)</code></pre>\n\n\n\n<p>We can also summarize the data for output as per our choice. Here, we are setting the precision to 1 and showing the first 10 rows in the output.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>set_printoptions(precision=1)\nprint (\"\\nScaled data:\\n\", data_rescaled&#91;0:10])</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Scaled data:\n&#91;&#91;0.4 0.7 0.6 0.4 0.  0.5 0.2 0.5 1. ]\n&#91;0.1  0.4 0.5 0.3 0.  0.4 0.1 0.2 0. ]\n&#91;0.5  0.9 0.5 0.  0.  0.3 0.3 0.2 1. ]\n&#91;0.1  0.4 0.5 0.2 0.1 0.4 0.  0.  0. ]\n&#91;0.   0.7 0.3 0.4 0.2 0.6 0.9 0.2 1. ]\n&#91;0.3  0.6 0.6 0.  0.  0.4 0.1 0.2 0. ]\n&#91;0.2  0.4 0.4 0.3 0.1 0.5 0.1 0.1 1. ]\n&#91;0.6  0.6 0.  0.  0.  0.5 0.  0.1 0. ]\n&#91;0.1  1.  0.6 0.5 0.6 0.5 0.  0.5 1. ]\n&#91;0.5  0.6 0.8 0.  0.  0.  0.1 0.6 1. ]]\n﻿</code></pre>\n\n\n\n<p>From the above output, all the data got rescaled into the range of 0 and 1.</p>\n\n\n\n<h2>Normalization</h2>\n\n\n\n<p>Another useful data preprocessing technique is Normalization. This is used to rescale each row of data to have a length of 1. It is mainly useful in Sparse dataset where we have lots of zeros. We can rescale the data with the help of&nbsp;<em>Normalizer</em>&nbsp;class of&nbsp;<em>scikit-learn</em>&nbsp;Python library.</p>\n\n\n\n<h2>Types of Normalization</h2>\n\n\n\n<p>In machine learning, there are two types of normalization preprocessing techniques as follows −</p>\n\n\n\n<ul><li>L1 Normalization</li><li>L2 Normalization</li></ul>\n\n\n\n<h2>Binarization</h2>\n\n\n\n<p>As the name suggests, this is the technique with the help of which we can make our data binary. We can use a binary threshold for making our data binary. The values above that threshold value will be converted to 1 and below that threshold will be converted to 0.</p>\n\n\n\n<p>For example, if we choose threshold value = 0.5, then the dataset value above it will become 1 and below this will become 0. That is why we can call it&nbsp;<strong>binarizing</strong>&nbsp;the data or&nbsp;<strong>thresholding</strong>&nbsp;the data. This technique is useful when we have probabilities in our dataset and want to convert them into crisp values.</p>\n\n\n\n<p>We can binarize the data with the help of&nbsp;<em>Binarizer</em>&nbsp;class of&nbsp;<em>scikit-learn</em>&nbsp;Python library</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In this example, we will rescale the data of Pima Indians Diabetes dataset which we used earlier. First, the CSV data will be loaded and then with the help of&nbsp;<em>Binarizer</em>&nbsp;class it will be converted into binary values i.e. 0 and 1 depending upon the threshold value. We are taking 0.5 as threshold value.</p>\n\n\n\n<p>The first few lines of following script are same as we have written in previous chapters while loading CSV data.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.preprocessing import Binarizer\npath = r'C:\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(path, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Now, we can use&nbsp;<em>Binarize</em>&nbsp;class to convert the data into binary values.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>binarizer = Binarizer(threshold=0.5).fit(array)\nData_binarized = binarizer.transform(array)</code></pre>\n\n\n\n<p>Here, we are showing the first 5 rows in the output.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print (\"\\nBinary data:\\n\", Data_binarized &#91;0:5])</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Binary data:\n&#91;&#91;1. 1. 1. 1. 0. 1. 1. 1. 1.]\n&#91;1.  1. 1. 1. 0. 1. 0. 1. 0.]\n&#91;1.  1. 1. 0. 0. 1. 1. 1. 1.]\n&#91;1.  1. 1. 1. 1. 1. 0. 1. 0.]\n&#91;0.  1. 1. 1. 1. 1. 1. 1. 1.]]</code></pre>\n\n\n\n<h2>Standardization</h2>\n\n\n\n<p>Another useful data pre-processing technique which is basically used to transform the data attributes with a Gaussian distribution. It differs the mean and SD (Standard Deviation) to a standard Gaussian distribution with a mean of 0 and a SD of 1. This technique is useful in ML algorithms like linear regression, logistic regression that assumes a Gaussian distribution in input dataset and produce better results with rescaled data. We can standardize the data (mean = 0 and SD =1) with the help of <em>StandardScaler</em> class of <em>scikit-learn</em> Python library.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In this example, we will rescale the data of Pima Indians Diabetes dataset which we used earlier. First, the CSV data will be loaded and then with the help of&nbsp;<em>StandardScaler</em>&nbsp;class it will be converted into Gaussian Distribution with mean = 0 and SD = 1.</p>\n\n\n\n<p>The first few lines of following script are same as we have written in previous chapters while loading CSV data.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.preprocessing import StandardScaler\nfrom pandas import read_csv\nfrom numpy import set_printoptions\npath = r'C:\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(path, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Now, we can use&nbsp;<em>StandardScaler</em>&nbsp;class to rescale the data.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>data_scaler = StandardScaler().fit(array)\ndata_rescaled = data_scaler.transform(array)</code></pre>\n\n\n\n<p>We can also summarize the data for output as per our choice. Here, we are setting the precision to 2 and showing the first 5 rows in the output.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>set_printoptions(precision=2)\nprint (\"\\nRescaled data:\\n\", data_rescaled &#91;0:5])</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Rescaled data:\n&#91;&#91; 0.64 0.85  0.15  0.91 -0.69  0.2   0.47  1.43  1.37]\n&#91;-0.84 -1.12 -0.16  0.53 -0.69 -0.68 -0.37 -0.19 -0.73]\n&#91; 1.23  1.94 -0.26 -1.29 -0.69 -1.1   0.6  -0.11  1.37]\n&#91;-0.84 -1.   -0.16  0.15  0.12 -0.49 -0.92 -1.04 -0.73]\n&#91;-1.14  0.5  -1.5   0.91  0.77  1.41  5.48 -0.02  1.37]]</code></pre>\n\n\n\n<h2>Data Labeling</h2>\n\n\n\n<p>We discussed the importance of good fata for ML algorithms as well as some techniques to pre-process the data before sending it to ML algorithms. One more aspect in this regard is data labeling. It is also very important to send the data to ML algorithms having proper labeling. For example, in case of classification problems, lot of labels in the form of words, numbers etc. are there on the data.</p>\n\n\n\n<h2>What is Label Encoding?</h2>\n\n\n\n<p>Most of the sklearn functions expect that the data with number labels rather than word labels. Hence, we need to convert such labels into number labels. This process is called label encoding. We can perform label encoding of data with the help of&nbsp;<em>LabelEncoder()</em>&nbsp;function of&nbsp;<em>scikit-learn</em>&nbsp;Python library.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In the following example, Python script will perform the label encoding.</p>\n\n\n\n<p>First, import the required Python libraries as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\nfrom sklearn import preprocessing</code></pre>\n\n\n\n<p>Now, we need to provide the input labels as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>input_labels = &#91;'red','black','red','green','black','yellow','white']</code></pre>\n\n\n\n<p>The next line of code will create the label encoder and train it.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>encoder = preprocessing.LabelEncoder()\nencoder.fit(input_labels)</code></pre>\n\n\n\n<p>The next lines of script will check the performance by encoding the random ordered list −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>test_labels = &#91;'green','red','black']\nencoded_values = encoder.transform(test_labels)\nprint(\"\\nLabels =\", test_labels)\nprint(\"Encoded values =\", list(encoded_values))\nencoded_values = &#91;3,0,4,1]\ndecoded_list = encoder.inverse_transform(encoded_values)</code></pre>\n\n\n\n<p>We can get the list of encoded values with the help of following python script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(\"\\nEncoded values =\", encoded_values)\nprint(\"\\nDecoded labels =\", list(decoded_list))</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Labels = &#91;'green', 'red', 'black']\nEncoded values = &#91;1, 2, 0]\nEncoded values = &#91;3, 0, 4, 1]\nDecoded labels = &#91;'white', 'black', 'yellow', 'green']</code></pre>\n","protected":false},"excerpt":{"rendered":"<p>Introduction Machine Learning algorithms are completely dependent on data because it is the most crucial aspect that makes model training possible. On the other hand, if we won’t be able to make sense out of that data, before feeding it to ML algorithms, a machine will be useless. In simple words, we always need to [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1933,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1373"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1373"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1373/revisions"}],"predecessor-version":[{"id":2348,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1373/revisions/2348"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1933"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1373"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1373"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1373"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1382,"date":"2020-05-21T09:11:46","date_gmt":"2020-05-21T09:11:46","guid":{"rendered":"http://python3.foobrdigital.com/?p=1382"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"data-feature-selection","status":"publish","type":"post","link":"https://python3.foobrdigital.com/data-feature-selection/","title":{"rendered":"Data Feature Selection"},"content":{"rendered":"\n<p>In the previous chapter, we have seen in detail how to pre-process and prepare data for machine learning. In this chapter, let us understand in detail data feature selection and various aspects involved in it.</p>\n\n\n\n<h2>Importance of Data Feature Selection</h2>\n\n\n\n<p>The performance of machine learning model is directly proportional to the data features used to train it. The performance of ML model will be affected negatively if the data features provided to it are irrelevant. On the other hand, use of relevant data features can increase the accuracy of your ML model especially linear and logistic regression.</p>\n\n\n\n<p>Now the question arise that what is automatic feature selection? It may be defined as the process with the help of which we select those features in our data that are most relevant to the output or prediction variable in which we are interested. It is also called attribute selection.</p>\n\n\n\n<p>The following are some of the benefits of automatic feature selection before modeling the data −</p>\n\n\n\n<ul><li>Performing feature selection before data modeling will reduce the over-fitting.</li><li>Performing feature selection before data modeling will increases the accuracy of ML model.</li><li>Performing feature selection before data modeling will reduce the training time</li></ul>\n\n\n\n<h2>Feature Selection Techniques</h2>\n\n\n\n<p>The followings are automatic feature selection techniques that we can use to model ML data in Python −</p>\n\n\n\n<h3>Univariate Selection</h3>\n\n\n\n<p>This feature selection technique is very useful in selecting those features, with the help of statistical testing, having strongest relationship with the prediction variables. We can implement univariate feature selection technique with the help of SelectKBest0class of scikit-learn Python library.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In this example, we will use Pima Indians Diabetes dataset to select 4 of the attributes having best features with the help of chi-square statistical test.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom numpy import set_printoptions\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\npath = r'C:\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(path, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Next, we will separate array into input and output components −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>The following lines of code will select the best features from dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>test = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(X,Y)\n﻿</code></pre>\n\n\n\n<p>We can also summarize the data for output as per our choice. Here, we are setting the precision to 2 and showing the 4 data attributes with best features along with best score of each attribute −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>set_printoptions(precision=2)\nprint(fit.scores_)\nfeatured_data = fit.transform(X)\nprint (\"\\nFeatured data:\\n\", featured_data&#91;0:4])</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91; 111.52 1411.89 17.61 53.11 2175.57 127.67 5.39 181.3 ]\nFeatured data:\n&#91;&#91;148.  0. 33.6 50. ]\n&#91;  85.  0. 26.6 31. ]\n&#91; 183.  0. 23.3 32. ]\n&#91;  89. 94. 28.1 21. ]]</code></pre>\n\n\n\n<h2>Recursive Feature Elimination</h2>\n\n\n\n<p>As the name suggests, RFE (Recursive feature elimination) feature selection technique removes the attributes recursively and builds the model with remaining attributes. We can implement RFE feature selection technique with the help of&nbsp;<em>RFE</em>&nbsp;class of&nbsp;<em>scikit-learn</em>&nbsp;Python library.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we will use RFE with logistic regression algorithm to select the best 3 attributes having the best features from Pima Indians Diabetes dataset to.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\npath = r'C:\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(path, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Next, we will separate the array into its input and output components −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>The following lines of code will select the best features from a dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>model = LogisticRegression()\nrfe = RFE(model, 3)\nfit = rfe.fit(X, Y)\nprint(\"Number of Features: %d\")\nprint(\"Selected Features: %s\")\nprint(\"Feature Ranking: %s\")</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>Number of Features: 3\nSelected Features: &#91; True False False False False True True False]\nFeature Ranking: &#91;1 2 3 5 6 1 1 4]</code></pre>\n\n\n\n<p>We can see in above output, RFE choose preg, mass and pedi as the first 3 best features. They are marked as 1 in the output.</p>\n\n\n\n<h2>Principal Component Analysis (PCA)</h2>\n\n\n\n<p>PCA, generally called data reduction technique, is very useful feature selection technique as it uses linear algebra to transform the dataset into a compressed form. We can implement PCA feature selection technique with the help of PCA class of scikit-learn Python library. We can select number of principal components in the output.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we will use PCA to select best 3 Principal components from Pima Indians Diabetes dataset.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.decomposition import PCA\npath = r'C:\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(path, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Next, we will separate array into input and output components −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>The following lines of code will extract features from dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>pca = PCA(n_components = 3)\nfit = pca.fit(X)\nprint(\"Explained Variance: %s\") % fit.explained_variance_ratio_\nprint(fit.components_)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>Explained Variance: &#91; 0.88854663 0.06159078 0.02579012]\n&#91;&#91; -2.02176587e-03 9.78115765e-02 1.60930503e-02 6.07566861e-02\n9.93110844e-01 1.40108085e-02 5.37167919e-04 -3.56474430e-03]\n&#91; 2.26488861e-02 9.72210040e-01 1.41909330e-01 -5.78614699e-02\n-9.46266913e-02 4.69729766e-02 8.16804621e-04 1.40168181e-01]\n&#91; -2.24649003e-02 1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]</code></pre>\n\n\n\n<p>We can observe from the above output that 3 Principal Components bear little resemblance to the source data.</p>\n\n\n\n<h2>Feature Importance</h2>\n\n\n\n<p>As the name suggests, feature importance technique is used to choose the importance features. It basically uses a trained supervised classifier to select features. We can implement this feature selection technique with the help of ExtraTreeClassifier class of scikit-learn Python library.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In this example, we will use ExtraTreeClassifier to select features from Pima Indians Diabetes dataset.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.ensemble import ExtraTreesClassifier\npath = r'C:\\Desktop\\pima-indians-diabetes.csv'\nnames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(data, names=names)\narray = dataframe.values</code></pre>\n\n\n\n<p>Next, we will separate array into input and output components −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>The following lines of code will extract features from dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>model = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)\n﻿</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91; 0.11070069 0.2213717 0.08824115 0.08068703 0.07281761 0.14548537 0.12654214 0.15415431]</code></pre>\n\n\n\n<p>From the output, we can observe that there are scores for each attribute. The higher the score, higher is the importance of that attribute.</p>\n","protected":false},"excerpt":{"rendered":"<p>In the previous chapter, we have seen in detail how to pre-process and prepare data for machine learning. In this chapter, let us understand in detail data feature selection and various aspects involved in it. Importance of Data Feature Selection The performance of machine learning model is directly proportional to the data features used to [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1934,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1382"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1382"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1382/revisions"}],"predecessor-version":[{"id":2349,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1382/revisions/2349"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1934"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1382"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1382"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1382"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1386,"date":"2020-05-21T09:19:36","date_gmt":"2020-05-21T09:19:36","guid":{"rendered":"http://python3.foobrdigital.com/?p=1386"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"classification-algorithms-introduction","status":"publish","type":"post","link":"https://python3.foobrdigital.com/classification-algorithms-introduction/","title":{"rendered":"Classification Algorithms &#8211; Introduction"},"content":{"rendered":"\n<h2>Introduction to Classification</h2>\n\n\n\n<p>Classification may be defined as the process of predicting class or category from observed values or given data points. The categorized output can have the form such as “Black” or “White” or “spam” or “no spam”.</p>\n\n\n\n<p>Mathematically, classification is the task of approximating a mapping function (f) from input variables (X) to output variables (Y). It is basically belongs to the supervised machine learning in which targets are also provided along with the input data set.</p>\n\n\n\n<p>An example of classification problem can be the spam detection in emails. There can be only two categories of output, “spam” and “no spam”; hence this is a binary type classification.</p>\n\n\n\n<p>To implement this classification, we first need to train the classifier. For this example, “spam” and “no spam” emails would be used as the training data. After successfully train the classifier, it can be used to detect an unknown email.</p>\n\n\n\n<h2>Types of Learners in Classification</h2>\n\n\n\n<p>We have two types of learners in respective to classification problems −</p>\n\n\n\n<h3>Lazy Learners</h3>\n\n\n\n<p>As the name suggests, such kind of learners waits for the testing data to be appeared after storing the training data. Classification is done only after getting the testing data. They spend less time on training but more time on predicting. Examples of lazy learners are K-nearest neighbor and case-based reasoning.</p>\n\n\n\n<h3>Eager Learners</h3>\n\n\n\n<p>As opposite to lazy learners, eager learners construct classification model without waiting for the testing data to be appeared after storing the training data. They spend more time on training but less time on predicting. Examples of eager learners are Decision Trees, Naïve Bayes and Artificial Neural Networks (ANN).</p>\n\n\n\n<h2>Building a Classifier in Python</h2>\n\n\n\n<p>Scikit-learn, a Python library for machine learning can be used to build a classifier in Python. The steps for building a classifier in Python are as follows −</p>\n\n\n\n<p><strong>Step 1: Importing necessary python package</strong></p>\n\n\n\n<p>For building a classifier using scikit-learn, we need to import it. We can import it by using following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import sklearn\n﻿</code></pre>\n\n\n\n<p><strong>Step 2: Importing dataset</strong></p>\n\n\n\n<p>After importing necessary package, we need a dataset to build classification prediction model. We can import it from sklearn dataset or can use other one as per our requirement. We are going to use sklearn’s Breast Cancer Wisconsin Diagnostic Database. We can import it with the help of following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.datasets import load_breast_cancer</code></pre>\n\n\n\n<p>The following script will load the dataset;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>data = load_breast_cancer()</code></pre>\n\n\n\n<p>We also need to organize the data and it can be done with the help of following scripts −</p>\n\n\n\n<pre class=\"wp-block-code\"><code></code></pre>\n\n\n\n<p>The following command will print the name of the labels,&nbsp;<strong>‘malignant’</strong>&nbsp;and&nbsp;<strong>‘benign’</strong>&nbsp;in case of our database.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(label_names)</code></pre>\n\n\n\n<p>The output of the above command is the names of the labels −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;'malignant' 'benign']</code></pre>\n\n\n\n<p>These labels are mapped to binary values 0 and 1.&nbsp;<strong>Malignant</strong>&nbsp;cancer is represented by 0 and&nbsp;<strong>Benign</strong>&nbsp;cancer is represented by 1.</p>\n\n\n\n<p>The feature names and feature values of these labels can be seen with the help of following commands −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(feature_names&#91;0])</code></pre>\n\n\n\n<p>The output of the above command is the names of the features for label 0 i.e.&nbsp;<strong>Malignant</strong>&nbsp;cancer −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>mean radius</code></pre>\n\n\n\n<p>Similarly, names of the features for label can be produced as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(feature_names&#91;1])</code></pre>\n\n\n\n<p>The output of the above command is the names of the features for label 1 i.e. Benign cancer −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>mean texture</code></pre>\n\n\n\n<p>We can print the features for these labels with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(features&#91;0])</code></pre>\n\n\n\n<p>This will give the following output −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n 4.601e-01 1.189e-01]</code></pre>\n\n\n\n<p>We can print the features for these labels with the help of following command −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(features&#91;1])</code></pre>\n\n\n\n<p>This will give the following output −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n7.017e-02  1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n5.225e-03  1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n2.341e+01  1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n2.750e-01  8.902e-02]</code></pre>\n\n\n\n<p><strong>Step 3: Organizing data into training &amp; testing sets</strong></p>\n\n\n\n<p>As we need to test our model on unseen data, we will divide our dataset into two parts: a training set and a test set. We can use&nbsp;<em>train_test_split()</em>&nbsp;function of&nbsp;<em>sklearn</em>&nbsp;python package to split the data into sets. The following command will import the function −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.model_selection import train_test_split</code></pre>\n\n\n\n<p>Now, next command will split the data into training &amp; testing data. In this example, we are using taking 40 percent of the data for testing purpose and 60 percent of the data for training purpose −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>train, test, train_labels, test_labels = \n   train_test_split(features,labels,test_size = 0.40, random_state = 42)</code></pre>\n\n\n\n<p><strong>Step 4: Model evaluation</strong></p>\n\n\n\n<p>After dividing the data into training and testing we need to build the model. We will be using&nbsp;<em>Naïve Bayes</em>&nbsp;algorithm for this purpose. The following commands will import the&nbsp;<em>GaussianNB</em>&nbsp;module −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.naive_bayes import GaussianNB</code></pre>\n\n\n\n<p>Now, initialize the model as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>gnb = GaussianNB()\n﻿</code></pre>\n\n\n\n<p>Next, with the help of following command we can train the model −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>model = gnb.fit(train, train_labels)</code></pre>\n\n\n\n<p>Now, for evaluation purpose we need to make predictions. It can be done by using predict() function as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>preds = gnb.predict(test)\nprint(preds)</code></pre>\n\n\n\n<p>This will give the following output −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0\n 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n 0 0 1 1 0 1]</code></pre>\n\n\n\n<p>The above series of 0s and 1s in output are the predicted values for the&nbsp;<strong>Malignant</strong>&nbsp;and&nbsp;<strong>Benign</strong>&nbsp;tumor classes.</p>\n\n\n\n<p><strong>Step 5: Finding accuracy</strong></p>\n\n\n\n<p>We can find the accuracy of the model build in previous step by comparing the two arrays namely&nbsp;<em>test_labels</em>&nbsp;and&nbsp;<em>preds</em>. We will be using the&nbsp;<em>accuracy_score()</em>&nbsp;function to determine the accuracy.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import accuracy_score\nprint(accuracy_score(test_labels,preds))\n0.951754385965</code></pre>\n\n\n\n<p>The above output shows that&nbsp;<em>NaïveBayes</em>&nbsp;classifier is 95.17% accurate.</p>\n\n\n\n<h2>Classification Evaluation Metrics</h2>\n\n\n\n<p>The job is not done even if you have finished implementation of your Machine Learning application or model. We must have to find out how effective our model is? There can be different evaluation metrics, but we must choose it carefully because the choice of metrics influences how the performance of a machine learning algorithm is measured and compared.</p>\n\n\n\n<p>The following are some of the important classification evaluation metrics among which you can choose based upon your dataset and kind of problem −</p>\n\n\n\n<h2>Confusion Matrix</h2>\n\n\n\n<ul><li><strong>Confusion Matrix</strong>&nbsp;− It is the easiest way to measure the performance of a classification problem where the output can be of two or more type of classes.</li></ul>\n\n\n\n<h2>Various ML Classification Algorithms</h2>\n\n\n\n<p>The followings are some important ML classification algorithms −</p>\n\n\n\n<ul><li>Logistic Regression</li><li>Support Vector Machine (SVM)</li><li>Decision Tree</li><li>Naïve Bayes</li><li>Random Forest</li></ul>\n\n\n\n<p>We will be discussing all these classification algorithms in detail in further chapters.</p>\n\n\n\n<h2>Applications</h2>\n\n\n\n<p>Some of the most important applications of classification algorithms are as follows −</p>\n\n\n\n<ul><li>Speech Recognition</li><li>Handwriting Recognition</li><li>Biometric Identification</li><li>Document Classification</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Classification Classification may be defined as the process of predicting class or category from observed values or given data points. The categorized output can have the form such as “Black” or “White” or “spam” or “no spam”. Mathematically, classification is the task of approximating a mapping function (f) from input variables (X) to [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1935,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1386"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1386"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1386/revisions"}],"predecessor-version":[{"id":1955,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1386/revisions/1955"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1935"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1386"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1386"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1386"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1390,"date":"2020-05-21T09:20:21","date_gmt":"2020-05-21T09:20:21","guid":{"rendered":"http://python3.foobrdigital.com/?p=1390"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"logistic-regression","status":"publish","type":"post","link":"https://python3.foobrdigital.com/logistic-regression/","title":{"rendered":"Logistic Regression"},"content":{"rendered":"\n<h2>Introduction to Logistic Regression</h2>\n\n\n\n<p>Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.</p>\n\n\n\n<p>In simple words, the dependent variable is binary in nature having data coded as either 1 (stands for success/yes) or 0 (stands for failure/no).</p>\n\n\n\n<p>Mathematically, a logistic regression model predicts P(Y=1) as a function of X. It is one of the simplest ML algorithms that can be used for various classification problems such as spam detection, Diabetes prediction, cancer detection etc.</p>\n\n\n\n<h2>Types of Logistic Regression</h2>\n\n\n\n<p>Generally, logistic regression means binary logistic regression having binary target variables, but there can be two more categories of target variables that can be predicted by it. Based on those number of categories, Logistic regression can be divided into following types −</p>\n\n\n\n<h3>Binary or Binomial</h3>\n\n\n\n<p>In such a kind of classification, a dependent variable will have only two possible types either 1 and 0. For example, these variables may represent success or failure, yes or no, win or loss etc.</p>\n\n\n\n<h3>Multinomial</h3>\n\n\n\n<p>In such a kind of classification, dependent variable can have 3 or more possible&nbsp;<strong><em>unordered</em></strong>&nbsp;types or the types having no quantitative significance. For example, these variables may represent “Type A” or “Type B” or “Type C”.</p>\n\n\n\n<h3>Ordinal</h3>\n\n\n\n<p>In such a kind of classification, dependent variable can have 3 or more possible&nbsp;<strong><em>ordered</em></strong>&nbsp;types or the types having a quantitative significance. For example, these variables may represent “poor” or “good”, “very good”, “Excellent” and each category can have the scores like 0,1,2,3.</p>\n\n\n\n<h2>Logistic Regression Assumptions</h2>\n\n\n\n<p>Before diving into the implementation of logistic regression, we must be aware of the following assumptions about the same −</p>\n\n\n\n<ul><li>In case of binary logistic regression, the target variables must be binary always and the desired outcome is represented by the factor level 1.</li><li>There should not be any multi-collinearity in the model, which means the independent variables must be independent of each other.</li><li>We must include meaningful variables in our model.</li><li>We should choose a large sample size for logistic regression.</li></ul>\n\n\n\n<h2>Regression Models</h2>\n\n\n\n<ul><li>Binary Logistic Regression Model&nbsp;− The simplest form of logistic regression is binary or binomial logistic regression in which the target or dependent variable can have only 2 possible types either 1 or 0.</li><li>Multinomial Logistic Regression Model&nbsp;− Another useful form of logistic regression is multinomial logistic regression in which the target or dependent variable can have 3 or more possible&nbsp;<strong><em>unordered</em></strong>&nbsp;types i.e. the types having no quantitative significance.</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Logistic Regression Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes. In simple words, the dependent variable is binary in nature having data coded as either 1 [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1936,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1390"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1390"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1390/revisions"}],"predecessor-version":[{"id":1956,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1390/revisions/1956"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1936"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1390"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1390"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1390"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1392,"date":"2020-05-21T09:24:34","date_gmt":"2020-05-21T09:24:34","guid":{"rendered":"http://python3.foobrdigital.com/?p=1392"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"ml-support-vector-machinesvm","status":"publish","type":"post","link":"https://python3.foobrdigital.com/ml-support-vector-machinesvm/","title":{"rendered":"ML &#8211; Support Vector Machine(SVM)"},"content":{"rendered":"\n<h2>Introduction to SVM</h2>\n\n\n\n<p>Support vector machines (SVMs) are powerful yet flexible supervised machine learning algorithms which are used both for classification and regression. But generally, they are used in classification problems. In 1960s, SVMs were first introduced but later they got refined in 1990. SVMs have their unique way of implementation as compared to other machine learning algorithms. Lately, they are extremely popular because of their ability to handle multiple continuous and categorical variables.</p>\n\n\n\n<h2>Working of SVM</h2>\n\n\n\n<p>An SVM model is basically a representation of different classes in a hyperplane in multidimensional space. The hyperplane will be generated in an iterative manner by SVM so that the error can be minimized. The goal of SVM is to divide the datasets into classes to find a maximum marginal hyperplane (MMH).</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"275\" height=\"209\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm.png\" alt=\"\" class=\"wp-image-1393\"/></figure>\n\n\n\n<p>The followings are important concepts in SVM −</p>\n\n\n\n<ul><li><strong>Support Vectors</strong>&nbsp;− Datapoints that are closest to the hyperplane is called support vectors. Separating line will be defined with the help of these data points.</li><li><strong>Hyperplane</strong>&nbsp;− As we can see in the above diagram, it is a decision plane or space which is divided between a set of objects having different classes.</li><li><strong>Margin</strong>&nbsp;− It may be defined as the gap between two lines on the closet data points of different classes. It can be calculated as the perpendicular distance from the line to the support vectors. Large margin is considered as a good margin and small margin is considered as a bad margin.</li></ul>\n\n\n\n<p>The main goal of SVM is to divide the datasets into classes to find a maximum marginal hyperplane (MMH) and it can be done in the following two steps −</p>\n\n\n\n<ul><li>First, SVM will generate hyperplanes iteratively that segregates the classes in best way.</li><li>Then, it will choose the hyperplane that separates the classes correctly.</li></ul>\n\n\n\n<h2>Implementing SVM in Python</h2>\n\n\n\n<ul><li>For implementing SVM in Python&nbsp;− We will start with the standard libraries import as follows −</li></ul>\n\n\n\n<h2>SVM Kernels</h2>\n\n\n\n<p>In practice, SVM algorithm is implemented with kernel that transforms an input data space into the required form. SVM uses a technique called the kernel trick in which kernel takes a low dimensional input space and transforms it into a higher dimensional space. In simple words, kernel converts non-separable problems into separable problems by adding more dimensions to it. It makes SVM more powerful, flexible and accurate. The following are some of the types of kernels used by SVM.</p>\n\n\n\n<h3>Linear Kernel</h3>\n\n\n\n<p>It can be used as a dot product between any two observations. The formula of linear kernel is as below −K(x,xi)=sum(x∗xi)K(x,xi)=sum(x∗xi)</p>\n\n\n\n<p>From the above formula, we can see that the product between two vectors say 𝑥 &amp; 𝑥𝑖 is the sum of the multiplication of each pair of input values.</p>\n\n\n\n<h3>Polynomial Kernel</h3>\n\n\n\n<p>It is more generalized form of linear kernel and distinguish curved or nonlinear input space. Following is the formula for polynomial kernel −k(X,Xi)=1+sum(X∗Xi)^dk(X,Xi)=1+sum(X∗Xi)^d</p>\n\n\n\n<p>Here d is the degree of polynomial, which we need to specify manually in the learning algorithm.</p>\n\n\n\n<h3>Radial Basis Function (RBF) Kernel</h3>\n\n\n\n<p>RBF kernel, mostly used in SVM classification, maps input space in indefinite dimensional space. Following formula explains it mathematically −K(x,xi)=exp(−gamma∗sum(x−xi^2))K(x,xi)=exp(−gamma∗sum(x−xi^2))</p>\n\n\n\n<p>Here,&nbsp;<em>gamma</em>&nbsp;ranges from 0 to 1. We need to manually specify it in the learning algorithm. A good default value of&nbsp;<em>gamma</em>&nbsp;is 0.1.</p>\n\n\n\n<p>As we implemented SVM for linearly separable data, we can implement it in Python for the data that is not linearly separable. It can be done by using kernels.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>The following is an example for creating an SVM classifier by using kernels. We will be using&nbsp;<em>iris</em>&nbsp;dataset from&nbsp;<em>scikit-learn</em>&nbsp;−</p>\n\n\n\n<p>We will start by importing following packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import pandas as pd\nimport numpy as np\nfrom sklearn import svm, datasets\nimport matplotlib.pyplot as plt</code></pre>\n\n\n\n<p>Now, we need to load the input data −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>iris = datasets.load_iris()</code></pre>\n\n\n\n<p>From this dataset, we are taking first two features as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = iris.data&#91;:, :2]\ny = iris.target\n﻿</code></pre>\n\n\n\n<p>Next, we will plot the SVM boundaries with original data as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>x_min, x_max = X&#91;:, 0].min() - 1, X&#91;:, 0].max() + 1\ny_min, y_max = X&#91;:, 1].min() - 1, X&#91;:, 1].max() + 1\nh = (x_max / x_min)/100\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nX_plot = np.c_&#91;xx.ravel(), yy.ravel()]</code></pre>\n\n\n\n<p>Now, we need to provide the value of regularization parameter as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>C = 1.0</code></pre>\n\n\n\n<p>Next, SVM classifier object can be created as follows −</p>\n\n\n\n<p>Svc_classifier = svm.SVC(kernel=&#8217;linear&#8217;, C=C).fit(X, y)</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Z = svc_classifier.predict(X_plot)\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3)\nplt.scatter(X&#91;:, 0], X&#91;:, 1], c=y, cmap=plt.cm.Set1)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.xlim(xx.min(), xx.max())\nplt.title('Support Vector Classifier with linear kernel')</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Text(0.5, 1.0, 'Support Vector Classifier with linear kernel')</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"405\" height=\"288\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-1.png\" alt=\"\" class=\"wp-image-1402\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-1.png 405w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-1-300x213.png 300w\" sizes=\"(max-width: 405px) 100vw, 405px\" /></figure>\n\n\n\n<p>For creating SVM classifier with&nbsp;<strong>rbf</strong>&nbsp;kernel, we can change the kernel to&nbsp;<strong>rbf</strong>&nbsp;as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>Svc_classifier = svm.SVC(kernel = 'rbf', gamma =‘auto’,C = C).fit(X, y)\nZ = svc_classifier.predict(X_plot)\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.contourf(xx, yy, Z, cmap = plt.cm.tab10, alpha = 0.3)\nplt.scatter(X&#91;:, 0], X&#91;:, 1], c = y, cmap = plt.cm.Set1)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.xlim(xx.min(), xx.max())\nplt.title('Support Vector Classifier with rbf kernel')</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Text(0.5, 1.0, 'Support Vector Classifier with rbf kernel')</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"389\" height=\"282\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-2.png\" alt=\"\" class=\"wp-image-1403\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-2.png 389w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-2-300x217.png 300w\" sizes=\"(max-width: 389px) 100vw, 389px\" /></figure>\n\n\n\n<p>We put the value of gamma to ‘auto’ but you can provide its value between 0 to 1 also.</p>\n\n\n\n<h2>Pros and Cons of SVM Classifiers</h2>\n\n\n\n<p><strong>Pros of SVM classifiers</strong></p>\n\n\n\n<p>SVM classifiers offers great accuracy and work well with high dimensional space. SVM classifiers basically use a subset of training points hence in result uses very less memory.</p>\n\n\n\n<p><strong>Cons of SVM classifiers</strong></p>\n\n\n\n<p>They have high training time hence in practice not suitable for large datasets. Another disadvantage is that SVM classifiers do not work well with overlapping classes.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to SVM Support vector machines (SVMs) are powerful yet flexible supervised machine learning algorithms which are used both for classification and regression. But generally, they are used in classification problems. In 1960s, SVMs were first introduced but later they got refined in 1990. SVMs have their unique way of implementation as compared to other [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1937,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1392"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1392"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1392/revisions"}],"predecessor-version":[{"id":1957,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1392/revisions/1957"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1937"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1392"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1392"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1392"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1406,"date":"2020-05-21T09:28:16","date_gmt":"2020-05-21T09:28:16","guid":{"rendered":"http://python3.foobrdigital.com/?p=1406"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"classification-algorithms-decision-tree","status":"publish","type":"post","link":"https://python3.foobrdigital.com/classification-algorithms-decision-tree/","title":{"rendered":"Classification Algorithms &#8211; Decision Tree"},"content":{"rendered":"\n<h2>Introduction to Decision Tree</h2>\n\n\n\n<p>In general, Decision tree analysis is a predictive modelling tool that can be applied across many areas. Decision trees can be constructed by an algorithmic approach that can split the dataset in different ways based on different conditions. Decisions trees are the most powerful algorithms that falls under the category of supervised algorithms.</p>\n\n\n\n<p>They can be used for both classification and regression tasks. The two main entities of a tree are decision nodes, where the data is split and leaves, where we got outcome. The example of a binary tree for predicting whether a person is fit or unfit providing various information like age, eating habits and exercise habits, is given below −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"547\" height=\"345\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-3.png\" alt=\"\" class=\"wp-image-1407\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-3.png 547w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-3-300x189.png 300w\" sizes=\"(max-width: 547px) 100vw, 547px\" /></figure>\n\n\n\n<p>In the above decision tree, the question are decision nodes and final outcomes are leaves. We have the following two types of decision trees.</p>\n\n\n\n<ul><li><strong>Classification decision trees</strong>&nbsp;− In this kind of decision trees, the decision variable is categorical. The above decision tree is an example of classification decision tree.</li><li><strong>Regression decision trees</strong>&nbsp;− In this kind of decision trees, the decision variable is continuous.</li></ul>\n\n\n\n<h2>Implementing Decision Tree Algorithm</h2>\n\n\n\n<h3>Gini Index</h3>\n\n\n\n<p>It is the name of the cost function that is used to evaluate the binary splits in the dataset and works with the categorial target variable “Success” or “Failure”.</p>\n\n\n\n<p>Higher the value of Gini index, higher the homogeneity. A perfect Gini index value is 0 and worst is 0.5 (for 2 class problem). Gini index for a split can be calculated with the help of following steps −</p>\n\n\n\n<ul><li>First, calculate Gini index for sub-nodes by using the formula&nbsp;<em>p^2+q^2</em>, which is the sum of the square of probability for success and failure.</li><li>Next, calculate Gini index for split using weighted Gini score of each node of that split.</li></ul>\n\n\n\n<p>Classification and Regression Tree (CART) algorithm uses Gini method to generate binary splits.</p>\n\n\n\n<h3>Split Creation</h3>\n\n\n\n<p>A split is basically including an attribute in the dataset and a value. We can create a split in dataset with the help of following three parts −</p>\n\n\n\n<ul><li><strong>Part 1: Calculating Gini Score</strong>&nbsp;− We have just discussed this part in the previous section.</li><li><strong>Part 2: Splitting a dataset</strong>&nbsp;− It may be defined as separating a dataset into two lists of rows having index of an attribute and a split value of that attribute. After getting the two groups &#8211; right and left, from the dataset, we can calculate the value of split by using Gini score calculated in first part. Split value will decide in which group the attribute will reside.</li><li><strong>Part 3: Evaluating all splits</strong>&nbsp;− Next part after finding Gini score and splitting dataset is the evaluation of all splits. For this purpose, first, we must check every value associated with each attribute as a candidate split. Then we need to find the best possible split by evaluating the cost of the split. The best split will be used as a node in the decision tree.</li></ul>\n\n\n\n<h2>Building a Tree</h2>\n\n\n\n<p>As we know that a tree has root node and terminal nodes. After creating the root node, we can build the tree by following two parts −</p>\n\n\n\n<h3>Part 1: Terminal node creation</h3>\n\n\n\n<p>While creating terminal nodes of decision tree, one important point is to decide when to stop growing tree or creating further terminal nodes. It can be done by using two criteria namely maximum tree depth and minimum node records as follows −</p>\n\n\n\n<ul><li><strong>Maximum Tree Depth</strong>&nbsp;− As name suggests, this is the maximum number of the nodes in a tree after root node. We must stop adding terminal nodes once a tree reached at maximum depth i.e. once a tree got maximum number of terminal nodes.</li><li><strong>Minimum Node Records</strong>&nbsp;− It may be defined as the minimum number of training patterns that a given node is responsible for. We must stop adding terminal nodes once tree reached at these minimum node records or below this minimum.</li></ul>\n\n\n\n<p>Terminal node is used to make a final prediction.</p>\n\n\n\n<h3>Part 2: Recursive Splitting</h3>\n\n\n\n<p>As we understood about when to create terminal nodes, now we can start building our tree. Recursive splitting is a method to build the tree. In this method, once a node is created, we can create the child nodes (nodes added to an existing node) recursively on each group of data, generated by splitting the dataset, by calling the same function again and again.</p>\n\n\n\n<h3>Prediction</h3>\n\n\n\n<p>After building a decision tree, we need to make a prediction about it. Basically, prediction involves navigating the decision tree with the specifically provided row of data.</p>\n\n\n\n<p>We can make a prediction with the help of recursive function, as did above. The same prediction routine is called again with the left or the child right nodes.</p>\n\n\n\n<h3>Assumptions</h3>\n\n\n\n<p>The following are some of the assumptions we make while creating decision tree −</p>\n\n\n\n<ul><li>While preparing decision trees, the training set is as root node.</li><li>Decision tree classifier prefers the features values to be categorical. In case if you want to use continuous values then they must be done discretized prior to model building.</li><li>Based on the attribute’s values, the records are recursively distributed.</li><li>Statistical approach will be used to place attributes at any node position i.e.as root node or internal node.</li></ul>\n\n\n\n<h2>Implementation in Python</h2>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>In the following example, we are going to implement Decision Tree classifier on Pima Indian Diabetes −</p>\n\n\n\n<p>First, start with importing necessary python packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n﻿</code></pre>\n\n\n\n<p>Next, download the iris dataset from its weblink as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>col_names = &#91;'pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\npima = pd.read_csv(r\"C:\\pima-indians-diabetes.csv\", header = None, names = col_names)\npima.head()</code></pre>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><th></th><th>Pregnant</th><th>Glucose</th><th>BP</th><th>Skin</th><th>Insulin</th><th>Bmi</th><th>Pedigree</th><th>Age</th><th>Label</th></tr><tr><td>0</td><td>6</td><td>148</td><td>72</td><td>35</td><td>0</td><td>33.6</td><td>0.627</td><td>50</td><td>1</td></tr><tr><td>1</td><td>1</td><td>85</td><td>66</td><td>29</td><td>0</td><td>33.6</td><td>0.627</td><td>50</td><td>1</td></tr><tr><td>2</td><td>8</td><td>183</td><td>64</td><td>0</td><td>0</td><td>23.3</td><td>0.672</td><td>32</td><td>1</td></tr><tr><td>3</td><td>1</td><td>89</td><td>66</td><td>23</td><td>94</td><td>28.1</td><td>0.167</td><td>21</td><td>0</td></tr><tr><td>4</td><td>0</td><td>137</td><td>40</td><td>35</td><td>168</td><td>43.1</td><td>2.288</td><td>33</td><td>1</td></tr></tbody></table></figure>\n\n\n\n<p>Now, split the dataset into features and target variable as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>feature_cols = &#91;'pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\nX = pima&#91;feature_cols] # Features\ny = pima.label # Target variable</code></pre>\n\n\n\n<p>Next, we will divide the data into train and test split. The following code will split the dataset into 70% training data and 30% of testing data −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)</code></pre>\n\n\n\n<p>Next, train the model with the help of DecisionTreeClassifier class of sklearn as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>clf = DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)</code></pre>\n\n\n\n<p>At last we need to make prediction. It can be done with the help of following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>y_pred = clf.predict(X_test)</code></pre>\n\n\n\n<p>Next, we can get the accuracy score, confusion matrix and classification report as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",result2)</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Confusion Matrix:\n   &#91;&#91;116 30]\n   &#91; 46 39]]\nClassification Report:\n              precision     recall      f1-score     support\n        0          0.72       0.79          0.75         146\n        1          0.57       0.46          0.51          85\nmicro avg          0.67       0.67          0.67         231\nmacro avg          0.64       0.63          0.63         231\nweighted avg       0.66       0.67          0.66         231\n\nAccuracy: 0.670995670995671</code></pre>\n\n\n\n<h3>Visualizing Decision Tree</h3>\n\n\n\n<p>The above decision tree can be visualized with the help of following code −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data, filled=True, rounded=True,\n   special_characters=True,feature_names = feature_cols,class_names=&#91;'0','1'])\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\ngraph.write_png('Pima_diabetes_Tree.png')\nImage(graph.create_png())</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"600\" height=\"431\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-4.png\" alt=\"\" class=\"wp-image-1408\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-4.png 600w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-4-300x216.png 300w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></figure>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Decision Tree In general, Decision tree analysis is a predictive modelling tool that can be applied across many areas. Decision trees can be constructed by an algorithmic approach that can split the dataset in different ways based on different conditions. Decisions trees are the most powerful algorithms that falls under the category of [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1938,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1406"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1406"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1406/revisions"}],"predecessor-version":[{"id":1409,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1406/revisions/1409"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1938"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1406"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1406"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1406"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1410,"date":"2020-05-21T09:30:36","date_gmt":"2020-05-21T09:30:36","guid":{"rendered":"http://python3.foobrdigital.com/?p=1410"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"classification-algorithms-naive-bayes","status":"publish","type":"post","link":"https://python3.foobrdigital.com/classification-algorithms-naive-bayes/","title":{"rendered":"Classification Algorithms &#8211; Naïve Bayes"},"content":{"rendered":"\n<h2>Introduction to Naïve Bayes Algorithm</h2>\n\n\n\n<p>Naïve Bayes algorithms is a classification technique based on applying Bayes’ theorem with a strong assumption that all the predictors are independent to each other. In simple words, the assumption is that the presence of a feature in a class is independent to the presence of any other feature in the same class. For example, a phone may be considered as smart if it is having touch screen, internet facility, good camera etc. Though all these features are dependent on each other, they contribute independently to the probability of that the phone is a smart phone.</p>\n\n\n\n<p>In Bayesian classification, the main interest is to find the posterior probabilities i.e. the probability of a label given some observed features, 𝑃(𝐿 | 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠). With the help of Bayes theorem, we can express this in quantitative form as follows −P(L|features)=P(L)P(features|L)P(features)P(L|features)=P(L)P(features|L)P(features)</p>\n\n\n\n<p>Here, (𝐿 | 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠) is the posterior probability of class.</p>\n\n\n\n<p>𝑃(𝐿) is the prior probability of class.</p>\n\n\n\n<p>𝑃(𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠|𝐿) is the likelihood which is the probability of predictor given class.</p>\n\n\n\n<p>𝑃(𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠) is the prior probability of predictor.</p>\n\n\n\n<h2>Building model using Naïve Bayes in Python</h2>\n\n\n\n<p>Python library, Scikit learn is the most useful library that helps us to build a Naïve Bayes model in Python. We have the following three types of Naïve Bayes model under Scikit learn Python library −</p>\n\n\n\n<h3>Gaussian Naïve Bayes</h3>\n\n\n\n<p>It is the simplest Naïve Bayes classifier having the assumption that the data from each label is drawn from a simple Gaussian distribution.</p>\n\n\n\n<h3>Multinomial Naïve Bayes</h3>\n\n\n\n<p>Another useful Naïve Bayes classifier is Multinomial Naïve Bayes in which the features are assumed to be drawn from a simple Multinomial distribution. Such kind of Naïve Bayes are most appropriate for the features that represents discrete counts.</p>\n\n\n\n<h3>Bernoulli Naïve Bayes</h3>\n\n\n\n<p>Another important model is Bernoulli Naïve Bayes in which features are assumed to be binary (0s and 1s). Text classification with ‘bag of words’ model can be an application of Bernoulli Naïve Bayes.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>Depending on our data set, we can choose any of the Naïve Bayes model explained above. Here, we are implementing Gaussian Naïve Bayes model in Python −</p>\n\n\n\n<p>We will start with required imports as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()</code></pre>\n\n\n\n<p>Now, by using&nbsp;<em>make_blobs()</em>&nbsp;function of&nbsp;<em>Scikit learn</em>, we can generate blobs of points with Gaussian distribution as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.datasets import make_blobs\nX, y = make_blobs(300, 2, centers = 2, random_state = 2, cluster_std = 1.5)\nplt.scatter(X&#91;:, 0], X&#91;:, 1], c = y, s = 50, cmap = 'summer');</code></pre>\n\n\n\n<p>Next, for using&nbsp;<em>GaussianNB</em>&nbsp;model, we need to import and make its object as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.naive_bayes import GaussianNB\nmodel_GBN = GaussianNB()\nmodel_GNB.fit(X, y);</code></pre>\n\n\n\n<p>Now, we have to do prediction. It can be done after generating some new data as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>rng = np.random.RandomState(0)\nXnew = &#91;-6, -14] + &#91;14, 18] * rng.rand(2000, 2)\nynew = model_GNB.predict(Xnew)</code></pre>\n\n\n\n<p>Next, we are plotting new data to find its boundaries −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>plt.scatter(X&#91;:, 0], X&#91;:, 1], c = y, s = 50, cmap = 'summer')\nlim = plt.axis()\nplt.scatter(Xnew&#91;:, 0], Xnew&#91;:, 1], c = ynew, s = 20, cmap = 'summer', alpha = 0.1)\nplt.axis(lim);\n﻿</code></pre>\n\n\n\n<p>Now, with the help of following line of codes, we can find the posterior probabilities of first and second label −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>yprob = model_GNB.predict_proba(Xnew)\nyprob&#91;-10:].round(3)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>array(&#91;&#91;0.998, 0.002],\n   &#91;1. , 0. ],\n   &#91;0.987, 0.013],\n   &#91;1. , 0. ],\n   &#91;1. , 0. ],\n   &#91;1. , 0. ],\n   &#91;1. , 0. ],\n   &#91;1. , 0. ],\n   &#91;0. , 1. ],\n   &#91;0.986, 0.014]])</code></pre>\n\n\n\n<h2>Pros &amp; Cons</h2>\n\n\n\n<h3>Pros</h3>\n\n\n\n<p>The followings are some pros of using Naïve Bayes classifiers −</p>\n\n\n\n<ul><li>Naïve Bayes classification is easy to implement and fast.</li><li>It will converge faster than discriminative models like logistic regression.</li><li>It requires less training data.</li><li>It is highly scalable in nature, or they scale linearly with the number of predictors and data points.</li><li>It can make probabilistic predictions and can handle continuous as well as discrete data.</li><li>Naïve Bayes classification algorithm can be used for binary as well as multi-class classification problems both.</li></ul>\n\n\n\n<h3>Cons</h3>\n\n\n\n<p>The followings are some cons of using Naïve Bayes classifiers −</p>\n\n\n\n<ul><li>One of the most important cons of Naïve Bayes classification is its strong feature independence because in real life it is almost impossible to have a set of features which are completely independent of each other.</li><li>Another issue with Naïve Bayes classification is its ‘zero frequency’ which means that if a categorial variable has a category but not being observed in training data set, then Naïve Bayes model will assign a zero probability to it and it will be unable to make a prediction.</li></ul>\n\n\n\n<h2>Applications of Naïve Bayes classification</h2>\n\n\n\n<p>The following are some common applications of Naïve Bayes classification −</p>\n\n\n\n<ul><li><strong>Real-time prediction</strong>&nbsp;− Due to its ease of implementation and fast computation, it can be used to do prediction in real-time.</li><li><strong>Multi-class prediction</strong>&nbsp;− Naïve Bayes classification algorithm can be used to predict posterior probability of multiple classes of target variable.</li><li><strong>Text classification</strong>&nbsp;− Due to the feature of multi-class prediction, Naïve Bayes classification algorithms are well suited for text classification. That is why it is also used to solve problems like spam-filtering and sentiment analysis.</li><li><strong>Recommendation system</strong>&nbsp;− Along with the algorithms like collaborative filtering, Naïve Bayes makes a Recommendation system which can be used to filter unseen information and to predict weather a user would like the given resource or not.</li></ul>\n\n\n\n<p></p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Naïve Bayes Algorithm Naïve Bayes algorithms is a classification technique based on applying Bayes’ theorem with a strong assumption that all the predictors are independent to each other. In simple words, the assumption is that the presence of a feature in a class is independent to the presence of any other feature in [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1939,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1410"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1410"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1410/revisions"}],"predecessor-version":[{"id":1958,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1410/revisions/1958"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1939"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1410"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1410"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1410"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1413,"date":"2020-05-21T09:33:48","date_gmt":"2020-05-21T09:33:48","guid":{"rendered":"http://python3.foobrdigital.com/?p=1413"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"classification-algorithms-random-forest","status":"publish","type":"post","link":"https://python3.foobrdigital.com/classification-algorithms-random-forest/","title":{"rendered":"Classification Algorithms &#8211; Random Forest"},"content":{"rendered":"\n<h2>Introduction</h2>\n\n\n\n<p>Random forest is a supervised learning algorithm which is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result.</p>\n\n\n\n<h2>Working of Random Forest Algorithm</h2>\n\n\n\n<p>We can understand the working of Random Forest algorithm with the help of following steps −</p>\n\n\n\n<ul><li><strong>Step 1</strong>&nbsp;− First, start with the selection of random samples from a given dataset.</li><li><strong>Step 2</strong>&nbsp;− Next, this algorithm will construct a decision tree for every sample. Then it will get the prediction result from every decision tree.</li><li><strong>Step 3</strong>&nbsp;− In this step, voting will be performed for every predicted result.</li><li><strong>Step 4</strong>&nbsp;− At last, select the most voted prediction result as the final prediction result.</li></ul>\n\n\n\n<p>The following diagram will illustrate its working −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"489\" height=\"377\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-5.png\" alt=\"\" class=\"wp-image-1414\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-5.png 489w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-5-300x231.png 300w\" sizes=\"(max-width: 489px) 100vw, 489px\" /></figure>\n\n\n\n<h2>Implementation in Python</h2>\n\n\n\n<p>First, start with importing necessary Python packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd</code></pre>\n\n\n\n<p>Next, download the iris dataset from its weblink as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"</code></pre>\n\n\n\n<p>Next, we need to assign column names to the dataset as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>headernames = &#91;'sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']</code></pre>\n\n\n\n<p>Now, we need to read dataset to pandas dataframe as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>dataset = pd.read_csv(path, names = headernames)\ndataset.head()\n﻿</code></pre>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><th></th><th>sepal-length</th><th>sepal-width</th><th>petal-length</th><th>petal-width</th><th>Class</th></tr><tr><td>0</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>1</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>2</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>3</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>4</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table></figure>\n\n\n\n<p>Data Preprocessing will be done with the help of following script lines.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = dataset.iloc&#91;:, :-1].values\ny = dataset.iloc&#91;:, 4].values</code></pre>\n\n\n\n<p>Next, we will divide the data into train and test split. The following code will split the dataset into 70% training data and 30% of testing data −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)</code></pre>\n\n\n\n<p>Next, train the model with the help of&nbsp;<em>RandomForestClassifier</em>&nbsp;class of sklearn as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 50)\nclassifier.fit(X_train, y_train)</code></pre>\n\n\n\n<p>At last, we need to make prediction. It can be done with the help of following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>y_pred = classifier.predict(X_test)</code></pre>\n\n\n\n<p>Next, print the results as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",result2)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>Confusion Matrix:\n&#91;&#91;14 0 0]\n   &#91; 0 18 1]\n   &#91; 0 0 12]]\nClassification Report:\n              precision   recall   f1-score   support\n    Iris-setosa    1.00     1.00       1.00        14\nIris-versicolor    1.00     0.95       0.97        19\n Iris-virginica    0.92     1.00       0.96        12\n\n      micro avg    0.98     0.98        0.98       45\n      macro avg    0.97     0.98        0.98       45\n   weighted avg    0.98     0.98        0.98       45\n\nAccuracy: 0.9777777777777777</code></pre>\n\n\n\n<h2>Pros and Cons of Random Forest</h2>\n\n\n\n<h3>Pros</h3>\n\n\n\n<p>The following are the advantages of Random Forest algorithm −</p>\n\n\n\n<ul><li>It overcomes the problem of overfitting by averaging or combining the results of different decision trees.</li><li>Random forests work well for a large range of data items than a single decision tree does.</li><li>Random forest has less variance then single decision tree.</li><li>Random forests are very flexible and possess very high accuracy.</li><li>Scaling of data does not require in random forest algorithm. It maintains good accuracy even after providing data without scaling.</li><li>Random Forest algorithms maintains good accuracy even a large proportion of the data is missing.</li></ul>\n\n\n\n<h3>Cons</h3>\n\n\n\n<p>The following are the disadvantages of Random Forest algorithm −</p>\n\n\n\n<ul><li>Complexity is the main disadvantage of Random forest algorithms.</li><li>Construction of Random forests are much harder and time-consuming than decision trees.</li><li>More computational resources are required to implement Random Forest algorithm.</li><li>It is less intuitive in case when we have a large collection of decision trees.</li><li>The prediction process using random forests is very time-consuming in comparison with other algorithms.</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>Introduction Random forest is a supervised learning algorithm which is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1940,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1413"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1413"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1413/revisions"}],"predecessor-version":[{"id":1959,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1413/revisions/1959"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1940"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1413"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1413"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1413"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1417,"date":"2020-05-21T09:37:17","date_gmt":"2020-05-21T09:37:17","guid":{"rendered":"http://python3.foobrdigital.com/?p=1417"},"modified":"2020-09-24T09:01:10","modified_gmt":"2020-09-24T09:01:10","slug":"regression-algorithms-overview","status":"publish","type":"post","link":"https://python3.foobrdigital.com/regression-algorithms-overview/","title":{"rendered":"Regression Algorithms &#8211; Overview"},"content":{"rendered":"\n<h2>Introduction to Regression</h2>\n\n\n\n<p>Regression is another important and broadly used statistical and machine learning tool. The key objective of regression-based tasks is to predict output labels or responses which are continues numeric values, for the given input data. The output will be based on what the model has learned in training phase. Basically, regression models use the input data features (independent variables) and their corresponding continuous numeric output values (dependent or outcome variables) to learn specific association between inputs and corresponding outputs.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"445\" height=\"306\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-6.png\" alt=\"\" class=\"wp-image-1418\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-6.png 445w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-6-300x206.png 300w\" sizes=\"(max-width: 445px) 100vw, 445px\" /></figure>\n\n\n\n<h2>Types of Regression Models</h2>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"363\" height=\"159\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-7.png\" alt=\"\" class=\"wp-image-1419\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-7.png 363w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-7-300x131.png 300w\" sizes=\"(max-width: 363px) 100vw, 363px\" /></figure>\n\n\n\n<p>Regression models are of following two types −</p>\n\n\n\n<p><strong>Simple regression model</strong>&nbsp;− This is the most basic regression model in which predictions are formed from a single, univariate feature of the data.</p>\n\n\n\n<p><strong>Multiple regression model</strong>&nbsp;− As name implies, in this regression model the predictions are formed from multiple features of the data.</p>\n\n\n\n<h2>Building a Regressor in Python</h2>\n\n\n\n<p>Regressor model in Python can be constructed just like we constructed the classifier. Scikit-learn, a Python library for machine learning can also be used to build a regressor in Python.</p>\n\n\n\n<p>In the following example, we will be building basic regression model that will fit a line to the data i.e. linear regressor. The necessary steps for building a regressor in Python are as follows −</p>\n\n\n\n<p><strong>Step 1: Importing necessary python package</strong></p>\n\n\n\n<p>For building a regressor using scikit-learn, we need to import it along with other necessary packages. We can import the by using following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\nfrom sklearn import linear_model\nimport sklearn.metrics as sm\nimport matplotlib.pyplot as plt</code></pre>\n\n\n\n<p><strong>Step 2: Importing dataset</strong></p>\n\n\n\n<p>After importing necessary package, we need a dataset to build regression prediction model. We can import it from sklearn dataset or can use other one as per our requirement. We are going to use our saved input data. We can import it with the help of following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>input = r'C:\\linear.txt'</code></pre>\n\n\n\n<p>Next, we need to load this data. We are using&nbsp;<em>np.loadtxt</em>&nbsp;function to load it.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>input_data = np.loadtxt(input, delimiter=',')\nX, y = input_data&#91;:, :-1], input_data&#91;:, -1]</code></pre>\n\n\n\n<p><strong>Step 3: Organizing data into training &amp; testing sets</strong></p>\n\n\n\n<p>As we need to test our model on unseen data hence, we will divide our dataset into two parts: a training set and a test set. The following command will perform it −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>training_samples = int(0.6 * len(X))\ntesting_samples = len(X) - num_training\nX_train, y_train = X&#91;:training_samples], y&#91;:training_samples]\nX_test, y_test = X&#91;training_samples:], y&#91;training_samples:]</code></pre>\n\n\n\n<p><strong>Step 4: Model evaluation &amp; prediction</strong></p>\n\n\n\n<p>After dividing the data into training and testing we need to build the model. We will be using LinearRegression() function of Scikit-learn for this purpose. Following command will create a linear regressor object.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>reg_linear = linear_model.LinearRegression()</code></pre>\n\n\n\n<p>Next, train this model with the training samples as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>reg_linear.fit(X_train, y_train)\n﻿</code></pre>\n\n\n\n<p>Now, at last we need to do the prediction with the testing data.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>y_test_pred = reg_linear.predict(X_test)\n﻿</code></pre>\n\n\n\n<p><strong>Step 5: Plot &amp; visualization</strong></p>\n\n\n\n<p>After prediction, we can plot and visualize it with the help of following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>plt.scatter(X_test, y_test, color = 'red')\nplt.plot(X_test, y_test_pred, color = 'black', linewidth = 2)\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"323\" height=\"219\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-8.png\" alt=\"\" class=\"wp-image-1420\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-8.png 323w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-8-300x203.png 300w\" sizes=\"(max-width: 323px) 100vw, 323px\" /></figure>\n\n\n\n<p>In the above output, we can see the regression line between the data points.</p>\n\n\n\n<p><strong>Step 6: Performance computation</strong>&nbsp;− We can also compute the performance of our regression model with the help of various performance metrics as follows.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(\"Regressor model performance:\")\nprint(\"Mean absolute error(MAE) =\", round(sm.mean_absolute_error(y_test, y_test_pred), 2))\nprint(\"Mean squared error(MSE) =\", round(sm.mean_squared_error(y_test, y_test_pred), 2))\nprint(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_test_pred), 2))\nprint(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_test_pred), 2))\nprint(\"R2 score =\", round(sm.r2_score(y_test, y_test_pred), 2))</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Regressor model performance:\nMean absolute error(MAE) = 1.78\nMean squared error(MSE) = 3.89\nMedian absolute error = 2.01\nExplain variance score = -0.09\nR2 score = -0.09</code></pre>\n\n\n\n<h2>Types of ML Regression Algorithms</h2>\n\n\n\n<p>The most useful and popular ML regression algorithm is Linear regression algorithm which further divided into two types namely −</p>\n\n\n\n<ul><li>Simple Linear Regression algorithm</li><li>Multiple Linear Regression algorithm.</li></ul>\n\n\n\n<p>We will discuss about it and implement it in Python in the next chapter.</p>\n\n\n\n<h2>Applications</h2>\n\n\n\n<p>The applications of ML regression algorithms are as follows −</p>\n\n\n\n<p><strong>Forecasting or Predictive analysis</strong>&nbsp;− One of the important uses of regression is forecasting or predictive analysis. For example, we can forecast GDP, oil prices or in simple words the quantitative data that changes with the passage of time.</p>\n\n\n\n<p><strong>Optimization</strong>&nbsp;− We can optimize business processes with the help of regression. For example, a store manager can create a statistical model to understand the peek time of coming of customers.</p>\n\n\n\n<p><strong>Error correction</strong>&nbsp;− In business, taking correct decision is equally important as optimizing the business process. Regression can help us to take correct decision as well in correcting the already implemented decision.</p>\n\n\n\n<p><strong>Economics</strong>&nbsp;− It is the most used tool in economics. We can use regression to predict supply, demand, consumption, inventory investment etc.</p>\n\n\n\n<p><strong>Finance</strong>&nbsp;− A financial company is always interested in minimizing the risk portfolio and want to know the factors that affects the customers. All these can be predicted with the help of regression model.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Regression Regression is another important and broadly used statistical and machine learning tool. The key objective of regression-based tasks is to predict output labels or responses which are continues numeric values, for the given input data. The output will be based on what the model has learned in training phase. Basically, regression models [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1941,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1417"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1417"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1417/revisions"}],"predecessor-version":[{"id":2350,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1417/revisions/2350"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1941"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1417"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1417"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1417"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1422,"date":"2020-05-21T09:38:46","date_gmt":"2020-05-21T09:38:46","guid":{"rendered":"http://python3.foobrdigital.com/?p=1422"},"modified":"2020-09-24T09:01:10","modified_gmt":"2020-09-24T09:01:10","slug":"regression-algorithms-linear-regression","status":"publish","type":"post","link":"https://python3.foobrdigital.com/regression-algorithms-linear-regression/","title":{"rendered":"Regression Algorithms &#8211; Linear Regression"},"content":{"rendered":"\n<h2>Introduction to Linear Regression</h2>\n\n\n\n<p>Linear regression may be defined as the statistical model that analyzes the linear relationship between a dependent variable with given set of independent variables. Linear relationship between variables means that when the value of one or more independent variables will change (increase or decrease), the value of dependent variable will also change accordingly (increase or decrease).</p>\n\n\n\n<p>Mathematically the relationship can be represented with the help of following equation −Y=mX+bY=mX+b</p>\n\n\n\n<p>Here, Y is the dependent variable we are trying to predict.</p>\n\n\n\n<p>X is the independent variable we are using to make predictions.</p>\n\n\n\n<p>m is the slop of the regression line which represents the effect X has on Y</p>\n\n\n\n<p>b is a constant, known as the 𝑌Y-intercept. If X = 0,Y would be equal to 𝑏b.</p>\n\n\n\n<p>Furthermore, the linear relationship can be positive or negative in nature as explained below −</p>\n\n\n\n<h3>Positive Linear Relationship</h3>\n\n\n\n<p>A linear relationship will be called positive if both independent and dependent variable increases. It can be understood with the help of following graph −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"342\" height=\"230\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-9.png\" alt=\"\" class=\"wp-image-1424\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-9.png 342w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-9-300x202.png 300w\" sizes=\"(max-width: 342px) 100vw, 342px\" /></figure>\n\n\n\n<h3>Negative Linear relationship</h3>\n\n\n\n<p>A linear relationship will be called positive if independent increases and dependent variable decreases. It can be understood with the help of following graph −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"396\" height=\"247\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-10.png\" alt=\"\" class=\"wp-image-1425\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-10.png 396w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-10-300x187.png 300w\" sizes=\"(max-width: 396px) 100vw, 396px\" /></figure>\n\n\n\n<h2>Types of Linear Regression</h2>\n\n\n\n<p>Linear regression is of the following two types −</p>\n\n\n\n<ul><li>Simple Linear Regression</li><li>Multiple Linear Regression</li></ul>\n\n\n\n<h2>Assumptions</h2>\n\n\n\n<p>The following are some assumptions about dataset that is made by Linear Regression model −</p>\n\n\n\n<p><strong>Multi-collinearity</strong>&nbsp;− Linear regression model assumes that there is very little or no multi-collinearity in the data. Basically, multi-collinearity occurs when the independent variables or features have dependency in them.</p>\n\n\n\n<p><strong>Auto-correlation</strong>&nbsp;− Another assumption Linear regression model assumes is that there is very little or no auto-correlation in the data. Basically, auto-correlation occurs when there is dependency between residual errors.</p>\n\n\n\n<p><strong>Relationship between variables</strong>&nbsp;− Linear regression model assumes that the relationship between response and feature variables must be linear.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Linear Regression Linear regression may be defined as the statistical model that analyzes the linear relationship between a dependent variable with given set of independent variables. Linear relationship between variables means that when the value of one or more independent variables will change (increase or decrease), the value of dependent variable will also [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1942,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1422"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1422"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1422/revisions"}],"predecessor-version":[{"id":1426,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1422/revisions/1426"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1942"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1422"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1422"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1422"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1427,"date":"2020-05-21T09:40:12","date_gmt":"2020-05-21T09:40:12","guid":{"rendered":"http://python3.foobrdigital.com/?p=1427"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"clustering-algorithms-overview","status":"publish","type":"post","link":"https://python3.foobrdigital.com/clustering-algorithms-overview/","title":{"rendered":"Clustering Algorithms &#8211; Overview"},"content":{"rendered":"\n<h2>Introduction to Clustering</h2>\n\n\n\n<p>Clustering methods are one of the most useful unsupervised ML methods. These methods are used to find similarity as well as the relationship patterns among data samples and then cluster those samples into groups having similarity based on features.</p>\n\n\n\n<p>Clustering is important because it determines the intrinsic grouping among the present unlabeled data. They basically make some assumptions about data points to constitute their similarity. Each assumption will construct different but equally valid clusters.</p>\n\n\n\n<p>For example, below is the diagram which shows clustering system grouped together the similar kind of data in different clusters −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"566\" height=\"182\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-11.png\" alt=\"\" class=\"wp-image-1428\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-11.png 566w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-11-300x96.png 300w\" sizes=\"(max-width: 566px) 100vw, 566px\" /></figure>\n\n\n\n<h2>Cluster Formation Methods</h2>\n\n\n\n<p>It is not necessary that clusters will be formed in spherical form. Followings are some other cluster formation methods −</p>\n\n\n\n<h3>Density-based</h3>\n\n\n\n<p>In these methods, the clusters are formed as the dense region. The advantage of these methods is that they have good accuracy as well as good ability to merge two clusters. Ex. Density-Based Spatial Clustering of Applications with Noise (DBSCAN), Ordering Points to identify Clustering structure (OPTICS) etc.</p>\n\n\n\n<h3>Hierarchical-based</h3>\n\n\n\n<p>In these methods, the clusters are formed as a tree type structure based on the hierarchy. They have two categories namely, Agglomerative (Bottom up approach) and Divisive (Top down approach). Ex. Clustering using Representatives (CURE), Balanced iterative Reducing Clustering using Hierarchies (BIRCH) etc.</p>\n\n\n\n<h3>Partitioning</h3>\n\n\n\n<p>In these methods, the clusters are formed by portioning the objects into k clusters. Number of clusters will be equal to the number of partitions. Ex. K-means, Clustering Large Applications based upon randomized Search (CLARANS).</p>\n\n\n\n<h3>Grid</h3>\n\n\n\n<p>In these methods, the clusters are formed as a grid like structure. The advantage of these methods is that all the clustering operation done on these grids are fast and independent of the number of data objects. Ex. Statistical Information Grid (STING), Clustering in Quest (CLIQUE).</p>\n\n\n\n<h2>Measuring Clustering Performance</h2>\n\n\n\n<p>One of the most important consideration regarding ML model is assessing its performance or you can say model’s quality. In case of supervised learning algorithms, assessing the quality of our model is easy because we already have labels for every example.</p>\n\n\n\n<p>On the other hand, in case of unsupervised learning algorithms we are not that much blessed because we deal with unlabeled data. But still we have some metrics that give the practitioner an insight about the happening of change in clusters depending on algorithm.</p>\n\n\n\n<p>Before we deep dive into such metrics, we must understand that these metrics only evaluates the comparative performance of models against each other rather than measuring the validity of the model’s prediction. Followings are some of the metrics that we can deploy on clustering algorithms to measure the quality of model −</p>\n\n\n\n<h2>Silhouette Analysis</h2>\n\n\n\n<p>Silhouette analysis used to check the quality of clustering model by measuring the distance between the clusters. It basically provides us a way to assess the parameters like number of clusters with the help of&nbsp;<strong>Silhouette score</strong>. This score measures how close each point in one cluster is to points in the neighboring clusters.</p>\n\n\n\n<h2>Analysis of Silhouette Score</h2>\n\n\n\n<ul><li>Analysis of Silhouette Score&nbsp;− The range of&nbsp;<strong>Silhouette score</strong>&nbsp;is [-1, 1].</li></ul>\n\n\n\n<h2>Types of ML Clustering Algorithms</h2>\n\n\n\n<p>The following are the most important and useful ML clustering algorithms −</p>\n\n\n\n<h3>K-means Clustering</h3>\n\n\n\n<p>This clustering algorithm computes the centroids and iterates until we it finds optimal centroid. It assumes that the number of clusters are already known. It is also called&nbsp;<strong>flat clustering</strong>&nbsp;algorithm. The number of clusters identified from data by algorithm is represented by ‘K’ in K-means.</p>\n\n\n\n<h3>Mean-Shift Algorithm</h3>\n\n\n\n<p>It is another powerful clustering algorithm used in unsupervised learning. Unlike K-means clustering, it does not make any assumptions hence it is a non-parametric algorithm.</p>\n\n\n\n<h3>Hierarchical Clustering</h3>\n\n\n\n<p>It is another unsupervised learning algorithm that is used to group together the unlabeled data points having similar characteristics.</p>\n\n\n\n<p>We will be discussing all these algorithms in detail in the upcoming chapters.</p>\n\n\n\n<h2>Applications of Clustering</h2>\n\n\n\n<p>We can find clustering useful in the following areas −</p>\n\n\n\n<p><strong>Data summarization and compression</strong>&nbsp;− Clustering is widely used in the areas where we require data summarization, compression and reduction as well. The examples are image processing and vector quantization.</p>\n\n\n\n<p><strong>Collaborative systems and customer segmentation</strong>&nbsp;− Since clustering can be used to find similar products or same kind of users, it can be used in the area of collaborative systems and customer segmentation.</p>\n\n\n\n<p><strong>Serve as a key intermediate step for other data mining tasks</strong>&nbsp;− Cluster analysis can generate a compact summary of data for classification, testing, hypothesis generation; hence, it serves as a key intermediate step for other data mining tasks also.</p>\n\n\n\n<p><strong>Trend detection in dynamic data</strong>&nbsp;− Clustering can also be used for trend detection in dynamic data by making various clusters of similar trends.</p>\n\n\n\n<p><strong>Social network analysis</strong>&nbsp;− Clustering can be used in social network analysis. The examples are generating sequences in images, videos or audios.</p>\n\n\n\n<p><strong>Biological data analysis</strong>&nbsp;− Clustering can also be used to make clusters of images, videos hence it can successfully be used in biological data analysis.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Clustering Clustering methods are one of the most useful unsupervised ML methods. These methods are used to find similarity as well as the relationship patterns among data samples and then cluster those samples into groups having similarity based on features. Clustering is important because it determines the intrinsic grouping among the present unlabeled [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1943,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1427"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1427"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1427/revisions"}],"predecessor-version":[{"id":1961,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1427/revisions/1961"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1943"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1427"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1427"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1427"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1431,"date":"2020-05-21T09:45:48","date_gmt":"2020-05-21T09:45:48","guid":{"rendered":"http://python3.foobrdigital.com/?p=1431"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"ml-clustering-k-means-algorithm","status":"publish","type":"post","link":"https://python3.foobrdigital.com/ml-clustering-k-means-algorithm/","title":{"rendered":"ML &#8211; Clustering K-Means Algorithm"},"content":{"rendered":"\n<h2>Introduction to K-Means Algorithm</h2>\n\n\n\n<p>K-means clustering algorithm computes the centroids and iterates until we it finds optimal centroid. It assumes that the number of clusters are already known. It is also called&nbsp;<strong>flat clustering</strong>&nbsp;algorithm. The number of clusters identified from data by algorithm is represented by ‘K’ in K-means.</p>\n\n\n\n<p>In this algorithm, the data points are assigned to a cluster in such a manner that the sum of the squared distance between the data points and centroid would be minimum. It is to be understood that less variation within the clusters will lead to more similar data points within same cluster.</p>\n\n\n\n<h2>Working of K-Means Algorithm</h2>\n\n\n\n<p>We can understand the working of K-Means clustering algorithm with the help of following steps −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− First, we need to specify the number of clusters, K, need to be generated by this algorithm.</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Next, randomly select K data points and assign each data point to a cluster. In simple words, classify the data based on the number of data points.</p>\n\n\n\n<p><strong>Step 3</strong>&nbsp;− Now it will compute the cluster centroids.</p>\n\n\n\n<p><strong>Step 4</strong>&nbsp;− Next, keep iterating the following until we find optimal centroid which is the assignment of data points to the clusters that are not changing any more</p>\n\n\n\n<ul><li><strong>4.1</strong>&nbsp;− First, the sum of squared distance between data points and centroids would be computed.</li><li><strong>4.2</strong>&nbsp;− Now, we have to assign each data point to the cluster that is closer than other cluster (centroid).</li><li><strong>4.3</strong>&nbsp;− At last compute the centroids for the clusters by taking the average of all data points of that cluster.</li></ul>\n\n\n\n<p>K-means follows&nbsp;<strong>Expectation-Maximization</strong>&nbsp;approach to solve the problem. The Expectation-step is used for assigning the data points to the closest cluster and the Maximization-step is used for computing the centroid of each cluster.</p>\n\n\n\n<p>While working with K-means algorithm we need to take care of the following things −</p>\n\n\n\n<ul><li>While working with clustering algorithms including K-Means, it is recommended to standardize the data because such algorithms use distance-based measurement to determine the similarity between data points.</li><li>Due to the iterative nature of K-Means and random initialization of centroids, K-Means may stick in a local optimum and may not converge to global optimum. That is why it is recommended to use different initializations of centroids.</li></ul>\n\n\n\n<h2>Implementation in Python</h2>\n\n\n\n<p>The following two examples of implementing K-Means clustering algorithm will help us in its better understanding −</p>\n\n\n\n<h3>Example 1</h3>\n\n\n\n<p>It is a simple example to understand how k-means works. In this example, we are going to first generate 2D dataset containing 4 different blobs and after that will apply k-means algorithm to see the result.</p>\n\n\n\n<p>First, we will start by importing the necessary packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nfrom sklearn.cluster import KMeans</code></pre>\n\n\n\n<p>The following code will generate the 2D, containing four blobs −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)</code></pre>\n\n\n\n<p>Next, the following code will help us to visualize the dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>plt.scatter(X&#91;:, 0], X&#91;:, 1], s = 20);\nplt.show()</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"329\" height=\"223\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-12.png\" alt=\"\" class=\"wp-image-1432\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-12.png 329w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-12-300x203.png 300w\" sizes=\"(max-width: 329px) 100vw, 329px\" /></figure>\n\n\n\n<p>Next, make an object of KMeans along with providing number of clusters, train the model and do the prediction as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>kmeans = KMeans(n_clusters = 4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)</code></pre>\n\n\n\n<p>Now, with the help of following code we can plot and visualize the cluster’s centers picked by k-means Python estimator −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)</code></pre>\n\n\n\n<p>Next, the following code will help us to visualize the dataset −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>plt.scatter(X&#91;:, 0], X&#91;:, 1], c = y_kmeans, s = 20, cmap = 'summer')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers&#91;:, 0], centers&#91;:, 1], c = 'blue', s = 100, alpha = 0.9);\nplt.show()</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"350\" height=\"235\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-13.png\" alt=\"\" class=\"wp-image-1434\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-13.png 350w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-13-300x201.png 300w\" sizes=\"(max-width: 350px) 100vw, 350px\" /></figure>\n\n\n\n<h3>Example 2</h3>\n\n\n\n<p>Let us move to another example in which we are going to apply K-means clustering on simple digits dataset. K-means will try to identify similar digits without using the original label information.</p>\n\n\n\n<p>First, we will start by importing the necessary packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nfrom sklearn.cluster import KMeans</code></pre>\n\n\n\n<p>Next, load the digit dataset from sklearn and make an object of it. We can also find number of rows and columns in this dataset as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.data.shape</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>(1797, 64)</code></pre>\n\n\n\n<p>The above output shows that this dataset is having 1797 samples with 64 features.</p>\n\n\n\n<p>We can perform the clustering as we did in Example 1 above −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>kmeans = KMeans(n_clusters = 10, random_state = 0)\nclusters = kmeans.fit_predict(digits.data)\nkmeans.cluster_centers_.shape</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>(10, 64)\n﻿</code></pre>\n\n\n\n<p>The above output shows that K-means created 10 clusters with 64 features.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>fig, ax = plt.subplots(2, 5, figsize=(8, 3))\ncenters = kmeans.cluster_centers_.reshape(10, 8, 8)\nfor axi, center in zip(ax.flat, centers):\naxi.set(xticks=&#91;], yticks=&#91;])\naxi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<p>As output, we will get following image showing clusters centers learned by k-means.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"469\" height=\"189\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-14.png\" alt=\"\" class=\"wp-image-1435\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-14.png 469w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-14-300x121.png 300w\" sizes=\"(max-width: 469px) 100vw, 469px\" /></figure>\n\n\n\n<p>The following lines of code will match the learned cluster labels with the true labels found in them −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from scipy.stats import mode\nlabels = np.zeros_like(clusters)\nfor i in range(10):\n   mask = (clusters == i)\n   labels&#91;mask] = mode(digits.target&#91;mask])&#91;0]</code></pre>\n\n\n\n<p>Next, we can check the accuracy as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import accuracy_score\naccuracy_score(digits.target, labels)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>0.7935447968836951</code></pre>\n\n\n\n<p>The above output shows that the accuracy is around 80%.</p>\n\n\n\n<h2>Advantages and Disadvantages</h2>\n\n\n\n<h3>Advantages</h3>\n\n\n\n<p>The following are some advantages of K-Means clustering algorithms −</p>\n\n\n\n<ul><li>It is very easy to understand and implement.</li><li>If we have large number of variables then, K-means would be faster than Hierarchical clustering.</li><li>On re-computation of centroids, an instance can change the cluster.</li><li>Tighter clusters are formed with K-means as compared to Hierarchical clustering.</li></ul>\n\n\n\n<h3>Disadvantages</h3>\n\n\n\n<p>The following are some disadvantages of K-Means clustering algorithms −</p>\n\n\n\n<ul><li>It is a bit difficult to predict the number of clusters i.e. the value of k.</li><li>Output is strongly impacted by initial inputs like number of clusters (value of k)</li><li>Order of data will have strong impact on the final output.</li><li>It is very sensitive to rescaling. If we will rescale our data by means of normalization or standardization, then the output will completely change.</li><li>It is not good in doing clustering job if the clusters have a complicated geometric shape.</li></ul>\n\n\n\n<h2>Applications of K-Means Clustering Algorithm</h2>\n\n\n\n<p>The main goals of cluster analysis are −</p>\n\n\n\n<ul><li>To get a meaningful intuition from the data we are working with.</li><li>Cluster-then-predict where different models will be built for different subgroups.</li></ul>\n\n\n\n<p>To fulfill the above-mentioned goals, K-means clustering is performing well enough. It can be used in following applications −</p>\n\n\n\n<ul><li>Market segmentation</li><li>Document Clustering</li><li>Image segmentation</li><li>Image compression</li><li>Customer segmentation</li><li>Analyzing the trend on dynamic data</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to K-Means Algorithm K-means clustering algorithm computes the centroids and iterates until we it finds optimal centroid. It assumes that the number of clusters are already known. It is also called&nbsp;flat clustering&nbsp;algorithm. The number of clusters identified from data by algorithm is represented by ‘K’ in K-means. In this algorithm, the data points are [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1944,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1431"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1431"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1431/revisions"}],"predecessor-version":[{"id":1441,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1431/revisions/1441"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1944"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1431"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1431"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1431"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1442,"date":"2020-05-21T09:48:23","date_gmt":"2020-05-21T09:48:23","guid":{"rendered":"http://python3.foobrdigital.com/?p=1442"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"ml-clustering-mean-shift-algorithm","status":"publish","type":"post","link":"https://python3.foobrdigital.com/ml-clustering-mean-shift-algorithm/","title":{"rendered":"ML &#8211; Clustering Mean Shift Algorithm"},"content":{"rendered":"\n<h2>Introduction to Mean-Shift Algorithm</h2>\n\n\n\n<p>As discussed earlier, it is another powerful clustering algorithm used in unsupervised learning. Unlike K-means clustering, it does not make any assumptions; hence it is a non-parametric algorithm.</p>\n\n\n\n<p>Mean-shift algorithm basically assigns the datapoints to the clusters iteratively by shifting points towards the highest density of datapoints i.e. cluster centroid.</p>\n\n\n\n<p>The difference between K-Means algorithm and Mean-Shift is that later one does not need to specify the number of clusters in advance because the number of clusters will be determined by the algorithm w.r.t data.</p>\n\n\n\n<h2>Working of Mean-Shift Algorithm</h2>\n\n\n\n<p>We can understand the working of Mean-Shift clustering algorithm with the help of following steps −</p>\n\n\n\n<ul><li><strong>Step 1</strong>&nbsp;− First, start with the data points assigned to a cluster of their own.</li><li><strong>Step 2</strong>&nbsp;− Next, this algorithm will compute the centroids.</li><li><strong>Step 3</strong>&nbsp;− In this step, location of new centroids will be updated.</li><li><strong>Step 4</strong>&nbsp;− Now, the process will be iterated and moved to the higher density region.</li><li><strong>Step 5</strong>&nbsp;− At last, it will be stopped once the centroids reach at position from where it cannot move further.</li></ul>\n\n\n\n<h2>Implementation in Python</h2>\n\n\n\n<p>It is a simple example to understand how Mean-Shift algorithm works. In this example, we are going to first generate 2D dataset containing 4 different blobs and after that will apply Mean-Shift algorithm to see the result.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>%matplotlib inline\nimport numpy as np\nfrom sklearn.cluster import MeanShift\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\"ggplot\")\nfrom sklearn.datasets.samples_generator import make_blobs\ncenters = &#91;&#91;3,3,3],&#91;4,5,5],&#91;3,10,10]]\nX, _ = make_blobs(n_samples = 700, centers = centers, cluster_std = 0.5)\nplt.scatter(X&#91;:,0],X&#91;:,1])\nplt.show()</code></pre>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"395\" height=\"261\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-15.png\" alt=\"\" class=\"wp-image-1443\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-15.png 395w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-15-300x198.png 300w\" sizes=\"(max-width: 395px) 100vw, 395px\" /></figure></div>\n\n\n\n<pre class=\"wp-block-code\"><code>ms = MeanShift()\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\nprint(cluster_centers)\nn_clusters_ = len(np.unique(labels))\nprint(\"Estimated clusters:\", n_clusters_)\ncolors = 10*&#91;'r.','g.','b.','c.','k.','y.','m.']\n\nfor i in range(len(X)):\n   plt.plot(X&#91;i]&#91;0], X&#91;i]&#91;1], colors&#91;labels&#91;i]], markersize = 3)\nplt.scatter(cluster_centers&#91;:,0],cluster_centers&#91;:,1],\n   marker = \".\",color = 'k', s = 20, linewidths = 5, zorder = 10)\nplt.show()</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>&#91;&#91; 2.98462798 9.9733794 10.02629344]\n&#91; 3.94758484 4.99122771 4.99349433]\n&#91; 3.00788996 3.03851268 2.99183033]]\nEstimated clusters: 3</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"379\" height=\"250\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-16.png\" alt=\"\" class=\"wp-image-1444\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-16.png 379w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-16-300x198.png 300w\" sizes=\"(max-width: 379px) 100vw, 379px\" /></figure>\n\n\n\n<h2>Advantages and Disadvantages</h2>\n\n\n\n<h3>Advantages</h3>\n\n\n\n<p>The following are some advantages of Mean-Shift clustering algorithm −</p>\n\n\n\n<ul><li>It does not need to make any model assumption as like in K-means or Gaussian mixture.</li><li>It can also model the complex clusters which have nonconvex shape.</li><li>It only needs one parameter named bandwidth which automatically determines the number of clusters.</li><li>There is no issue of local minima as like in K-means.</li><li>No problem generated from outliers.</li></ul>\n\n\n\n<h3>Disadvantages</h3>\n\n\n\n<p>The following are some disadvantages of Mean-Shift clustering algorithm −</p>\n\n\n\n<ul><li>Mean-shift algorithm does not work well in case of high dimension, where number of clusters changes abruptly.</li><li>We do not have any direct control on the number of clusters but in some applications, we need a specific number of clusters.</li><li>It cannot differentiate between meaningful and meaningless modes.</li></ul>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Mean-Shift Algorithm As discussed earlier, it is another powerful clustering algorithm used in unsupervised learning. Unlike K-means clustering, it does not make any assumptions; hence it is a non-parametric algorithm. Mean-shift algorithm basically assigns the datapoints to the clusters iteratively by shifting points towards the highest density of datapoints i.e. cluster centroid. The [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1945,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1442"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1442"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1442/revisions"}],"predecessor-version":[{"id":1445,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1442/revisions/1445"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1945"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1442"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1442"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1442"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1447,"date":"2020-05-21T09:53:29","date_gmt":"2020-05-21T09:53:29","guid":{"rendered":"http://python3.foobrdigital.com/?p=1447"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"hierarchical-clustering","status":"publish","type":"post","link":"https://python3.foobrdigital.com/hierarchical-clustering/","title":{"rendered":"Hierarchical Clustering"},"content":{"rendered":"\n<h2>Introduction to Hierarchical Clustering</h2>\n\n\n\n<p>Hierarchical clustering is another unsupervised learning algorithm that is used to group together the unlabeled data points having similar characteristics. Hierarchical clustering algorithms falls into following two categories.</p>\n\n\n\n<p><strong>Agglomerative hierarchical algorithms</strong>&nbsp;− In agglomerative hierarchical algorithms, each data point is treated as a single cluster and then successively merge or agglomerate (bottom-up approach) the pairs of clusters. The hierarchy of the clusters is represented as a dendrogram or tree structure.</p>\n\n\n\n<p><strong>Divisive hierarchical algorithms</strong>&nbsp;− On the other hand, in divisive hierarchical algorithms, all the data points are treated as one big cluster and the process of clustering involves dividing (Top-down approach) the one big cluster into various small clusters.</p>\n\n\n\n<h2>Steps to Perform Agglomerative Hierarchical Clustering</h2>\n\n\n\n<p>We are going to explain the most used and important Hierarchical clustering i.e. agglomerative. The steps to perform the same is as follows −</p>\n\n\n\n<ul><li><strong>Step 1</strong>&nbsp;− Treat each data point as single cluster. Hence, we will be having, say K clusters at start. The number of data points will also be K at start.</li><li><strong>Step 2</strong>&nbsp;− Now, in this step we need to form a big cluster by joining two closet datapoints. This will result in total of K-1 clusters.</li><li><strong>Step 3</strong>&nbsp;− Now, to form more clusters we need to join two closet clusters. This will result in total of K-2 clusters.</li><li><strong>Step 4</strong>&nbsp;− Now, to form one big cluster repeat the above three steps until K would become 0 i.e. no more data points left to join.</li><li><strong>Step 5</strong>&nbsp;− At last, after making one single big cluster, dendrograms will be used to divide into multiple clusters depending upon the problem.</li></ul>\n\n\n\n<h2>Role of Dendrograms in Agglomerative Hierarchical Clustering</h2>\n\n\n\n<p>As we discussed in the last step, the role of dendrogram starts once the big cluster is formed. Dendrogram will be used to split the clusters into multiple cluster of related data points depending upon our problem. It can be understood with the help of following example −</p>\n\n\n\n<h3>Example 1</h3>\n\n\n\n<p>To understand, let us start with importing the required libraries as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np</code></pre>\n\n\n\n<p>Next, we will be plotting the datapoints we have taken for this example −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = np.array(\n   &#91;&#91;7,8],&#91;12,20],&#91;17,19],&#91;26,15],&#91;32,37],&#91;87,75],&#91;73,85], &#91;62,80],&#91;73,60],&#91;87,96],])\nlabels = range(1, 11)\nplt.figure(figsize = (10, 7))\nplt.subplots_adjust(bottom = 0.1)\nplt.scatter(X&#91;:,0],X&#91;:,1], label = 'True Position')\nfor label, x, y in zip(labels, X&#91;:, 0], X&#91;:, 1]):\n   plt.annotate(\n      label,xy = (x, y), xytext = (-3, 3),textcoords = 'offset points', ha = 'right', va = 'bottom')\nplt.show()</code></pre>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"416\" height=\"291\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-17.png\" alt=\"\" class=\"wp-image-1449\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-17.png 416w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-17-300x210.png 300w\" sizes=\"(max-width: 416px) 100vw, 416px\" /></figure></div>\n\n\n\n<p>From the above diagram, it is very easy to see that we have two clusters in out datapoints but in the real world data, there can be thousands of clusters. Next, we will be plotting the dendrograms of our datapoints by using Scipy library −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\nlinked = linkage(X, 'single')\nlabelList = range(1, 11)\nplt.figure(figsize = (10, 7))\ndendrogram(linked, orientation = 'top',labels = labelList, \n   distance_sort ='descending',show_leaf_counts = True)\nplt.show()</code></pre>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"416\" height=\"285\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-18.png\" alt=\"\" class=\"wp-image-1450\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-18.png 416w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-18-300x206.png 300w\" sizes=\"(max-width: 416px) 100vw, 416px\" /></figure></div>\n\n\n\n<p>Now, once the big cluster is formed, the longest vertical distance is selected. A vertical line is then drawn through it as shown in the following diagram. As the horizontal line crosses the blue line at two points, the number of clusters would be two.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"404\" height=\"279\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-19.png\" alt=\"\" class=\"wp-image-1451\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-19.png 404w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-19-300x207.png 300w\" sizes=\"(max-width: 404px) 100vw, 404px\" /></figure></div>\n\n\n\n<p>Next, we need to import the class for clustering and call its fit_predict method to predict the cluster. We are importing&nbsp;<em>AgglomerativeClustering</em>&nbsp;class of&nbsp;<em>sklearn.cluster</em>&nbsp;library −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\ncluster.fit_predict(X)</code></pre>\n\n\n\n<p>Next, plot the cluster with the help of following code −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>plt.scatter(X&#91;:,0],X&#91;:,1], c = cluster.labels_, cmap = 'rainbow')</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"375\" height=\"251\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-20.png\" alt=\"\" class=\"wp-image-1452\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-20.png 375w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-20-300x201.png 300w\" sizes=\"(max-width: 375px) 100vw, 375px\" /></figure>\n\n\n\n<p>The above diagram shows the two clusters from our datapoints.</p>\n\n\n\n<h3>Example 2</h3>\n\n\n\n<p>As we understood the concept of dendrograms from the simple example discussed above, let us move to another example in which we are creating clusters of the data point in Pima Indian Diabetes Dataset by using hierarchical clustering.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\nimport numpy as np\nfrom pandas import read_csv\npath = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names = headernames)\narray = data.values\nX = array&#91;:,0:8]\nY = array&#91;:,8]\ndata.shape\n(768, 9)\ndata.head()\n﻿</code></pre>\n\n\n\n<figure class=\"wp-block-table\"><table><tbody><tr><th></th><th>Preg</th><th>Plas</th><th>Pres</th><th>Skin</th><th>Test</th><th>Mass</th><th>Pedi</th><th>Age</th><th>Class</th></tr><tr><td>0</td><td>6</td><td>148</td><td>72</td><td>35</td><td>0</td><td>33.6</td><td>0.627</td><td>50</td><td>1</td></tr><tr><td>1</td><td>1</td><td>85</td><td>66</td><td>29</td><td>0</td><td>26.6</td><td>0.351</td><td>31</td><td>0</td></tr><tr><td>2</td><td>8</td><td>183</td><td>64</td><td>0</td><td>0</td><td>23.3</td><td>0.672</td><td>32</td><td>1</td></tr><tr><td>3</td><td>1</td><td>89</td><td>66</td><td>23</td><td>94</td><td>28.1</td><td>0.167</td><td>21</td><td>0</td></tr><tr><td>4</td><td>0</td><td>137</td><td>40</td><td>35</td><td>168</td><td>43.1</td><td>2.288</td><td>33</td><td>1</td></tr></tbody></table></figure>\n\n\n\n<pre class=\"wp-block-code\"><code>patient_data = data.iloc&#91;:, 3:5].values\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize = (10, 7))\nplt.title(\"Patient Dendograms\")\ndend = shc.dendrogram(shc.linkage(data, method = 'ward'))</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"591\" height=\"307\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-21.png\" alt=\"\" class=\"wp-image-1454\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-21.png 591w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-21-300x156.png 300w\" sizes=\"(max-width: 591px) 100vw, 591px\" /></figure>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters = 4, affinity = 'euclidean', linkage = 'ward')\ncluster.fit_predict(patient_data)\nplt.figure(figsize = (10, 7))\nplt.scatter(patient_data&#91;:,0], patient_data&#91;:,1], c = cluster.labels_, cmap = 'rainbow')</code></pre>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"499\" height=\"339\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-22.png\" alt=\"\" class=\"wp-image-1455\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-22.png 499w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-22-300x204.png 300w\" sizes=\"(max-width: 499px) 100vw, 499px\" /></figure>\n","protected":false},"excerpt":{"rendered":"<p>Introduction to Hierarchical Clustering Hierarchical clustering is another unsupervised learning algorithm that is used to group together the unlabeled data points having similar characteristics. Hierarchical clustering algorithms falls into following two categories. Agglomerative hierarchical algorithms&nbsp;− In agglomerative hierarchical algorithms, each data point is treated as a single cluster and then successively merge or agglomerate (bottom-up [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1946,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1447"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1447"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1447/revisions"}],"predecessor-version":[{"id":1457,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1447/revisions/1457"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1946"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1447"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1447"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1447"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1460,"date":"2020-05-21T09:57:50","date_gmt":"2020-05-21T09:57:50","guid":{"rendered":"http://python3.foobrdigital.com/?p=1460"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"knn-algorithm-finding-nearest-neighbors","status":"publish","type":"post","link":"https://python3.foobrdigital.com/knn-algorithm-finding-nearest-neighbors/","title":{"rendered":"KNN Algorithm &#8211; Finding Nearest Neighbors"},"content":{"rendered":"\n<h2>Introduction</h2>\n\n\n\n<p>K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry. The following two properties would define KNN well −</p>\n\n\n\n<ul><li><strong>Lazy learning algorithm</strong>&nbsp;− KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training while classification.</li><li><strong>Non-parametric learning algorithm</strong>&nbsp;− KNN is also a non-parametric learning algorithm because it doesn’t assume anything about the underlying data.</li></ul>\n\n\n\n<h2>Working of KNN Algorithm</h2>\n\n\n\n<p>K-nearest neighbors (KNN) algorithm uses ‘feature similarity’ to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps −</p>\n\n\n\n<p><strong>Step 1</strong>&nbsp;− For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data.</p>\n\n\n\n<p><strong>Step 2</strong>&nbsp;− Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.</p>\n\n\n\n<p><strong>Step 3</strong>&nbsp;− For each point in the test data do the following −</p>\n\n\n\n<ul><li><strong>3.1</strong>&nbsp;− Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean.</li><li><strong>3.2</strong>&nbsp;− Now, based on the distance value, sort them in ascending order.</li><li><strong>3.3</strong>&nbsp;− Next, it will choose the top K rows from the sorted array.</li><li><strong>3.4</strong>&nbsp;− Now, it will assign a class to the test point based on most frequent class of these rows.</li></ul>\n\n\n\n<p><strong>Step 4</strong>&nbsp;− End</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>The following is an example to understand the concept of K and working of KNN algorithm −</p>\n\n\n\n<p>Suppose we have a dataset which can be plotted as follows −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"378\" height=\"253\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-23.png\" alt=\"\" class=\"wp-image-1461\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-23.png 378w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-23-300x201.png 300w\" sizes=\"(max-width: 378px) 100vw, 378px\" /></figure>\n\n\n\n<p>Now, we need to classify new data point with black dot (at point 60,60) into blue or red class. We are assuming K = 3 i.e. it would find three nearest data points. It is shown in the next diagram −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"380\" height=\"249\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-24.png\" alt=\"\" class=\"wp-image-1462\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-24.png 380w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-24-300x197.png 300w\" sizes=\"(max-width: 380px) 100vw, 380px\" /></figure>\n\n\n\n<p>We can see in the above diagram the three nearest neighbors of the data point with black dot. Among those three, two of them lies in Red class hence the black dot will also be assigned in red class.</p>\n\n\n\n<h2>Implementation in Python</h2>\n\n\n\n<p>As we know K-nearest neighbors (KNN) algorithm can be used for both classification as well as regression. The following are the recipes in Python to use KNN as classifier as well as regressor −</p>\n\n\n\n<h2>KNN as Classifier</h2>\n\n\n\n<p>First, start with importing necessary python packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n﻿</code></pre>\n\n\n\n<p>Next, download the iris dataset from its weblink as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n﻿</code></pre>\n\n\n\n<p>Next, we need to assign column names to the dataset as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>headernames = &#91;'sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']</code></pre>\n\n\n\n<p>Now, we need to read dataset to pandas dataframe as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>dataset = pd.read_csv(path, names = headernames)\ndataset.head()</code></pre>\n\n\n\n<table><tbody><tr><th></th><th>sepal-length</th><th>sepal-width</th><th>petal-length</th><th>petal-width</th><th>Class</th></tr><tr><td>0</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>1</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>2</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>3</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>Iris-setosa</td></tr><tr><td>4</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>Iris-setosa</td></tr></tbody></table>\n\n\n\n<p>Data Preprocessing will be done with the help of following script lines.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>X = dataset.iloc&#91;:, :-1].values\ny = dataset.iloc&#91;:, 4].values\n﻿</code></pre>\n\n\n\n<p>Next, we will divide the data into train and test split. Following code will split the dataset into 60% training data and 40% of testing data −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40)\n﻿</code></pre>\n\n\n\n<p>Next, data scaling will be done as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)</code></pre>\n\n\n\n<p>Next, train the model with the help of KNeighborsClassifier class of sklearn as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 8)\nclassifier.fit(X_train, y_train)</code></pre>\n\n\n\n<p>At last we need to make prediction. It can be done with the help of following script −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>y_pred = classifier.predict(X_test)</code></pre>\n\n\n\n<p>Next, print the results as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",result2)</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>Confusion Matrix:\n&#91;&#91;21 0 0]\n&#91; 0 16 0]\n&#91; 0 7 16]]\nClassification Report:\n                  precision   recall   f1-score   support\n    Iris-setosa        1.00     1.00       1.00        21\nIris-versicolor        0.70     1.00       0.82        16\n Iris-virginica        1.00     0.70       0.82        23\n      micro avg        0.88     0.88       0.88        60\n      macro avg        0.90     0.90       0.88        60\n   weighted avg        0.92     0.88       0.88        60\n\nAccuracy: 0.8833333333333333</code></pre>\n\n\n\n<h2>KNN as Regressor</h2>\n\n\n\n<p>First, start with importing necessary Python packages −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy as np\nimport pandas as pd</code></pre>\n\n\n\n<p>Next, download the iris dataset from its weblink as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n﻿</code></pre>\n\n\n\n<p>Next, we need to assign column names to the dataset as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>headernames = &#91;'sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']</code></pre>\n\n\n\n<p>Now, we need to read dataset to pandas dataframe as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>data = pd.read_csv(url, names = headernames)\narray = data.values\nX = array&#91;:,:2]\nY = array&#91;:,2]\ndata.shape\noutput:(150, 5)</code></pre>\n\n\n\n<p>Next, import&nbsp;<em>KNeighborsRegressor</em>&nbsp;from&nbsp;<em>sklearn</em>&nbsp;to fit the model −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.neighbors import KNeighborsRegressor\nknnr = KNeighborsRegressor(n_neighbors = 10)\nknnr.fit(X, y)</code></pre>\n\n\n\n<p>At last, we can find the MSE as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print (\"The MSE is:\",format(np.power(y-knnr.predict(X),2).mean()))</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>The MSE is: 0.12226666666666669</code></pre>\n\n\n\n<h2>Pros and Cons of KNN</h2>\n\n\n\n<h3>Pros</h3>\n\n\n\n<ul><li>It is very simple algorithm to understand and interpret.</li><li>It is very useful for nonlinear data because there is no assumption about data in this algorithm.</li><li>It is a versatile algorithm as we can use it for classification as well as regression.</li><li>It has relatively high accuracy but there are much better supervised learning models than KNN.</li></ul>\n\n\n\n<h3>Cons</h3>\n\n\n\n<ul><li>It is computationally a bit expensive algorithm because it stores all the training data.</li><li>High memory storage required as compared to other supervised learning algorithms.</li><li>Prediction is slow in case of big N.</li><li>It is very sensitive to the scale of data as well as irrelevant features.</li></ul>\n\n\n\n<h2>Applications of KNN</h2>\n\n\n\n<p>The following are some of the areas in which KNN can be applied successfully −</p>\n\n\n\n<h3>Banking System</h3>\n\n\n\n<p>KNN can be used in banking system to predict weather an individual is fit for loan approval? Does that individual have the characteristics similar to the defaulters one?</p>\n\n\n\n<h3>Calculating Credit Ratings</h3>\n\n\n\n<p>KNN algorithms can be used to find an individual’s credit rating by comparing with the persons having similar traits.</p>\n\n\n\n<h3>Politics</h3>\n\n\n\n<p>With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party ‘Congress’, “Will Vote to Party ‘BJP’.</p>\n\n\n\n<p>Other areas in which KNN algorithm can be used are Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry. The following two properties would define KNN well − Lazy learning algorithm&nbsp;− KNN is a lazy learning algorithm because [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1947,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1460"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1460"}],"version-history":[{"count":3,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1460/revisions"}],"predecessor-version":[{"id":2481,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1460/revisions/2481"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1947"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1460"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1460"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1460"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1466,"date":"2020-05-21T10:01:16","date_gmt":"2020-05-21T10:01:16","guid":{"rendered":"http://python3.foobrdigital.com/?p=1466"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"performance-metrics","status":"publish","type":"post","link":"https://python3.foobrdigital.com/performance-metrics/","title":{"rendered":"Performance Metrics"},"content":{"rendered":"\n<p>There are various metrics which we can use to evaluate the performance of ML algorithms, classification as well as regression algorithms. We must carefully choose the metrics for evaluating ML performance because −</p>\n\n\n\n<ul><li>How the performance of ML algorithms is measured and compared will be dependent entirely on the metric you choose.</li><li>How you weight the importance of various characteristics in the result will be influenced completely by the metric you choose.</li></ul>\n\n\n\n<h2>Performance Metrics for Classification Problems</h2>\n\n\n\n<p>We have discussed classification and its algorithms in the previous chapters. Here, we are going to discuss various performance metrics that can be used to evaluate predictions for classification problems.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"458\" height=\"200\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-26.png\" alt=\"\" class=\"wp-image-1468\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-26.png 458w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-26-300x131.png 300w\" sizes=\"(max-width: 458px) 100vw, 458px\" /></figure>\n\n\n\n<h3>Confusion Matrix</h3>\n\n\n\n<p>It is the easiest way to measure the performance of a classification problem where the output can be of two or more type of classes. A confusion matrix is nothing but a table with two dimensions viz. “Actual” and “Predicted” and furthermore, both the dimensions have “True Positives (TP)”, “True Negatives (TN)”, “False Positives (FP)”, “False Negatives (FN)” as shown below −</p>\n\n\n\n<p>Explanation of the terms associated with confusion matrix are as follows −</p>\n\n\n\n<ul><li><strong>True Positives (TP)</strong>&nbsp;− It is the case when both actual class &amp; predicted class of data point is 1.</li><li><strong>True Negatives (TN)</strong>&nbsp;− It is the case when both actual class &amp; predicted class of data point is 0.</li><li><strong>False Positives (FP)</strong>&nbsp;− It is the case when actual class of data point is 0 &amp; predicted class of data point is 1.</li><li><strong>False Negatives (FN)</strong>&nbsp;− It is the case when actual class of data point is 1 &amp; predicted class of data point is 0.</li></ul>\n\n\n\n<p>We can use&nbsp;<em>confusion_matrix</em>&nbsp;function of&nbsp;<em>sklearn.metrics</em>&nbsp;to compute Confusion Matrix of our classification model.</p>\n\n\n\n<h3>Classification Accuracy</h3>\n\n\n\n<p>It is most common performance metric for classification algorithms. It may be defined as the number of correct predictions made as a ratio of all predictions made. We can easily calculate it by confusion matrix with the help of following formula −Accuracy=TP+TNTP+FP+FN+TNAccuracy=TP+TNTP+FP+FN+TN</p>\n\n\n\n<p>We can use&nbsp;<em>accuracy_score</em>&nbsp;function of&nbsp;<em>sklearn.metrics</em>&nbsp;to compute accuracy of our classification model.</p>\n\n\n\n<h3>Classification Report</h3>\n\n\n\n<p>This report consists of the scores of Precisions, Recall, F1 and Support. They are explained as follows −</p>\n\n\n\n<h3>Precision</h3>\n\n\n\n<p>Precision, used in document retrievals, may be defined as the number of correct documents returned by our ML model. We can easily calculate it by confusion matrix with the help of following formula −Precision=TPTP+FNPrecision=TPTP+FN</p>\n\n\n\n<h3>Recall or Sensitivity</h3>\n\n\n\n<p>Recall may be defined as the number of positives returned by our ML model. We can easily calculate it by confusion matrix with the help of following formula.Recall=TPTP+FNRecall=TPTP+FN</p>\n\n\n\n<h3>Specificity</h3>\n\n\n\n<p>Specificity, in contrast to recall, may be defined as the number of negatives returned by our ML model. We can easily calculate it by confusion matrix with the help of following formula −Specificity=TNTN+FPSpecificity=TNTN+FP</p>\n\n\n\n<h3>Support</h3>\n\n\n\n<p>Support may be defined as the number of samples of the true response that lies in each class of target values.</p>\n\n\n\n<h3>F1 Score</h3>\n\n\n\n<p>This score will give us the harmonic mean of precision and recall. Mathematically, F1 score is the weighted average of the precision and recall. The best value of F1 would be 1 and worst would be 0. We can calculate F1 score with the help of following formula −F1=2∗(precision∗recall)/(precision+recall)F1=2∗(precision∗recall)/(precision+recall)</p>\n\n\n\n<p>F1 score is having equal relative contribution of precision and recall.</p>\n\n\n\n<p>We can use classification_report function of sklearn.metrics to get the classification report of our classification model.</p>\n\n\n\n<h3>AUC (Area Under ROC curve)</h3>\n\n\n\n<p>AUC (Area Under Curve)-ROC (Receiver Operating Characteristic) is a performance metric, based on varying threshold values, for classification problems. As name suggests, ROC is a probability curve and AUC measure the separability. In simple words, AUC-ROC metric will tell us about the capability of model in distinguishing the classes. Higher the AUC, better the model.</p>\n\n\n\n<p>Mathematically, it can be created by plotting TPR (True Positive Rate) i.e. Sensitivity or recall vs FPR (False Positive Rate) i.e. 1-Specificity, at various threshold values. Following is the graph showing ROC, AUC having TPR at y-axis and FPR at x-axis −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"350\" height=\"246\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-25.png\" alt=\"\" class=\"wp-image-1467\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-25.png 350w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-25-300x211.png 300w\" sizes=\"(max-width: 350px) 100vw, 350px\" /></figure>\n\n\n\n<p>We can use roc_auc_score function of sklearn.metrics to compute AUC-ROC.</p>\n\n\n\n<h3>LOGLOSS (Logarithmic Loss)</h3>\n\n\n\n<p>It is also called Logistic regression loss or cross-entropy loss. It basically defined on probability estimates and measures the performance of a classification model where the input is a probability value between 0 and 1. It can be understood more clearly by differentiating it with accuracy. As we know that accuracy is the count of predictions (predicted value = actual value) in our model whereas Log Loss is the amount of uncertainty of our prediction based on how much it varies from the actual label. With the help of Log Loss value, we can have more accurate view of the performance of our model. We can use log_loss function of sklearn.metrics to compute Log Loss.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>The following is a simple recipe in Python which will give us an insight about how we can use the above explained performance metrics on binary classification model −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss\nX_actual = &#91;1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\nY_predic = &#91;1, 0, 1, 1, 1, 0, 1, 1, 0, 0]\nresults = confusion_matrix(X_actual, Y_predic)\nprint ('Confusion Matrix :')\nprint(results)\nprint ('Accuracy Score is',accuracy_score(X_actual, Y_predic))\nprint ('Classification Report : ')\nprint (classification_report(X_actual, Y_predic))\nprint('AUC-ROC:',roc_auc_score(X_actual, Y_predic))\nprint('LOGLOSS Value is',log_loss(X_actual, Y_predic))</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>Confusion Matrix :\n&#91;&#91;3 3]\n&#91;1 3]]\nAccuracy Score is 0.6\nClassification Report :\n               precision   recall   f1-score   support\n           0        0.75     0.50       0.60         6\n           1        0.50     0.75       0.60         4\n   micro avg        0.60     0.60       0.60        10\n   macro avg        0.62     0.62       0.60        10\nweighted avg        0.65     0.60       0.60        10\nAUC-ROC: 0.625\nLOGLOSS Value is 13.815750437193334</code></pre>\n\n\n\n<h2>Performance Metrics for Regression Problems</h2>\n\n\n\n<p>We have discussed regression and its algorithms in previous chapters. Here, we are going to discuss various performance metrics that can be used to evaluate predictions for regression problems.</p>\n\n\n\n<h3>Mean Absolute Error (MAE)</h3>\n\n\n\n<p>It is the simplest error metric used in regression problems. It is basically the sum of average of the absolute difference between the predicted and actual values. In simple words, with MAE, we can get an idea of how wrong the predictions were. MAE does not indicate the direction of the model i.e. no indication about underperformance or overperformance of the model. The following is the formula to calculate MAE −MAE=1n∑∣Y−Y^∣MAE=1n∑∣Y−Y^∣</p>\n\n\n\n<p>Here,y = Actual Output Values</p>\n\n\n\n<p>And&nbsp;Y^Y^= Predicted Output Values.</p>\n\n\n\n<p>We can use mean_absolute_error function of sklearn.metrics to compute MAE.</p>\n\n\n\n<h3>Mean Square Error (MSE)</h3>\n\n\n\n<p>MSE is like the MAE, but the only difference is that the it squares the difference of actual and predicted output values before summing them all instead of using the absolute value. The difference can be noticed in the following equation −MSE=1n∑(Y−Y^)MSE=1n∑(Y−Y^)</p>\n\n\n\n<p>Here,Y = Actual Output Values</p>\n\n\n\n<p>And&nbsp;Y^Y^&nbsp;= Predicted Output Values.</p>\n\n\n\n<p>We can use mean_squared_error function of sklearn.metrics to compute MSE.</p>\n\n\n\n<h3>R Squared (R<sup>2</sup>)</h3>\n\n\n\n<p>R Squared metric is generally used for explanatory purpose and provides an indication of the goodness or fit of a set of predicted output values to the actual output values. The following formula will help us understanding it −R2=1−1n∑ni=1(Yi−Y^i)21n∑ni=1(Yi−Y^i)2R2=1−1n∑i=1n(Yi−Y^i)21n∑i=1n(Yi−Y^i)2</p>\n\n\n\n<p>In the above equation, numerator is MSE and the denominator is the variance in Y values.</p>\n\n\n\n<p>We can use r2_score function of sklearn.metrics to compute R squared value.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>The following is a simple recipe in Python which will give us an insight about how we can use the above explained performance metrics on regression model −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nX_actual = &#91;5, -1, 2, 10]\nY_predic = &#91;3.5, -0.9, 2, 9.9]\nprint ('R Squared =',r2_score(X_actual, Y_predic))\nprint ('MAE =',mean_absolute_error(X_actual, Y_predic))\nprint ('MSE =',mean_squared_error(X_actual, Y_predic))</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>R Squared = 0.9656060606060606\nMAE = 0.42499999999999993\nMSE = 0.5674999999999999</code></pre>\n","protected":false},"excerpt":{"rendered":"<p>There are various metrics which we can use to evaluate the performance of ML algorithms, classification as well as regression algorithms. We must carefully choose the metrics for evaluating ML performance because − How the performance of ML algorithms is measured and compared will be dependent entirely on the metric you choose. How you weight [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1948,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1466"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1466"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1466/revisions"}],"predecessor-version":[{"id":1469,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1466/revisions/1469"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1948"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1466"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1466"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1466"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1470,"date":"2020-05-21T10:04:40","date_gmt":"2020-05-21T10:04:40","guid":{"rendered":"http://python3.foobrdigital.com/?p=1470"},"modified":"2020-09-24T09:00:35","modified_gmt":"2020-09-24T09:00:35","slug":"automatic-workflows","status":"publish","type":"post","link":"https://python3.foobrdigital.com/automatic-workflows/","title":{"rendered":"Automatic Workflows"},"content":{"rendered":"\n<h2>Introduction</h2>\n\n\n\n<p>In order to execute and produce results successfully, a machine learning model must automate some standard workflows. The process of automate these standard workflows can be done with the help of Scikit-learn Pipelines. From a data scientist’s perspective, pipeline is a generalized, but very important concept. It basically allows data flow from its raw format to some useful information. The working of pipelines can be understood with the help of following diagram −</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"596\" height=\"210\" src=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-27.png\" alt=\"\" class=\"wp-image-1471\" srcset=\"https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-27.png 596w, https://python3.foobrdigital.com/wp-content/uploads/2020/05/working_of_svm-27-300x106.png 300w\" sizes=\"(max-width: 596px) 100vw, 596px\" /></figure>\n\n\n\n<p>The blocks of ML pipelines are as follows −</p>\n\n\n\n<p><strong>Data ingestion</strong>&nbsp;− As the name suggests, it is the process of importing the data for use in ML project. The data can be extracted in real time or batches from single or multiple systems. It is one of the most challenging steps because the quality of data can affect the whole ML model.</p>\n\n\n\n<p><strong>Data Preparation</strong>&nbsp;− After importing the data, we need to prepare data to be used for our ML model. Data preprocessing is one of the most important technique of data preparation.</p>\n\n\n\n<p><strong>ML Model Training</strong>&nbsp;− Next step is to train our ML model. We have various ML algorithms like supervised, unsupervised, reinforcement to extract the features from data, and make predictions.</p>\n\n\n\n<p><strong>Model Evaluation</strong>&nbsp;− Next, we need to evaluate the ML model. In case of AutoML pipeline, ML model can be evaluated with the help of various statistical methods and business rules.</p>\n\n\n\n<p><strong>ML Model retraining</strong>&nbsp;− In case of AutoML pipeline, it is not necessary that the first model is best one. The first model is considered as a baseline model and we can train it repeatably to increase model’s accuracy.</p>\n\n\n\n<p><strong>Deployment</strong>&nbsp;− At last, we need to deploy the model. This step involves applying and migrating the model to business operations for their use.</p>\n\n\n\n<h2>Challenges Accompanying ML Pipelines</h2>\n\n\n\n<p>In order to create ML pipelines, data scientists face many challenges. These challenges fall into the following three categories −</p>\n\n\n\n<h3>Quality of Data</h3>\n\n\n\n<p>The success of any ML model depends heavily on the quality of data. If the data we are providing to ML model is not accurate, reliable and robust, then we are going to end with wrong or misleading output.</p>\n\n\n\n<h3>Data Reliability</h3>\n\n\n\n<p>Another challenge associated with ML pipelines is the reliability of data we are providing to the ML model. As we know, there can be various sources from which data scientist can acquire data but to get the best results, it must be assured that the data sources are reliable and trusted.</p>\n\n\n\n<h3>Data Accessibility</h3>\n\n\n\n<p>To get the best results out of ML pipelines, the data itself must be accessible which requires consolidation, cleansing and curation of data. As a result of data accessibility property, metadata will be updated with new tags.</p>\n\n\n\n<h2>Modelling ML Pipeline and Data Preparation</h2>\n\n\n\n<p>Data leakage, happening from training dataset to testing dataset, is an important issue for data scientist to deal with while preparing data for ML model. Generally, at the time of data preparation, data scientist uses techniques like standardization or normalization on entire dataset before learning. But these techniques cannot help us from the leakage of data because the training dataset would have been influenced by the scale of the data in the testing dataset.</p>\n\n\n\n<p>By using ML pipelines, we can prevent this data leakage because pipelines ensure that data preparation like standardization is constrained to each fold of our cross-validation procedure.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>The following is an example in Python that demonstrate data preparation and model evaluation workflow. For this purpose, we are using Pima Indian Diabetes dataset from Sklearn. First, we will be creating pipeline that standardized the data. Then a Linear Discriminative analysis model will be created and at last the pipeline will be evaluated using 10-fold cross validation.</p>\n\n\n\n<p>First, import the required packages as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis</code></pre>\n\n\n\n<p>Now, we need to load the Pima diabetes dataset as did in previous examples −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names = headernames)\narray = data.values</code></pre>\n\n\n\n<p>Next, we will create a pipeline with the help of the following code −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>estimators = &#91;]\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('lda', LinearDiscriminantAnalysis()))\nmodel = Pipeline(estimators)</code></pre>\n\n\n\n<p>At last, we are going to evaluate this pipeline and output its accuracy as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>kfold = KFold(n_splits = 20, random_state = 7)\nresults = cross_val_score(model, X, Y, cv = kfold)\nprint(results.mean())</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>0.7790148448043184</code></pre>\n\n\n\n<p>The above output is the summary of accuracy of the setup on the dataset.</p>\n\n\n\n<h2>Modelling ML Pipeline and Feature Extraction</h2>\n\n\n\n<p>Data leakage can also happen at feature extraction step of ML model. That is why feature extraction procedures should also be restricted to stop data leakage in our training dataset. As in the case of data preparation, by using ML pipelines, we can prevent this data leakage also. FeatureUnion, a tool provided by ML pipelines can be used for this purpose.</p>\n\n\n\n<h3>Example</h3>\n\n\n\n<p>The following is an example in Python that demonstrates feature extraction and model evaluation workflow. For this purpose, we are using Pima Indian Diabetes dataset from Sklearn.</p>\n\n\n\n<p>First, 3 features will be extracted with PCA (Principal Component Analysis). Then, 6 features will be extracted with Statistical Analysis. After feature extraction, result of multiple feature selection and extraction procedures will be combined by using</p>\n\n\n\n<p>FeatureUnion tool. At last, a Logistic Regression model will be created, and the pipeline will be evaluated using 10-fold cross validation.</p>\n\n\n\n<p>First, import the required packages as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest</code></pre>\n\n\n\n<p>Now, we need to load the Pima diabetes dataset as did in previous examples −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names = headernames)\narray = data.values</code></pre>\n\n\n\n<p>Next, feature union will be created as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>features = &#91;]\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)</code></pre>\n\n\n\n<p>Next, pipeline will be creating with the help of following script lines −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>estimators = &#91;]\nestimators.append(('feature_union', feature_union))\nestimators.append(('logistic', LogisticRegression()))\nmodel = Pipeline(estimators)</code></pre>\n\n\n\n<p>At last, we are going to evaluate this pipeline and output its accuracy as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>kfold = KFold(n_splits = 20, random_state = 7)\nresults = cross_val_score(model, X, Y, cv = kfold)\nprint(results.mean())</code></pre>\n\n\n\n<h3>Output</h3>\n\n\n\n<pre class=\"wp-block-code\"><code>0.7789811066126855</code></pre>\n\n\n\n<p>The above output is the summary of accuracy of the setup on the dataset.</p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction In order to execute and produce results successfully, a machine learning model must automate some standard workflows. The process of automate these standard workflows can be done with the help of Scikit-learn Pipelines. From a data scientist’s perspective, pipeline is a generalized, but very important concept. It basically allows data flow from its raw [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1949,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1470"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1470"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1470/revisions"}],"predecessor-version":[{"id":1472,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1470/revisions/1472"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1949"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1470"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1470"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1470"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1473,"date":"2020-05-21T10:06:38","date_gmt":"2020-05-21T10:06:38","guid":{"rendered":"http://python3.foobrdigital.com/?p=1473"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"improving-performance-of-ml-models","status":"publish","type":"post","link":"https://python3.foobrdigital.com/improving-performance-of-ml-models/","title":{"rendered":"Improving Performance of ML Models"},"content":{"rendered":"\n<h2>Performance Improvement with Ensembles</h2>\n\n\n\n<p>Ensembles can give us boost in the machine learning result by combining several models. Basically, ensemble models consist of several individually trained supervised learning models and their results are merged in various ways to achieve better predictive performance compared to a single model. Ensemble methods can be divided into following two groups −</p>\n\n\n\n<h3>Sequential ensemble methods</h3>\n\n\n\n<p>As the name implies, in these kind of ensemble methods, the base learners are generated sequentially. The motivation of such methods is to exploit the dependency among base learners.</p>\n\n\n\n<h3>Parallel ensemble methods</h3>\n\n\n\n<p>As the name implies, in these kind of ensemble methods, the base learners are generated in parallel. The motivation of such methods is to exploit the independence among base learners.</p>\n\n\n\n<h2>Ensemble Learning Methods</h2>\n\n\n\n<p>The following are the most popular ensemble learning methods i.e. the methods for combining the predictions from different models −</p>\n\n\n\n<h3>Bagging</h3>\n\n\n\n<p>The term bagging is also known as bootstrap aggregation. In bagging methods, ensemble model tries to improve prediction accuracy and decrease model variance by combining predictions of individual models trained over randomly generated training samples. The final prediction of ensemble model will be given by calculating the average of all predictions from the individual estimators. One of the best examples of bagging methods are random forests.</p>\n\n\n\n<h3>Boosting</h3>\n\n\n\n<p>In boosting method, the main principle of building ensemble model is to build it incrementally by training each base model estimator sequentially. As the name suggests, it basically combine several week base learners, trained sequentially over multiple iterations of training data, to build powerful ensemble. During the training of week base learners, higher weights are assigned to those learners which were misclassified earlier. The example of boosting method is AdaBoost.</p>\n\n\n\n<h3>Voting</h3>\n\n\n\n<p>In this ensemble learning model, multiple models of different types are built and some simple statistics, like calculating mean or median etc., are used to combine the predictions. This prediction will serve as the additional input for training to make the final prediction.</p>\n\n\n\n<h2>Bagging Ensemble Algorithms</h2>\n\n\n\n<p>The following are three bagging ensemble algorithms −</p>\n\n\n\n<ul><li>Bagged Decision Tree</li><li>Random Forest</li><li>Extra Trees</li></ul>\n\n\n\n<h2>Boosting Ensemble Algorithms</h2>\n\n\n\n<p>The followings are the two most common boosting ensemble algorithms −</p>\n\n\n\n<ul><li>AdaBoost</li><li>Stochastic Gradient Boosting</li></ul>\n\n\n\n<h2>Voting Ensemble Algorithms</h2>\n\n\n\n<p>As discussed, voting first creates two or more standalone models from training dataset and then a voting classifier will wrap the model along with taking the average of the predictions of sub-model whenever needed new data.</p>\n\n\n\n<p>In the following Python recipe, we are going to build Voting ensemble model for classification by using VotingClassifier class of sklearn on Pima Indians diabetes dataset. We are combining the predictions of logistic regression, Decision Tree classifier and SVM together for a classification problem as follows −</p>\n\n\n\n<p>First, import the required packages as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>from pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier</code></pre>\n\n\n\n<p>Now, we need to load the Pima diabetes dataset as did in previous examples −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names = headernames)\narray = data.values\nX = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>Next, give the input for 10-fold cross validation as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>kfold = KFold(n_splits = 10, random_state = 7)</code></pre>\n\n\n\n<p>Next, we need to create sub-models as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>estimators = &#91;]\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))</code></pre>\n\n\n\n<p>Now, create the voting ensemble model by combining the predictions of above created sub models.</p>\n\n\n\n<pre class=\"wp-block-code\"><code>ensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X, Y, cv = kfold)\nprint(results.mean())</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>0.7382262474367738</code></pre>\n\n\n\n<p>The output above shows that we got around 74% accuracy of our voting classifier ensemble model.</p>\n","protected":false},"excerpt":{"rendered":"<p>Performance Improvement with Ensembles Ensembles can give us boost in the machine learning result by combining several models. Basically, ensemble models consist of several individually trained supervised learning models and their results are merged in various ways to achieve better predictive performance compared to a single model. Ensemble methods can be divided into following two [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1950,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1473"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1473"}],"version-history":[{"count":1,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1473/revisions"}],"predecessor-version":[{"id":1474,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1473/revisions/1474"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1950"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1473"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1473"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1473"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}},{"id":1475,"date":"2020-05-21T10:09:11","date_gmt":"2020-05-21T10:09:11","guid":{"rendered":"http://python3.foobrdigital.com/?p=1475"},"modified":"2020-09-24T09:00:36","modified_gmt":"2020-09-24T09:00:36","slug":"improving-performance-of-ml-modelcontd","status":"publish","type":"post","link":"https://python3.foobrdigital.com/improving-performance-of-ml-modelcontd/","title":{"rendered":"Improving Performance of ML Model(Contd..)"},"content":{"rendered":"\n<h2>Performance Improvement with Algorithm Tuning</h2>\n\n\n\n<p>As we know that ML models are parameterized in such a way that their behavior can be adjusted for a specific problem. Algorithm tuning means finding the best combination of these parameters so that the performance of ML model can be improved. This process sometimes called hyperparameter optimization and the parameters of algorithm itself are called hyperparameters and coefficients found by ML algorithm are called parameters.</p>\n\n\n\n<h2>Performance Improvement with Algorithm Tuning</h2>\n\n\n\n<p>Here, we are going to discuss about some methods for algorithm parameter tuning provided by Python Scikit-learn.</p>\n\n\n\n<h3>Grid Search Parameter Tuning</h3>\n\n\n\n<p>It is a parameter tuning approach. The key point of working of this method is that it builds and evaluate the model methodically for every possible combination of algorithm parameter specified in a grid. Hence, we can say that this algorithm is having search nature.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In the following Python recipe, we are going to perform grid search by using GridSearchCV class of sklearn for evaluating various alpha values for the Ridge Regression algorithm on Pima Indians diabetes dataset.</p>\n\n\n\n<p>First, import the required packages as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy\nfrom pandas import read_csv\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV</code></pre>\n\n\n\n<p>Now, we need to load the Pima diabetes dataset as did in previous examples −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names = headernames)\narray = data.values\nX = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>Next, evaluate the various alpha values as follows;</p>\n\n\n\n<pre class=\"wp-block-code\"><code>alphas = numpy.array(&#91;1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha = alphas)</code></pre>\n\n\n\n<p>Now, we need to apply grid search on our model −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>model = Ridge()\ngrid = GridSearchCV(estimator = model, param_grid = param_grid)\ngrid.fit(X, Y)</code></pre>\n\n\n\n<p>Print the result with following script line −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>print(grid.best_score_)\nprint(grid.best_estimator_.alpha)</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>0.2796175593129722\n1.0</code></pre>\n\n\n\n<p>The above output gives us the optimal score and the set of parameters in the grid that achieved that score. The alpha value in this case is 1.0.</p>\n\n\n\n<h3>Random Search Parameter Tuning</h3>\n\n\n\n<p>It is a parameter tuning approach. The key point of working of this method is that it samples the algorithm parameters from a random distribution for a fixed number of iterations.</p>\n\n\n\n<p><strong>Example</strong></p>\n\n\n\n<p>In the following Python recipe, we are going to perform random search by using RandomizedSearchCV class of sklearn for evaluating different alpha values between 0 and 1 for the Ridge Regression algorithm on Pima Indians diabetes dataset.</p>\n\n\n\n<p>First, import the required packages as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>import numpy\nfrom pandas import read_csv\nfrom scipy.stats import uniform\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV</code></pre>\n\n\n\n<p>Now, we need to load the Pima diabetes dataset as did in previous examples −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>path = r\"C:\\pima-indians-diabetes.csv\"\nheadernames = &#91;'preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv(path, names=headernames)\narray = data.values\nX = array&#91;:,0:8]\nY = array&#91;:,8]</code></pre>\n\n\n\n<p>Next, evaluate the various alpha values on Ridge regression algorithm as follows −</p>\n\n\n\n<pre class=\"wp-block-code\"><code>param_grid = {'alpha': uniform()}\nmodel = Ridge()\nrandom_search = RandomizedSearchCV(\n   estimator = model, param_distributions = param_grid, n_iter = 50, random_state=7)\nrandom_search.fit(X, Y)</code></pre>\n\n\n\n<p><strong>Output</strong></p>\n\n\n\n<pre class=\"wp-block-code\"><code>0.27961712703051084\n0.9779895119966027</code></pre>\n\n\n\n<p>The above output gives us the optimal score just similar to the grid search.</p>\n","protected":false},"excerpt":{"rendered":"<p>Performance Improvement with Algorithm Tuning As we know that ML models are parameterized in such a way that their behavior can be adjusted for a specific problem. Algorithm tuning means finding the best combination of these parameters so that the performance of ML model can be improved. This process sometimes called hyperparameter optimization and the [&hellip;]</p>\n","protected":false},"author":4,"featured_media":1951,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[11,66],"tags":[],"_links":{"self":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1475"}],"collection":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts"}],"about":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/types/post"}],"author":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/users/4"}],"replies":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/comments?post=1475"}],"version-history":[{"count":2,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1475/revisions"}],"predecessor-version":[{"id":2480,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/posts/1475/revisions/2480"}],"wp:featuredmedia":[{"embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media/1951"}],"wp:attachment":[{"href":"https://python3.foobrdigital.com/wp-json/wp/v2/media?parent=1475"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/categories?post=1475"},{"taxonomy":"post_tag","embeddable":true,"href":"https://python3.foobrdigital.com/wp-json/wp/v2/tags?post=1475"}],"curies":[{"name":"wp","href":"https://api.w.org/{rel}","templated":true}]}}]